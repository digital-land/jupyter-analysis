{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision data quality report\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date created**:  April 2025 <br>\n",
    "**Dataset Scope**: ODP datasets <br>\n",
    "**Report Type**: Ad-hoc <br>\n",
    "\n",
    "**Purpose**: The purpose of this report is demonstrate the basic methodology of scoring provisions using the data quality measurement framework. The data sources required for assessing whether a provision passes or fails criteria in the framework vary, so currently this stripped back method just uses issues logged in the `digital-land.issue` table which are summarised in `performance.endpoint_dataset_issue_type_summary`, and failed expectations logged in `digital-land.expectation`. The starting point is the `performance.provision_summary` table, which includes all provisions and whether they currently have an active endpoint or not. For simplicity only provisions with one or more active endpoints are scored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality framework  \n",
    "The table below visualises the framework that is used to assign a quality level to each ODP data provision. \n",
    "\n",
    "The criteria marked as \"true\" at each level must be met by a data provision in order for it to be scored at that level. The levels are cumulative, so all criteria must be met in order for a provision to be scored as *data that is trustworthy*.\n",
    "\n",
    "![quality framework table](quality-framework-table.png)\n",
    "\n",
    "\n",
    "\n",
    "Criteria not included in this example method are:\n",
    "\n",
    "* *Data from an authoritative source*\n",
    "* *No deleted entities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import functions_core as fc\n",
    "\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "db_dir = \"../../data/db_downloads/\"\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "output_dir = \"../../data/quality_report/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance db\n",
    "fc.download_dataset(\"performance\", db_dir, overwrite=False)\n",
    "path_perf_db = os.path.join(db_dir, \"performance.db\")\n",
    "\n",
    "# Issue quality criteria lookup\n",
    "lookup_issue_qual = fc.datasette_query(\n",
    "    \"digital-land\",\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        description,\n",
    "        issue_type,\n",
    "        name,\n",
    "        severity,\n",
    "        responsibility,\n",
    "        quality_criteria_level || \" - \" || quality_criteria as quality_criteria,\n",
    "        quality_criteria_level as quality_level\n",
    "    FROM issue_type\n",
    "    WHERE quality_criteria_level != ''\n",
    "    AND quality_criteria != ''\n",
    "    \"\"\" \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision = fc.query_sqlite(\n",
    "    path_perf_db,\n",
    "    \"\"\"\n",
    "    SELECT organisation, dataset, active_endpoint_count\n",
    "    FROM provision_summary\n",
    "\"\"\")\n",
    "\n",
    "# provision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY PROBLEMS - issues\n",
    "\n",
    "# extract issue count by provision from endpoint_dataset_issue_type_summary\n",
    "qual_issue = fc.query_sqlite(\n",
    "    path_perf_db,\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        organisation, dataset,\n",
    "        'issue' as problem_source,\n",
    "        issue_type as problem_type, \n",
    "        sum(count_issues) as count\n",
    "    FROM endpoint_dataset_issue_type_summary\n",
    "    WHERE resource_end_date is not NULL\n",
    "    AND issue_type is not NULL\n",
    "    GROUP BY organisation, dataset, issue_type\n",
    "\"\"\")\n",
    "\n",
    "# join on quality criteria and level from issue_type lookup (this restricts to only issues linked to a quality criteria)\n",
    "qual_issue = qual_issue.merge(\n",
    "    lookup_issue_qual[[\"issue_type\", \"quality_criteria\", \"quality_level\"]],\n",
    "    how = \"inner\",\n",
    "    left_on = \"problem_type\",\n",
    "    right_on = \"issue_type\"\n",
    ")\n",
    "\n",
    "qual_issue.drop(\"issue_type\", axis=1, inplace=True)\n",
    "\n",
    "qual_issue.to_csv(\"01_quality_problems-source-issues_all.csv\", index = False)\n",
    "# qual_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY PROBLEMS - expectations - entity beyond LPA bounds\n",
    "\n",
    "qual_expectation_bounds = fc.datasette_query(\n",
    "    \"digital-land\", \n",
    "    \"\"\"\n",
    "    SELECT organisation, dataset, details\n",
    "    FROM expectation\n",
    "    WHERE 1=1\n",
    "        AND name = 'Check no entities are outside of the local planning authority boundary'\n",
    "        AND passed = 'False'\n",
    "        AND message not like '%error%'\n",
    "    \"\"\")\n",
    "\n",
    "qual_expectation_bounds[\"problem_source\"] = \"expectation\"\n",
    "qual_expectation_bounds[\"problem_type\"] = \"entity outside of the local planning authority boundary\"\n",
    "qual_expectation_bounds[\"count\"] = [json.loads(v)[\"actual\"] for v in qual_expectation_bounds[\"details\"]]\n",
    "qual_expectation_bounds[\"quality_criteria\"] = \"3 - entities within LPA boundary\"\n",
    "qual_expectation_bounds[\"quality_level\"] = 3\n",
    "qual_expectation_bounds.drop(\"details\", axis=1, inplace=True)\n",
    "qual_expectation_bounds.to_csv(\"01_quality_problems-source-expectation_bounds.csv\", index = False)\n",
    "# qual_expectation_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY PROBLEMS - expectations - entity beyond LPA bounds\n",
    "\n",
    "qual_expectation_count = fc.datasette_query(\n",
    "    \"digital-land\", \n",
    "    \"\"\"\n",
    "    SELECT organisation, dataset, details\n",
    "    FROM expectation\n",
    "    WHERE 1=1\n",
    "        AND name = 'Check number of entities inside the local planning authority boundary matches the manual count' \n",
    "        AND passed = 'False'\n",
    "        AND message not like '%error%'\n",
    "    \"\"\")\n",
    "\n",
    "qual_expectation_count[\"problem_source\"] = \"expectation\"\n",
    "qual_expectation_count[\"problem_type\"] = \"entity count doesn't match manual count\"\n",
    "qual_expectation_count[\"count\"] = [json.loads(v)[\"actual\"] for v in qual_expectation_count[\"details\"]]\n",
    "qual_expectation_count[\"quality_criteria\"] = \"3 - conservation area entity count matches LPA\"\n",
    "qual_expectation_count[\"quality_level\"] = 3\n",
    "qual_expectation_count.drop(\"details\", axis=1, inplace=True)\n",
    "qual_expectation_count.to_csv(\"01_quality_problems-source-expectation_count.csv\", index = False)\n",
    "# qual_expectation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all problem source tables, and aggregate to criteria level\n",
    "\n",
    "qual_all_criteria = pd.concat(\n",
    "    [qual_issue, qual_expectation_bounds, qual_expectation_count]\n",
    ").groupby(\n",
    "    [\"organisation\", \"dataset\", \"quality_criteria\", \"quality_level\"],\n",
    "    as_index=False\n",
    ").agg(\n",
    "    count_failures = (\"count\", \"sum\")\n",
    ")\n",
    "\n",
    "# qual_all_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_qual_all = provision.merge(\n",
    "    qual_all_criteria,\n",
    "    how = \"left\",\n",
    "    on = [\"organisation\", \"dataset\"]\n",
    ")\n",
    "\n",
    "prov_qual_all.to_csv(\"02_provision_quality_all_criteria.csv\", index = False)\n",
    "# prov_qual_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when quality_level is null & active_endpoint_count > 0 then quality_level = 4 (we have endpoints with no issues, so quality = trustworthy)\n",
    "# active_endpoint_count = 0 then quality_level = 0 (we have no active endpoints, so quality = no score)\n",
    "prov_qual_all[\"quality_level_for_sort\"] = np.select(\n",
    "    [\n",
    "        (prov_qual_all[\"active_endpoint_count\"] == 0),\n",
    "        (prov_qual_all[\"quality_level\"].notnull()),\n",
    "        (prov_qual_all[\"active_endpoint_count\"] > 0) & (prov_qual_all[\"quality_level\"].isnull())\n",
    "    ],\n",
    "    [\n",
    "        0, prov_qual_all[\"quality_level\"], 4\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_level  quality                           \n",
      "0.0            0. no score                           4956\n",
      "3.0            3. data that is good for ODP           314\n",
      "4.0            4. data that is trustworthy            310\n",
      "2.0            2. authoritative data from the LPA     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "level_map = {\n",
    "    4: \"4. data that is trustworthy\",\n",
    "    3: \"3. data that is good for ODP\",\n",
    "    2: \"2. authoritative data from the LPA\",\n",
    "    1: \"1. some data\",\n",
    "    0: \"0. no score\"}\n",
    "\n",
    "prov_quality = prov_qual_all.groupby([\n",
    "    \"organisation\", \"dataset\"\n",
    "],\n",
    "    as_index=False,\n",
    "    dropna=False\n",
    ").agg(\n",
    "    quality_level = (\"quality_level_for_sort\", \"min\")\n",
    ")\n",
    "\n",
    "prov_quality[\"quality\"] = prov_quality[\"quality_level\"].map(level_map)\n",
    "prov_quality[\"notes\"] = \"\"\n",
    "prov_quality[\"end-date\"] = \"\"\n",
    "prov_quality[\"start-date\"] = td\n",
    "prov_quality[\"entry-date\"] = td\n",
    "\n",
    "print(prov_quality.value_counts([\"quality_level\", \"quality\"]))\n",
    "prov_quality[[\"dataset\", \"end-date\", \"entry-date\", \"notes\", \"organisation\", \"quality\", \"start-date\"]].to_csv(\"03_provision_quality_scored.csv\", index = False)\n",
    "# prov_quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
