{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision data quality report\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date created**:  April 2025 <br>\n",
    "**Dataset Scope**: ODP datasets <br>\n",
    "**Report Type**: Ad-hoc <br>\n",
    "\n",
    "**Purpose**: The purpose of this report is demonstrate the basic methodology of scoring provisions using the data quality measurement framework. The data sources required for assessing whether a provision passes or fails criteria in the framework vary, so currently this stripped back method just uses issues logged in the `digital-land.issue` table which are summarised in `performance.endpoint_dataset_issue_type_summary`. The starting point is the `performance.provision_summary` table, which includes all provisions and whether they currently have an active endpoint or not. For simplicity only provisions with one or more active endpoints are scored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality framework  \n",
    "The table below visualises the framework that is used to assign a quality level to each ODP data provision. \n",
    "\n",
    "The criteria marked as \"true\" at each level must be met by a data provision in order for it to be scored at that level. The levels are cumulative, so all criteria must be met in order for a provision to be scored as *data that is trustworthy*.\n",
    "\n",
    "![quality framework table](quality-framework-table.png)\n",
    "\n",
    "\n",
    "\n",
    "Criteria not included in this example method are:\n",
    "\n",
    "* *Data from an authoritative source*\n",
    "* *No deleted entities*\n",
    "* *Conservation area entity count matches LPA*\n",
    "* *Entities within LPA boundary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import functions_core as fc\n",
    "\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "db_dir = \"../../data/db_downloads/\"\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "output_dir = \"../../data/quality_report/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance db\n",
    "fc.download_dataset(\"performance\", db_dir, overwrite=False)\n",
    "path_perf_db = os.path.join(db_dir, \"performance.db\")\n",
    "\n",
    "# Issue quality criteria lookup\n",
    "lookup_issue_qual = fc.datasette_query(\n",
    "    \"digital-land\",\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        description,\n",
    "        issue_type,\n",
    "        name,\n",
    "        severity,\n",
    "        responsibility,\n",
    "        quality_criteria_level || \" - \" || quality_criteria as quality_criteria,\n",
    "        quality_criteria_level as quality_level\n",
    "    FROM issue_type\n",
    "    WHERE quality_criteria_level IS NOT NULL\n",
    "    \"\"\" \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision = fc.query_sqlite(\n",
    "    path_perf_db,\n",
    "    \"\"\"\n",
    "    SELECT organisation, dataset, active_endpoint_count\n",
    "    FROM provision_summary\n",
    "\"\"\")\n",
    "\n",
    "# provision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract issue count by provision from endpoint_dataset_issue_type_summary\n",
    "qual_issue = fc.query_sqlite(\n",
    "    path_perf_db,\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        organisation, dataset,\n",
    "        'issue' as problem_source,\n",
    "        issue_type as problem_type, \n",
    "        sum(count_issues) as count\n",
    "    FROM endpoint_dataset_issue_type_summary\n",
    "    WHERE resource_end_date is not NULL\n",
    "    AND issue_type is not NULL\n",
    "    GROUP BY organisation, dataset, issue_type\n",
    "\"\"\")\n",
    "\n",
    "# join on quality criteria and level from issue_type lookup (this restricts to only issues linked to a quality criteria)\n",
    "qual_issue = qual_issue.merge(\n",
    "    lookup_issue_qual[[\"issue_type\", \"quality_criteria\", \"quality_level\"]],\n",
    "    how = \"inner\",\n",
    "    left_on = \"problem_type\",\n",
    "    right_on = \"issue_type\"\n",
    ")\n",
    "\n",
    "qual_issue.drop(\"issue_type\", axis=1, inplace=True)\n",
    "\n",
    "qual_issue.to_csv(\"01_quality_problems-source-issues.csv\", index = False)\n",
    "# qual_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all problem source tables, and aggregate to criteria level\n",
    "\n",
    "qual_all_criteria = pd.concat(\n",
    "    [qual_issue]\n",
    ").groupby(\n",
    "    [\"organisation\", \"dataset\", \"quality_criteria\", \"quality_level\"],\n",
    "    as_index=False\n",
    ").agg(\n",
    "    count_failures = (\"count\", \"sum\")\n",
    ")\n",
    "\n",
    "# qual_all_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_qual_all = provision.merge(\n",
    "    qual_all_criteria,\n",
    "    how = \"left\",\n",
    "    on = [\"organisation\", \"dataset\"]\n",
    ")\n",
    "\n",
    "prov_qual_all.to_csv(\"02_provision_quality_all_criteria.csv\", index = False)\n",
    "# prov_qual_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when quality_level is null & active_endpoint_count > 0 then quality_level = 4 (we have endpoints with no issues, so quality = trustworthy)\n",
    "# active_endpoint_count = 0 then quality_level = 0 (we have no active endpoints, so quality = no score)\n",
    "prov_qual_all[\"quality_level_comp\"] = np.select(\n",
    "    [\n",
    "        (prov_qual_all[\"active_endpoint_count\"] == 0),\n",
    "        (prov_qual_all[\"quality_level\"].notnull()),\n",
    "        (prov_qual_all[\"active_endpoint_count\"] > 0) & (prov_qual_all[\"quality_level\"].isnull())\n",
    "    ],\n",
    "    [\n",
    "        0, prov_qual_all[\"quality_level\"], 4\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_level  quality                           \n",
      "0.0            0. no score                           4956\n",
      "4.0            4. data that is trustworthy            387\n",
      "3.0            3. data that is good for ODP           237\n",
      "2.0            2. authoritative data from the LPA     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "level_map = {\n",
    "    4: \"4. data that is trustworthy\",\n",
    "    3: \"3. data that is good for ODP\",\n",
    "    2: \"2. authoritative data from the LPA\",\n",
    "    1: \"1. some data\",\n",
    "    0: \"0. no score\"}\n",
    "\n",
    "prov_quality = prov_qual_all.groupby([\n",
    "    \"organisation\", \"dataset\"\n",
    "],\n",
    "    as_index=False,\n",
    "    dropna=False\n",
    ").agg(\n",
    "    quality_level = (\"quality_level_comp\", \"min\")\n",
    ")\n",
    "\n",
    "prov_quality[\"quality\"] = prov_quality[\"quality_level\"].map(level_map)\n",
    "prov_quality[\"notes\"] = \"\"\n",
    "prov_quality[\"end-date\"] = \"\"\n",
    "prov_quality[\"start-date\"] = td\n",
    "prov_quality[\"entry-date\"] = td\n",
    "\n",
    "print(prov_quality.value_counts([\"quality_level\", \"quality\"]))\n",
    "prov_quality[[\"dataset\", \"end-date\", \"entry-date\", \"notes\", \"organisation\", \"quality\", \"start-date\"]].to_csv(\"02_provision_quality_scored.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
