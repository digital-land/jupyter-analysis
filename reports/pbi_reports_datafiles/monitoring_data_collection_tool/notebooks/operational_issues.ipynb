{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0520741d",
   "metadata": {},
   "source": [
    "This script automates the process of exporting SQL query results from a Datasette API and saving them as CSV files. It is designed to batch export data from multiple Datasette URLs using custom SQL queries.\n",
    "\n",
    "The script begins by accepting command-line input for an output directory via `argparse`. It defines a dictionary of Datasette URLs (`urls`) and a corresponding list of SQL queries (`sqls`). Each SQL query is URL-encoded and appended to the respective Datasette endpoint using the `.json?sql=...&_shape=array` format, which returns query results in JSON array structure.\n",
    "\n",
    "Using `requests`, the script sends GET requests to these full URLs. The JSON response is parsed into a pandas DataFrame, which is then saved as a CSV file to the specified output directory. The script also ensures the directory exists and handles failures gracefullyâ€”printing error messages but continuing to the next item.\n",
    "\n",
    "Included examples:\n",
    "- `log_by_week`: Groups log records by week and HTTP status (200 vs. failures), showing request volumes.\n",
    "- `operational_issues`: Aggregates operational issues by week over the past 6 months.\n",
    "\n",
    "Each result is saved using the key name from the `urls` dictionary (e.g. `log_by_week.csv`, `operational_issues.csv`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f569797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: log_by_week from SQL URL:\n",
      "https://datasette.planning.data.gov.uk/digital-land.json?sql=%0A%20%20%20%20%20%20%20%20SELECT%0A%20%20%20%20%20%20%20%20%20%20%20%20COUNT%28endpoint%29%20AS%20endpoint_count%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20SUBSTR%28entry_date%2C%201%2C%2010%29%20AS%20entrydate%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20DATE%28entry_date%2C%20%27weekday%200%27%2C%20%27-6%20days%27%29%20AS%20week_start%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20CASE%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20WHEN%20status%20%3D%20200%20THEN%20%27200%27%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20ELSE%20%27FAIL%27%0A%20%20%20%20%20%20%20%20%20%20%20%20END%20AS%20status_group%0A%20%20%20%20%20%20%20%20FROM%20log%0A%20%20%20%20%20%20%20%20WHERE%0A%20%20%20%20%20%20%20%20%20%20%20%20entry_date%20%3E%3D%20DATE%28%27now%27%2C%20%27-6%20months%27%29%0A%20%20%20%20%20%20%20%20%20%20%20%20AND%20SUBSTR%28entry_date%2C%201%2C%2010%29%20%3C%3D%20DATE%28entry_date%2C%20%27weekday%200%27%2C%20%27-6%20days%27%29%0A%20%20%20%20%20%20%20%20GROUP%20BY%0A%20%20%20%20%20%20%20%20%20%20%20%20entrydate%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20week_start%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20status_group%0A%20%20%20%20%20%20%20%20ORDER%20BY%0A%20%20%20%20%20%20%20%20%20%20%20%20entry_date%20DESC%3B%0A%0A%20%20%20%20%20%20%20%20&_shape=array\n",
      "Rows returned: 52\n",
      "Saved: C:/Users/DanielGodden/Documents/MCHLG/collecting_and_managing_data\\log_by_week.csv\n",
      "Fetching: operational_issues from SQL URL:\n",
      "https://datasette.planning.data.gov.uk/digital-land.json?sql=%0A%20%20%20%20%20%20%20%20SELECT%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Bentry-date%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20COUNT%28rowid%29%20AS%20issue_count%0A%20%20%20%20%20%20%20%20FROM%0A%20%20%20%20%20%20%20%20%20%20%20%20operational_issue%0A%20%20%20%20%20%20%20%20WHERE%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Bentry-date%5D%20%3E%3D%20DATE%28%27now%27%2C%20%27-6%20months%27%29%0A%20%20%20%20%20%20%20%20GROUP%20BY%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Bentry-date%5D%3B%0A%20%20%20%20%20%20%20%20&_shape=array\n",
      "Rows returned: 181\n",
      "Saved: C:/Users/DanielGodden/Documents/MCHLG/collecting_and_managing_data\\operational_issues.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def sql_queried_datasette_tables(urls: dict, sqls: list, save_dir: str):\n",
    "    \"\"\"\n",
    "    Fetches data from a dictionary of Datasette URLs using optional SQL queries\n",
    "    and saves each result as a CSV file in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        urls (dict): Mapping of table names to Datasette base URLs.\n",
    "        sqls (list of str): SQL queries corresponding to each URL.\n",
    "        save_dir (str): Directory path where the resulting CSV files will be saved.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the lengths of the URLs and SQL lists do not match.\n",
    "    \"\"\"\n",
    "    if len(urls) != len(sqls):\n",
    "        raise ValueError(\"The number of URLs and SQL queries must match.\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each (name, URL) and associated SQL\n",
    "    for (name, url), sql in zip(urls.items(), sqls):\n",
    "        try:\n",
    "            # Define the output CSV filename\n",
    "            csv_name = f\"{name}.csv\"\n",
    "\n",
    "            # Encode SQL query and construct JSON API URL\n",
    "            encoded_sql = urllib.parse.quote(sql)\n",
    "            full_url = f\"{url}.json?sql={encoded_sql}&_shape=array\"\n",
    "            \n",
    "            print(f\"Fetching: {name} from SQL URL:\\n{full_url}\")\n",
    "\n",
    "            # Fetch JSON data and load into DataFrame\n",
    "            response = requests.get(full_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            print(f\"Rows returned: {len(data)}\")\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            # rename columns to match expected\n",
    "            df.rename(columns={'entry-date': 'entry_date'}, inplace=True)\n",
    "\n",
    "            # Save DataFrame to CSV in the specified directory\n",
    "            save_path = os.path.join(save_dir, csv_name)\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log failure and continue\n",
    "            print(f\"Failed to fetch from {url}: {e}\")\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command-line arguments for the output directory.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed arguments containing the output directory path.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Datasette batch exporter\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save exported CSVs\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse arguments from CLI\n",
    "    #args = parse_args()\n",
    "\n",
    "    # Define URLs and SQL queries to export\n",
    "    urls = {\n",
    "        \"operational_issues\": \"https://datasette.planning.data.gov.uk/digital-land\"\n",
    "    }\n",
    "\n",
    "    sqls = [\n",
    "        # SQL to count operational issues by week over the last 6 months\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            [entry-date],\n",
    "            COUNT(rowid) AS issue_count\n",
    "        FROM\n",
    "            operational_issue\n",
    "        WHERE\n",
    "            [entry-date] >= DATE('now', '-6 months')\n",
    "        GROUP BY\n",
    "            [entry-date];\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    # Execute the export\n",
    "    output_dir = \"C:/Users/DanielGodden/Documents/MCHLG/collecting_and_managing_data\"\n",
    "    sql_queried_datasette_tables(urls, sqls, output_dir)#args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
