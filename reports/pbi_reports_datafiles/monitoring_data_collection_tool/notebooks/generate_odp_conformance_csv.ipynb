{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c05b9ea",
   "metadata": {},
   "source": [
    "This script generates a conformance summary CSV for datasets published under the Open Digital Planning (ODP) project. It does so by integrating multiple sources of data from a Datasette instance, including expected provisions, endpoint summaries, and dataset specifications.\n",
    "\n",
    "### Detailed Overview:\n",
    "\n",
    "1. **Command-Line Configuration**:\n",
    "   The script uses `argparse` to receive an `--output-dir` argument, which specifies where the final CSV will be saved.\n",
    "\n",
    "2. **Dataset Filtering**:\n",
    "   A predefined list of relevant datasets (e.g., `conservation-area`, `listed-building-outline`, etc.) is used to restrict the scope of the analysis to specific pipelines.\n",
    "\n",
    "3. **Provisions Retrieval**:\n",
    "   The script queries the `provision`, `cohort`, and `organisation` tables to determine which datasets are expected for each organisation and cohort.\n",
    "\n",
    "4. **Dataset Specification Parsing**:\n",
    "   It loads a local CSV file (`specification.csv`) containing dataset definitions and expected fields. These are parsed into a DataFrame for matching against actual supplied fields.\n",
    "\n",
    "5. **Endpoint Data Extraction**:\n",
    "   The script paginates through the `endpoint_dataset_resource_summary` and joins with endpoint metadata and licences to retrieve a complete view of each endpointâ€™s status and content.\n",
    "\n",
    "6. **Field Matching Analysis**:\n",
    "   The script checks which fields were correctly mapped, supplied, or omitted by comparing the actual endpoint fields against those defined in the specification.\n",
    "\n",
    "7. **Metrics Calculation**:\n",
    "   For each dataset, it computes the total expected fields, matched fields, supplied fields, and calculates percentages for supplied and error-free fields.\n",
    "\n",
    "8. **Output**:\n",
    "   The final summary is written to a file named `odp_conformance_summary.csv` in the specified output directory, giving insights into data quality and completeness per organisation, cohort, and dataset.\n",
    "\n",
    "---\n",
    "\n",
    "This is used to monitor how well local authorities conform to the ODP dataset schema and whether they are supplying the expected content with sufficient coverage and correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a30d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Datasette query failed: HTTPSConnectionPool(host='datasette.planning.data.gov.uk', port=443): Max retries exceeded with url: /performance.json?sql=%0A++++++++SELECT+endpoint%2C+MAX%28entry_date%29+AS+latest_log_entry_date%0A++++++++FROM+log%0A++++++++GROUP+BY+endpoint%0A++++&_shape=array&_size=max (Caused by ResponseError('too many 400 error responses'))\n",
      "latest_logs_df.columns: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_24088\\2954844754.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# --- Config ---\u001b[39;00m\n\u001b[32m    180\u001b[39m     SPEC_PATH = \u001b[33mr\"C:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\documentation\\specification.csv\"\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m#args = parse_args()\u001b[39;00m\n\u001b[32m    182\u001b[39m     OUTPUT_DIR = \u001b[33mr\"C:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\"\u001b[39m\u001b[38;5;66;03m#args.output_dir\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     generate_odp_conformance_csv()\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_24088\\2954844754.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m         offset += \u001b[32m1000\u001b[39m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m     df = pd.concat(all_dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    139\u001b[39m     df = pd.merge(provision_df, df, on=[\u001b[33m\"organisation\"\u001b[39m, \u001b[33m\"cohort\"\u001b[39m], how=\u001b[33m\"inner\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     df = pd.merge(df, latest_logs_df, on=\u001b[33m\"endpoint\"\u001b[39m, how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Parse and clean fields\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m parse_fields(row, col):\n",
      "\u001b[32mc:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'endpoint'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to fetch, clean, and summarise conformance data from the Open Digital Planning (ODP) Datasette endpoints.\n",
    "It checks provisions, endpoint summaries, and issue logs against a specification, producing a comprehensive \n",
    "performance summary for each dataset per organisation.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import adapters\n",
    "from urllib3 import Retry\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Datasette batch exporter\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save exported CSVs\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_datasette_http():\n",
    "    \"\"\"\n",
    "    Creates and returns a requests.Session object with retry logic built in.\n",
    "    Used for robust querying of Datasette endpoints.\n",
    "    \"\"\"\n",
    "    retry_strategy = Retry(total=3, status_forcelist=[400], backoff_factor=0)\n",
    "\n",
    "    adapter = adapters.HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    return http\n",
    "\n",
    "\n",
    "def get_datasette_query(db, sql, filter=None, url=\"https://datasette.planning.data.gov.uk\"):\n",
    "    \"\"\"\n",
    "    Executes an SQL query against a Datasette database and returns the result as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        db (str): The database name (e.g. 'digital-land')\n",
    "        sql (str): SQL query string\n",
    "        filter (dict, optional): Additional query parameters\n",
    "        url (str): Base Datasette URL\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Query result as a DataFrame or None on failure\n",
    "    \"\"\"\n",
    "    url = f\"{url}/{db}.json\"\n",
    "    params = {\"sql\": sql, \"_shape\": \"array\", \"_size\": \"max\"}\n",
    "    if filter:\n",
    "        params.update(filter)\n",
    "    try:\n",
    "        http = get_datasette_http()\n",
    "        resp = http.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "        df = pd.DataFrame.from_dict(resp.json())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        return None\n",
    "\n",
    "def get_provisions(selected_cohorts, all_cohorts):\n",
    "    \"\"\"\n",
    "    Queries the 'provision' table for expected dataset provisions across selected cohorts.\n",
    "\n",
    "    Args:\n",
    "        selected_cohorts (list): Cohort IDs to include\n",
    "        all_cohorts (list): All cohort definitions to cross-check validity\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Provisions grouped by organisation and cohort\n",
    "    \"\"\"\n",
    "    filtered_cohorts = [\n",
    "        x\n",
    "        for x in selected_cohorts\n",
    "        if selected_cohorts[0] in [cohort[\"id\"] for cohort in all_cohorts]\n",
    "    ]\n",
    "    cohort_clause = (\n",
    "        \"AND (\"\n",
    "        + \" or \".join(\"c.cohort = '\" + str(n) + \"'\" for n in filtered_cohorts)\n",
    "        + \")\"\n",
    "        if filtered_cohorts\n",
    "        else \"\"\n",
    "    )\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        p.cohort,\n",
    "        p.organisation,\n",
    "        c.start_date as cohort_start_date,\n",
    "        org.name as name\n",
    "    FROM\n",
    "        provision p\n",
    "    INNER JOIN\n",
    "        cohort c on c.cohort = p.cohort\n",
    "    JOIN organisation org\n",
    "    WHERE\n",
    "        p.provision_reason = \"expected\"\n",
    "    AND p.project == \"open-digital-planning\"\n",
    "    {cohort_clause}\n",
    "    AND org.organisation == p.organisation\n",
    "    GROUP BY\n",
    "        p.organisation\n",
    "    ORDER BY\n",
    "        cohort_start_date,\n",
    "        p.cohort\n",
    "    \"\"\"\n",
    "    provision_df = get_datasette_query(\"digital-land\", sql)\n",
    "    return provision_df\n",
    "\n",
    "SPATIAL_DATASETS = [\n",
    "    \"article-4-direction-area\",\n",
    "    \"conservation-area\",\n",
    "    \"listed-building-outline\",\n",
    "    \"tree-preservation-zone\",\n",
    "    \"tree\",\n",
    "]\n",
    "DOCUMENT_DATASETS = [\n",
    "    \"article-4-direction\",\n",
    "    \"conservation-area-document\",\n",
    "    \"tree-preservation-order\",\n",
    "]\n",
    "\n",
    "# Separate variable for all datasets as arbitrary ordering required\n",
    "ALL_DATASETS = [\n",
    "    \"article-4-direction\",\n",
    "    \"article-4-direction-area\",\n",
    "    \"conservation-area\",\n",
    "    \"conservation-area-document\",\n",
    "    \"listed-building-outline\",\n",
    "    \"tree-preservation-order\",\n",
    "    \"tree-preservation-zone\",\n",
    "    \"tree\",\n",
    "]\n",
    "\n",
    "# Configs that are passed to the front end for the filters\n",
    "DATASET_TYPES = [\n",
    "    {\"name\": \"Spatial\", \"id\": \"spatial\"},\n",
    "    {\"name\": \"Document\", \"id\": \"document\"},\n",
    "]\n",
    "\n",
    "COHORTS = [\n",
    "    {\"name\": \"RIPA Beta\", \"id\": \"RIPA-Beta\"},\n",
    "    {\"name\": \"RIPA BOPS\", \"id\": \"RIPA-BOPS\"},\n",
    "    {\"name\": \"ODP Track 1\", \"id\": \"ODP-Track1\"},\n",
    "    {\"name\": \"ODP Track 2\", \"id\": \"ODP-Track2\"},\n",
    "    {\"name\": \"ODP Track 3\", \"id\": \"ODP-Track3\"},\n",
    "    {\"name\": \"ODP Track 4\", \"id\": \"ODP-Track4\"},\n",
    "]\n",
    "\n",
    "\n",
    "def get_column_field_summary(dataset_clause, offset):\n",
    "    sql = f\"\"\"\n",
    "    SELECT edrs.*, rle.licence\n",
    "    FROM endpoint_dataset_resource_summary AS edrs\n",
    "    LEFT JOIN (\n",
    "        SELECT endpoint, licence, dataset\n",
    "        FROM reporting_latest_endpoints\n",
    "    ) AS rle ON edrs.endpoint = rle.endpoint and edrs.dataset = rle.dataset\n",
    "    LEFT JOIN (\n",
    "        SELECT endpoint, end_date as endpoint_end_date, dataset\n",
    "        FROM endpoint_dataset_summary\n",
    "    ) as eds on edrs.endpoint = eds.endpoint and edrs.dataset = eds.dataset\n",
    "    WHERE edrs.resource != ''\n",
    "    and eds.endpoint_end_date=''\n",
    "    and ({dataset_clause})\n",
    "    limit 1000 offset {offset}\n",
    "    \"\"\"\n",
    "    column_field_df = get_datasette_query(\"performance\", sql)\n",
    "\n",
    "    return column_field_df\n",
    "\n",
    "\n",
    "def get_issue_summary(dataset_clause, offset):\n",
    "    sql = f\"\"\"\n",
    "    select  * from endpoint_dataset_issue_type_summary edrs\n",
    "    where ({dataset_clause})\n",
    "    limit 1000 offset {offset}\n",
    "    \"\"\"\n",
    "    issue_summary_df = get_datasette_query(\"performance\", sql)\n",
    "    return issue_summary_df\n",
    "\n",
    "\n",
    "def get_odp_conformance_summary(dataset_types, cohorts):\n",
    "    params = {\n",
    "        \"cohorts\": COHORTS,\n",
    "        \"dataset_types\": DATASET_TYPES,\n",
    "        \"selected_dataset_types\": dataset_types,\n",
    "        \"selected_cohorts\": cohorts,\n",
    "    }\n",
    "    if dataset_types == [\"spatial\"]:\n",
    "        datasets = SPATIAL_DATASETS\n",
    "    elif dataset_types == [\"document\"]:\n",
    "        datasets = DOCUMENT_DATASETS\n",
    "    else:\n",
    "        datasets = ALL_DATASETS\n",
    "    dataset_clause = \" or \".join(\n",
    "        (\"edrs.pipeline = '\" + str(dataset) + \"'\" for dataset in datasets)\n",
    "    )\n",
    "\n",
    "    provision_df = get_provisions(cohorts, COHORTS)\n",
    "\n",
    "    # Download column field summary table\n",
    "    # Use pagination in case rows returned > 1000\n",
    "    pagination_incomplete = True\n",
    "    offset = 0\n",
    "    column_field_df_list = []\n",
    "    while pagination_incomplete:\n",
    "        column_field_df = get_column_field_summary(dataset_clause, offset)\n",
    "        column_field_df_list.append(column_field_df)\n",
    "        pagination_incomplete = len(column_field_df) == 1000\n",
    "        offset += 1000\n",
    "    if len(column_field_df_list) == 0:\n",
    "        return {\"params\": params, \"rows\": [], \"headers\": []}\n",
    "    column_field_df = pd.concat(column_field_df_list)\n",
    "      \n",
    "    column_field_df = pd.merge(\n",
    "        column_field_df, provision_df, on=[\"organisation\", \"cohort\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Optional: Fill missing names or dates (if helpful for display/export)\n",
    "    column_field_df[\"organisation_name\"] = column_field_df[\"organisation_name\"].fillna(\"Unknown\")\n",
    "    column_field_df[\"cohort_start_date\"] = column_field_df[\"cohort_start_date\"].fillna(\"\")\n",
    "\n",
    "    # Download issue summary table\n",
    "    pagination_incomplete = True\n",
    "    offset = 0\n",
    "    issue_df_list = []\n",
    "    while pagination_incomplete:\n",
    "        issue_df = get_issue_summary(dataset_clause, offset)\n",
    "        issue_df_list.append(issue_df)\n",
    "        pagination_incomplete = len(issue_df) == 1000\n",
    "        offset += 1000\n",
    "    issue_df = pd.concat(issue_df_list)\n",
    "\n",
    "    dataset_field_df = get_dataset_field()\n",
    "\n",
    "    # remove fields that are auto-created in the pipeline from the dataset_field file to avoid mis-counting\n",
    "    # (\"entity\", \"organisation\", \"prefix\", \"point\" for all but tree, and \"entity\", \"organisation\", \"prefix\" for tree)\n",
    "    dataset_field_df = dataset_field_df[\n",
    "        (dataset_field_df[\"dataset\"] != \"tree\")\n",
    "        & (\n",
    "            ~dataset_field_df[\"field\"].isin(\n",
    "                [\"entity\", \"organisation\", \"prefix\", \"point\"]\n",
    "            )\n",
    "        )\n",
    "        | (dataset_field_df[\"dataset\"] == \"tree\")\n",
    "        & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\"]))\n",
    "    ]\n",
    "\n",
    "    # Filter out fields not in spec\n",
    "    column_field_df[\"mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n",
    "        lambda row: [\n",
    "            field\n",
    "            for field in (\n",
    "                row[\"mapping_field\"].split(\";\") if row[\"mapping_field\"] else \"\"\n",
    "            )\n",
    "            if field\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    column_field_df[\"non_mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n",
    "        lambda row: [\n",
    "            field\n",
    "            for field in (\n",
    "                row[\"non_mapping_field\"].split(\";\") if row[\"non_mapping_field\"] else \"\"\n",
    "            )\n",
    "            if field\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Map entity errors to reference field\n",
    "    issue_df[\"field\"] = issue_df[\"field\"].replace(\"entity\", \"reference\")\n",
    "    # Filter out issues for fields not in dataset field (specification)\n",
    "    issue_df[\"field\"] = issue_df.apply(\n",
    "        lambda row: (\n",
    "            row[\"field\"]\n",
    "            if row[\"field\"]\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "            else None\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Create field matched and field supplied scores\n",
    "    column_field_df[\"field_matched\"] = column_field_df.apply(\n",
    "        lambda row: len(row[\"mapping_field\"]) if row[\"mapping_field\"] else 0, axis=1\n",
    "    )\n",
    "    column_field_df[\"field_supplied\"] = column_field_df.apply(\n",
    "        lambda row: row[\"field_matched\"]\n",
    "        + (len(row[\"non_mapping_field\"]) if row[\"non_mapping_field\"] else 0),\n",
    "        axis=1,\n",
    "    )\n",
    "    column_field_df[\"field\"] = column_field_df.apply(\n",
    "        lambda row: len(\n",
    "            dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Check for fields which have error issues\n",
    "    results_issues = [\n",
    "        issue_df[\n",
    "            (issue_df[\"resource\"] == r[\"resource\"]) & (issue_df[\"severity\"] == \"error\")\n",
    "        ]\n",
    "        for index, r in column_field_df.iterrows()\n",
    "    ]\n",
    "    results_issues_df = pd.concat(results_issues)\n",
    "\n",
    "    # Create fields with errors column\n",
    "    column_field_df[\"field_errors\"] = column_field_df.apply(\n",
    "        lambda row: len(\n",
    "            results_issues_df[row[\"resource\"] == results_issues_df[\"resource\"]]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Create endpoint ID column to track multiple endpoints per organisation-dataset\n",
    "    column_field_df[\"endpoint_no.\"] = (\n",
    "        column_field_df.groupby([\"organisation\", \"dataset\"]).cumcount() + 1\n",
    "    )\n",
    "    column_field_df[\"endpoint_no.\"] = column_field_df[\"endpoint_no.\"].astype(str)\n",
    "\n",
    "    # group by and aggregate for final summaries\n",
    "    final_count = (\n",
    "        column_field_df.groupby(\n",
    "            [\n",
    "                \"organisation\",\n",
    "                \"organisation_name\",\n",
    "                \"cohort\",\n",
    "                \"dataset\",\n",
    "                \"licence\",\n",
    "                \"endpoint\",\n",
    "                \"endpoint_no.\",\n",
    "                \"resource\",\n",
    "                \"latest_log_entry_date\",\n",
    "                \"cohort_start_date\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"field\": \"sum\",\n",
    "                \"field_supplied\": \"sum\",\n",
    "                \"field_matched\": \"sum\",\n",
    "                \"field_errors\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    final_count[\"field_error_free\"] = (\n",
    "        final_count[\"field_supplied\"] - final_count[\"field_errors\"]\n",
    "    )\n",
    "    final_count[\"field_error_free\"] = final_count[\"field_error_free\"].replace(-1, 0)\n",
    "\n",
    "    # add string fields for [n fields]/[total fields] style counts\n",
    "    final_count[\"field_supplied_count\"] = (\n",
    "        final_count[\"field_supplied\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "    final_count[\"field_error_free_count\"] = (\n",
    "        final_count[\"field_error_free\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "    final_count[\"field_matched_count\"] = (\n",
    "        final_count[\"field_matched\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "\n",
    "    # create % columns\n",
    "    final_count[\"field_supplied_pct\"] = (\n",
    "        final_count[\"field_supplied\"] / final_count[\"field\"]\n",
    "    )\n",
    "    final_count[\"field_error_free_pct\"] = (\n",
    "        final_count[\"field_error_free\"] / final_count[\"field\"]\n",
    "    )\n",
    "    final_count[\"field_matched_pct\"] = (\n",
    "        final_count[\"field_matched\"] / final_count[\"field\"]\n",
    "    )\n",
    "\n",
    "    final_count.reset_index(drop=True, inplace=True)\n",
    "    final_count.sort_values(\n",
    "        [\"cohort_start_date\", \"cohort\", \"organisation_name\", \"dataset\"], inplace=True\n",
    "    )\n",
    "\n",
    "    provisions_with_100_pct_match = final_count[final_count[\"field_matched_pct\"] == 1.0]\n",
    "    percent_100_field_match = (\n",
    "        round(len(provisions_with_100_pct_match) / len(final_count) * 100, 1)\n",
    "        if len(final_count)\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    out_cols = [\n",
    "        \"cohort\",\n",
    "        \"organisation_name\",\n",
    "        \"organisation\",\n",
    "        \"dataset\",\n",
    "        \"licence\",\n",
    "        \"endpoint_no.\",\n",
    "        \"field_supplied_count\",\n",
    "        \"field_supplied_pct\",\n",
    "        \"field_matched_count\",\n",
    "        \"field_matched_pct\",\n",
    "    ]\n",
    "\n",
    "    csv_out_cols = [\n",
    "        \"organisation\",\n",
    "        \"organisation_name\",\n",
    "        \"cohort\",\n",
    "        \"dataset\",\n",
    "        \"licence\",\n",
    "        \"endpoint\",\n",
    "        \"endpoint_no.\",\n",
    "        \"resource\",\n",
    "        \"latest_log_entry_date\",\n",
    "        \"field\",\n",
    "        \"field_supplied\",\n",
    "        \"field_matched\",\n",
    "        \"field_errors\",\n",
    "        \"field_error_free\",\n",
    "        \"field_supplied_pct\",\n",
    "        \"field_error_free_pct\",\n",
    "        \"field_matched_pct\",\n",
    "    ]\n",
    "\n",
    "    headers = [\n",
    "        *map(\n",
    "            lambda column: {\n",
    "                \"text\": make_pretty(column).title(),\n",
    "                \"classes\": \"reporting-table-header\",\n",
    "            },\n",
    "            out_cols,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rows = [\n",
    "        [\n",
    "            {\n",
    "                \"text\": make_pretty(cell),\n",
    "                \"classes\": \"reporting-table-cell \" + get_background_class(cell),\n",
    "            }\n",
    "            for cell in r\n",
    "        ]\n",
    "        for index, r in final_count[out_cols].iterrows()\n",
    "    ]\n",
    "\n",
    "    # Calculate overview stats\n",
    "    overview_datasets = [\n",
    "        \"article-4-direction-area\",\n",
    "        \"conservation-area\",\n",
    "        \"listed-building-outline\",\n",
    "        \"tree\",\n",
    "        \"tree-preservation-zone\",\n",
    "    ]\n",
    "    overview_stats_df = pd.DataFrame()\n",
    "    overview_stats_df[\"dataset\"] = overview_datasets\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            final_count[\"field_supplied_pct\"] < 0.5\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"< 50%\"})\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            (final_count[\"field_supplied_pct\"] >= 0.5)\n",
    "            & (final_count[\"field_supplied_pct\"] < 0.8)\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"50% - 80%\"})\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            final_count[\"field_supplied_pct\"] >= 0.8\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"> 80%\"})\n",
    "    overview_stats_df.replace(np.nan, 0, inplace=True)\n",
    "    overview_stats_df = overview_stats_df.astype(\n",
    "        {\n",
    "            \"< 50%\": int,\n",
    "            \"50% - 80%\": int,\n",
    "            \"> 80%\": int,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    stats_headers = [\n",
    "        *map(\n",
    "            lambda column: {\n",
    "                \"text\": column.title(),\n",
    "                \"classes\": \"reporting-table-header\",\n",
    "            },\n",
    "            overview_stats_df.columns.values,\n",
    "        )\n",
    "    ]\n",
    "    stats_rows = [\n",
    "        [{\"text\": cell, \"classes\": \"reporting-table-cell\"} for cell in r]\n",
    "        for index, r in overview_stats_df.iterrows()\n",
    "    ]\n",
    "    return {\n",
    "        \"headers\": headers,\n",
    "        \"rows\": rows,\n",
    "        \"stats_headers\": stats_headers,\n",
    "        \"stats_rows\": stats_rows,\n",
    "        \"params\": params,\n",
    "        \"percent_100_field_match\": percent_100_field_match,\n",
    "    }, final_count[csv_out_cols]\n",
    "\n",
    "\n",
    "def make_pretty(text):\n",
    "    if type(text) is float:\n",
    "        # text is a float, make a percentage\n",
    "        return str((round(100 * text))) + \"%\"\n",
    "    elif \"_\" in text:\n",
    "        # text is a column name\n",
    "        return text.replace(\"_\", \" \").replace(\"pct\", \"%\").replace(\"count\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_background_class(text):\n",
    "    if type(text) is float:\n",
    "        group = int((text * 100) / 10)\n",
    "        if group == 10:\n",
    "            return \"reporting-100-background\"\n",
    "        else:\n",
    "            return \"reporting-\" + str(group) + \"0-\" + str(group + 1) + \"0-background\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_dataset_field():\n",
    "    specification_df = pd.read_csv(\n",
    "        r\"C:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\documentation\\specification.csv\"\n",
    "    )\n",
    "    rows = []\n",
    "    for index, row in specification_df.iterrows():\n",
    "        specification_dicts = json.loads(row[\"json\"])\n",
    "        for dict in specification_dicts:\n",
    "            dataset = dict[\"dataset\"]\n",
    "            fields = [field[\"field\"] for field in dict[\"fields\"]]\n",
    "            for field in fields:\n",
    "                rows.append({\"dataset\": dataset, \"field\": field})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = parse_args()\n",
    "\n",
    "    output_dir = args.output_dir \n",
    "    output_path = os.path.join(output_dir, \"odp-conformance.csv\")\n",
    "\n",
    "    # Run the function and get the DataFrame\n",
    "    _, df = get_odp_conformance_summary(dataset_types=[\"spatial\", \"document\"], cohorts=[\"ODP-Track1\", \"ODP-Track2\", \"ODP-Track3\", \"ODP-Track4\"])\n",
    "    df = df[df['cohort'].notna() & \n",
    "            (df['cohort'].str.strip() != \"\")]\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved ODP conformance summary to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
