{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18b889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielGodden\\AppData\\Local\\Temp\\ipykernel_18312\\1476547831.py:222: FutureWarning: DataFrame.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  column_field_df[\"mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n",
      "C:\\Users\\DanielGodden\\AppData\\Local\\Temp\\ipykernel_18312\\1476547831.py:235: FutureWarning: DataFrame.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  column_field_df[\"non_mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ODP conformance summary to C:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\odp-conformance.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to fetch, clean, and summarise conformance data from the Open Digital Planning (ODP) Datasette endpoints.\n",
    "It checks provisions, endpoint summaries, and issue logs against a specification, producing a comprehensive \n",
    "performance summary for each dataset per organisation.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import adapters\n",
    "from urllib3 import Retry\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Datasette batch exporter\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save exported CSVs\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_datasette_http():\n",
    "    \"\"\"\n",
    "    Creates and returns a requests.Session object with retry logic built in.\n",
    "    Used for robust querying of Datasette endpoints.\n",
    "    \"\"\"\n",
    "    retry_strategy = Retry(total=3, status_forcelist=[400], backoff_factor=0)\n",
    "\n",
    "    adapter = adapters.HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    return http\n",
    "\n",
    "\n",
    "def get_datasette_query(db, sql, filter=None, url=\"https://datasette.planning.data.gov.uk\"):\n",
    "    \"\"\"\n",
    "    Executes an SQL query against a Datasette database and returns the result as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        db (str): The database name (e.g. 'digital-land')\n",
    "        sql (str): SQL query string\n",
    "        filter (dict, optional): Additional query parameters\n",
    "        url (str): Base Datasette URL\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: Query result as a DataFrame or None on failure\n",
    "    \"\"\"\n",
    "    url = f\"{url}/{db}.json\"\n",
    "    params = {\"sql\": sql, \"_shape\": \"array\", \"_size\": \"max\"}\n",
    "    if filter:\n",
    "        params.update(filter)\n",
    "    try:\n",
    "        http = get_datasette_http()\n",
    "        resp = http.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "        df = pd.DataFrame.from_dict(resp.json())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        return None\n",
    "\n",
    "def get_provisions(selected_cohorts, all_cohorts):\n",
    "    \"\"\"\n",
    "    Queries the 'provision' table for expected dataset provisions across selected cohorts.\n",
    "\n",
    "    Args:\n",
    "        selected_cohorts (list): Cohort IDs to include\n",
    "        all_cohorts (list): All cohort definitions to cross-check validity\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Provisions grouped by organisation and cohort\n",
    "    \"\"\"\n",
    "    filtered_cohorts = [\n",
    "        x\n",
    "        for x in selected_cohorts\n",
    "        if selected_cohorts[0] in [cohort[\"id\"] for cohort in all_cohorts]\n",
    "    ]\n",
    "    cohort_clause = (\n",
    "        \"AND (\"\n",
    "        + \" or \".join(\"c.cohort = '\" + str(n) + \"'\" for n in filtered_cohorts)\n",
    "        + \")\"\n",
    "        if filtered_cohorts\n",
    "        else \"\"\n",
    "    )\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        p.cohort,\n",
    "        p.organisation,\n",
    "        c.start_date as cohort_start_date,\n",
    "        org.name as name\n",
    "    FROM\n",
    "        provision p\n",
    "    INNER JOIN\n",
    "        cohort c on c.cohort = p.cohort\n",
    "    JOIN organisation org\n",
    "    WHERE\n",
    "        p.provision_reason = \"expected\"\n",
    "    AND p.project == \"open-digital-planning\"\n",
    "    {cohort_clause}\n",
    "    AND org.organisation == p.organisation\n",
    "    GROUP BY\n",
    "        p.organisation\n",
    "    ORDER BY\n",
    "        cohort_start_date,\n",
    "        p.cohort\n",
    "    \"\"\"\n",
    "    provision_df = get_datasette_query(\"digital-land\", sql)\n",
    "    return provision_df\n",
    "\n",
    "SPATIAL_DATASETS = [\n",
    "    \"article-4-direction-area\",\n",
    "    \"conservation-area\",\n",
    "    \"listed-building-outline\",\n",
    "    \"tree-preservation-zone\",\n",
    "    \"tree\",\n",
    "]\n",
    "DOCUMENT_DATASETS = [\n",
    "    \"article-4-direction\",\n",
    "    \"conservation-area-document\",\n",
    "    \"tree-preservation-order\",\n",
    "]\n",
    "\n",
    "# Separate variable for all datasets as arbitrary ordering required\n",
    "ALL_DATASETS = [\n",
    "    \"article-4-direction\",\n",
    "    \"article-4-direction-area\",\n",
    "    \"conservation-area\",\n",
    "    \"conservation-area-document\",\n",
    "    \"listed-building-outline\",\n",
    "    \"tree-preservation-order\",\n",
    "    \"tree-preservation-zone\",\n",
    "    \"tree\",\n",
    "]\n",
    "\n",
    "# Configs that are passed to the front end for the filters\n",
    "DATASET_TYPES = [\n",
    "    {\"name\": \"Spatial\", \"id\": \"spatial\"},\n",
    "    {\"name\": \"Document\", \"id\": \"document\"},\n",
    "]\n",
    "\n",
    "COHORTS = [\n",
    "    {\"name\": \"RIPA Beta\", \"id\": \"RIPA-Beta\"},\n",
    "    {\"name\": \"RIPA BOPS\", \"id\": \"RIPA-BOPS\"},\n",
    "    {\"name\": \"ODP Track 1\", \"id\": \"ODP-Track1\"},\n",
    "    {\"name\": \"ODP Track 2\", \"id\": \"ODP-Track2\"},\n",
    "    {\"name\": \"ODP Track 3\", \"id\": \"ODP-Track3\"},\n",
    "    {\"name\": \"ODP Track 4\", \"id\": \"ODP-Track4\"},\n",
    "]\n",
    "\n",
    "\n",
    "def get_column_field_summary(dataset_clause, offset):\n",
    "    sql = f\"\"\"\n",
    "    SELECT edrs.*, rle.licence\n",
    "    FROM endpoint_dataset_resource_summary AS edrs\n",
    "    LEFT JOIN (\n",
    "        SELECT endpoint, licence, dataset\n",
    "        FROM reporting_latest_endpoints\n",
    "    ) AS rle ON edrs.endpoint = rle.endpoint and edrs.dataset = rle.dataset\n",
    "    LEFT JOIN (\n",
    "        SELECT endpoint, end_date as endpoint_end_date, dataset\n",
    "        FROM endpoint_dataset_summary\n",
    "    ) as eds on edrs.endpoint = eds.endpoint and edrs.dataset = eds.dataset\n",
    "    WHERE edrs.resource != ''\n",
    "    and eds.endpoint_end_date=''\n",
    "    and ({dataset_clause})\n",
    "    limit 1000 offset {offset}\n",
    "    \"\"\"\n",
    "    column_field_df = get_datasette_query(\"performance\", sql)\n",
    "\n",
    "    return column_field_df\n",
    "\n",
    "\n",
    "def get_issue_summary(dataset_clause, offset):\n",
    "    sql = f\"\"\"\n",
    "    select  * from endpoint_dataset_issue_type_summary edrs\n",
    "    where ({dataset_clause})\n",
    "    limit 1000 offset {offset}\n",
    "    \"\"\"\n",
    "    issue_summary_df = get_datasette_query(\"performance\", sql)\n",
    "    return issue_summary_df\n",
    "\n",
    "\n",
    "def get_odp_conformance_summary(dataset_types, cohorts):\n",
    "    params = {\n",
    "        \"cohorts\": COHORTS,\n",
    "        \"dataset_types\": DATASET_TYPES,\n",
    "        \"selected_dataset_types\": dataset_types,\n",
    "        \"selected_cohorts\": cohorts,\n",
    "    }\n",
    "    if dataset_types == [\"spatial\"]:\n",
    "        datasets = SPATIAL_DATASETS\n",
    "    elif dataset_types == [\"document\"]:\n",
    "        datasets = DOCUMENT_DATASETS\n",
    "    else:\n",
    "        datasets = ALL_DATASETS\n",
    "    dataset_clause = \" or \".join(\n",
    "        (\"edrs.pipeline = '\" + str(dataset) + \"'\" for dataset in datasets)\n",
    "    )\n",
    "\n",
    "    provision_df = get_provisions(cohorts, COHORTS)\n",
    "\n",
    "    # Download column field summary table\n",
    "    # Use pagination in case rows returned > 1000\n",
    "    pagination_incomplete = True\n",
    "    offset = 0\n",
    "    column_field_df_list = []\n",
    "    while pagination_incomplete:\n",
    "        column_field_df = get_column_field_summary(dataset_clause, offset)\n",
    "        column_field_df_list.append(column_field_df)\n",
    "        pagination_incomplete = len(column_field_df) == 1000\n",
    "        offset += 1000\n",
    "    if len(column_field_df_list) == 0:\n",
    "        return {\"params\": params, \"rows\": [], \"headers\": []}\n",
    "    column_field_df = pd.concat(column_field_df_list)\n",
    "      \n",
    "    column_field_df = pd.merge(\n",
    "        column_field_df, provision_df, on=[\"organisation\", \"cohort\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Optional: Fill missing names or dates (if helpful for display/export)\n",
    "    column_field_df[\"organisation_name\"] = column_field_df[\"organisation_name\"].fillna(\"Unknown\")\n",
    "    column_field_df[\"cohort_start_date\"] = column_field_df[\"cohort_start_date\"].fillna(\"\")\n",
    "\n",
    "    # Download issue summary table\n",
    "    pagination_incomplete = True\n",
    "    offset = 0\n",
    "    issue_df_list = []\n",
    "    while pagination_incomplete:\n",
    "        issue_df = get_issue_summary(dataset_clause, offset)\n",
    "        issue_df_list.append(issue_df)\n",
    "        pagination_incomplete = len(issue_df) == 1000\n",
    "        offset += 1000\n",
    "    issue_df = pd.concat(issue_df_list)\n",
    "\n",
    "    dataset_field_df = get_dataset_field()\n",
    "\n",
    "    # remove fields that are auto-created in the pipeline from the dataset_field file to avoid mis-counting\n",
    "    # (\"entity\", \"organisation\", \"prefix\", \"point\" for all but tree, and \"entity\", \"organisation\", \"prefix\" for tree)\n",
    "    dataset_field_df = dataset_field_df[\n",
    "        (dataset_field_df[\"dataset\"] != \"tree\")\n",
    "        & (\n",
    "            ~dataset_field_df[\"field\"].isin(\n",
    "                [\"entity\", \"organisation\", \"prefix\", \"point\"]\n",
    "            )\n",
    "        )\n",
    "        | (dataset_field_df[\"dataset\"] == \"tree\")\n",
    "        & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\"]))\n",
    "    ]\n",
    "\n",
    "    # Filter out fields not in spec\n",
    "    column_field_df[\"mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n",
    "        lambda row: [\n",
    "            field\n",
    "            for field in (\n",
    "                row[\"mapping_field\"].split(\";\") if row[\"mapping_field\"] else \"\"\n",
    "            )\n",
    "            if field\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    column_field_df[\"non_mapping_field\"] = column_field_df.replace({'\"', \"\"}).apply(\n",
    "        lambda row: [\n",
    "            field\n",
    "            for field in (\n",
    "                row[\"non_mapping_field\"].split(\";\") if row[\"non_mapping_field\"] else \"\"\n",
    "            )\n",
    "            if field\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Map entity errors to reference field\n",
    "    issue_df[\"field\"] = issue_df[\"field\"].replace(\"entity\", \"reference\")\n",
    "    # Filter out issues for fields not in dataset field (specification)\n",
    "    issue_df[\"field\"] = issue_df.apply(\n",
    "        lambda row: (\n",
    "            row[\"field\"]\n",
    "            if row[\"field\"]\n",
    "            in dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]][\n",
    "                \"field\"\n",
    "            ].values\n",
    "            else None\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Create field matched and field supplied scores\n",
    "    column_field_df[\"field_matched\"] = column_field_df.apply(\n",
    "        lambda row: len(row[\"mapping_field\"]) if row[\"mapping_field\"] else 0, axis=1\n",
    "    )\n",
    "    column_field_df[\"field_supplied\"] = column_field_df.apply(\n",
    "        lambda row: row[\"field_matched\"]\n",
    "        + (len(row[\"non_mapping_field\"]) if row[\"non_mapping_field\"] else 0),\n",
    "        axis=1,\n",
    "    )\n",
    "    column_field_df[\"field\"] = column_field_df.apply(\n",
    "        lambda row: len(\n",
    "            dataset_field_df[dataset_field_df[\"dataset\"] == row[\"dataset\"]]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Check for fields which have error issues\n",
    "    results_issues = [\n",
    "        issue_df[\n",
    "            (issue_df[\"resource\"] == r[\"resource\"]) & (issue_df[\"severity\"] == \"error\")\n",
    "        ]\n",
    "        for index, r in column_field_df.iterrows()\n",
    "    ]\n",
    "    results_issues_df = pd.concat(results_issues)\n",
    "\n",
    "    # Create fields with errors column\n",
    "    column_field_df[\"field_errors\"] = column_field_df.apply(\n",
    "        lambda row: len(\n",
    "            results_issues_df[row[\"resource\"] == results_issues_df[\"resource\"]]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Create endpoint ID column to track multiple endpoints per organisation-dataset\n",
    "    column_field_df[\"endpoint_no.\"] = (\n",
    "        column_field_df.groupby([\"organisation\", \"dataset\"]).cumcount() + 1\n",
    "    )\n",
    "    column_field_df[\"endpoint_no.\"] = column_field_df[\"endpoint_no.\"].astype(str)\n",
    "\n",
    "    # group by and aggregate for final summaries\n",
    "    final_count = (\n",
    "        column_field_df.groupby(\n",
    "            [\n",
    "                \"organisation\",\n",
    "                \"organisation_name\",\n",
    "                \"cohort\",\n",
    "                \"dataset\",\n",
    "                \"licence\",\n",
    "                \"endpoint\",\n",
    "                \"endpoint_no.\",\n",
    "                \"resource\",\n",
    "                \"latest_log_entry_date\",\n",
    "                \"cohort_start_date\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"field\": \"sum\",\n",
    "                \"field_supplied\": \"sum\",\n",
    "                \"field_matched\": \"sum\",\n",
    "                \"field_errors\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    final_count[\"field_error_free\"] = (\n",
    "        final_count[\"field_supplied\"] - final_count[\"field_errors\"]\n",
    "    )\n",
    "    final_count[\"field_error_free\"] = final_count[\"field_error_free\"].replace(-1, 0)\n",
    "\n",
    "    # add string fields for [n fields]/[total fields] style counts\n",
    "    final_count[\"field_supplied_count\"] = (\n",
    "        final_count[\"field_supplied\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "    final_count[\"field_error_free_count\"] = (\n",
    "        final_count[\"field_error_free\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "    final_count[\"field_matched_count\"] = (\n",
    "        final_count[\"field_matched\"].astype(int).map(str)\n",
    "        + \"/\"\n",
    "        + final_count[\"field\"].map(str)\n",
    "    )\n",
    "\n",
    "    # create % columns\n",
    "    final_count[\"field_supplied_pct\"] = (\n",
    "        final_count[\"field_supplied\"] / final_count[\"field\"]\n",
    "    )\n",
    "    final_count[\"field_error_free_pct\"] = (\n",
    "        final_count[\"field_error_free\"] / final_count[\"field\"]\n",
    "    )\n",
    "    final_count[\"field_matched_pct\"] = (\n",
    "        final_count[\"field_matched\"] / final_count[\"field\"]\n",
    "    )\n",
    "\n",
    "    final_count.reset_index(drop=True, inplace=True)\n",
    "    final_count.sort_values(\n",
    "        [\"cohort_start_date\", \"cohort\", \"organisation_name\", \"dataset\"], inplace=True\n",
    "    )\n",
    "\n",
    "    provisions_with_100_pct_match = final_count[final_count[\"field_matched_pct\"] == 1.0]\n",
    "    percent_100_field_match = (\n",
    "        round(len(provisions_with_100_pct_match) / len(final_count) * 100, 1)\n",
    "        if len(final_count)\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    out_cols = [\n",
    "        \"cohort\",\n",
    "        \"organisation_name\",\n",
    "        \"organisation\",\n",
    "        \"dataset\",\n",
    "        \"licence\",\n",
    "        \"endpoint_no.\",\n",
    "        \"field_supplied_count\",\n",
    "        \"field_supplied_pct\",\n",
    "        \"field_matched_count\",\n",
    "        \"field_matched_pct\",\n",
    "    ]\n",
    "\n",
    "    csv_out_cols = [\n",
    "        \"organisation\",\n",
    "        \"organisation_name\",\n",
    "        \"cohort\",\n",
    "        \"dataset\",\n",
    "        \"licence\",\n",
    "        \"endpoint\",\n",
    "        \"endpoint_no.\",\n",
    "        \"resource\",\n",
    "        \"latest_log_entry_date\",\n",
    "        \"field\",\n",
    "        \"field_supplied\",\n",
    "        \"field_matched\",\n",
    "        \"field_errors\",\n",
    "        \"field_error_free\",\n",
    "        \"field_supplied_pct\",\n",
    "        \"field_error_free_pct\",\n",
    "        \"field_matched_pct\",\n",
    "    ]\n",
    "\n",
    "    headers = [\n",
    "        *map(\n",
    "            lambda column: {\n",
    "                \"text\": make_pretty(column).title(),\n",
    "                \"classes\": \"reporting-table-header\",\n",
    "            },\n",
    "            out_cols,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    rows = [\n",
    "        [\n",
    "            {\n",
    "                \"text\": make_pretty(cell),\n",
    "                \"classes\": \"reporting-table-cell \" + get_background_class(cell),\n",
    "            }\n",
    "            for cell in r\n",
    "        ]\n",
    "        for index, r in final_count[out_cols].iterrows()\n",
    "    ]\n",
    "\n",
    "    # Calculate overview stats\n",
    "    overview_datasets = [\n",
    "        \"article-4-direction-area\",\n",
    "        \"conservation-area\",\n",
    "        \"listed-building-outline\",\n",
    "        \"tree\",\n",
    "        \"tree-preservation-zone\",\n",
    "    ]\n",
    "    overview_stats_df = pd.DataFrame()\n",
    "    overview_stats_df[\"dataset\"] = overview_datasets\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            final_count[\"field_supplied_pct\"] < 0.5\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"< 50%\"})\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            (final_count[\"field_supplied_pct\"] >= 0.5)\n",
    "            & (final_count[\"field_supplied_pct\"] < 0.8)\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"50% - 80%\"})\n",
    "    overview_stats_df = overview_stats_df.merge(\n",
    "        final_count[[\"dataset\", \"field_supplied_pct\"]][\n",
    "            final_count[\"field_supplied_pct\"] >= 0.8\n",
    "        ]\n",
    "        .groupby(\"dataset\")\n",
    "        .count(),\n",
    "        on=\"dataset\",\n",
    "        how=\"left\",\n",
    "    ).rename(columns={\"field_supplied_pct\": \"> 80%\"})\n",
    "    overview_stats_df.replace(np.nan, 0, inplace=True)\n",
    "    overview_stats_df = overview_stats_df.astype(\n",
    "        {\n",
    "            \"< 50%\": int,\n",
    "            \"50% - 80%\": int,\n",
    "            \"> 80%\": int,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    stats_headers = [\n",
    "        *map(\n",
    "            lambda column: {\n",
    "                \"text\": column.title(),\n",
    "                \"classes\": \"reporting-table-header\",\n",
    "            },\n",
    "            overview_stats_df.columns.values,\n",
    "        )\n",
    "    ]\n",
    "    stats_rows = [\n",
    "        [{\"text\": cell, \"classes\": \"reporting-table-cell\"} for cell in r]\n",
    "        for index, r in overview_stats_df.iterrows()\n",
    "    ]\n",
    "    return {\n",
    "        \"headers\": headers,\n",
    "        \"rows\": rows,\n",
    "        \"stats_headers\": stats_headers,\n",
    "        \"stats_rows\": stats_rows,\n",
    "        \"params\": params,\n",
    "        \"percent_100_field_match\": percent_100_field_match,\n",
    "    }, final_count[csv_out_cols]\n",
    "\n",
    "\n",
    "def make_pretty(text):\n",
    "    if type(text) is float:\n",
    "        # text is a float, make a percentage\n",
    "        return str((round(100 * text))) + \"%\"\n",
    "    elif \"_\" in text:\n",
    "        # text is a column name\n",
    "        return text.replace(\"_\", \" \").replace(\"pct\", \"%\").replace(\"count\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_background_class(text):\n",
    "    if type(text) is float:\n",
    "        group = int((text * 100) / 10)\n",
    "        if group == 10:\n",
    "            return \"reporting-100-background\"\n",
    "        else:\n",
    "            return \"reporting-\" + str(group) + \"0-\" + str(group + 1) + \"0-background\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_dataset_field():\n",
    "    specification_df = pd.read_csv(\n",
    "        r\"C:\\Users\\DanielGodden\\Documents\\MCHLG\\collecting_and_managing_data\\documentation\\specification.csv\"\n",
    "    )\n",
    "    rows = []\n",
    "    for index, row in specification_df.iterrows():\n",
    "        specification_dicts = json.loads(row[\"json\"])\n",
    "        for dict in specification_dicts:\n",
    "            dataset = dict[\"dataset\"]\n",
    "            fields = [field[\"field\"] for field in dict[\"fields\"]]\n",
    "            for field in fields:\n",
    "                rows.append({\"dataset\": dataset, \"field\": field})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = parse_args()\n",
    "\n",
    "    output_dir = args.output_dir \n",
    "    output_path = os.path.join(output_dir, \"odp-conformance.csv\")\n",
    "\n",
    "    # Run the function and get the DataFrame\n",
    "    _, df = get_odp_conformance_summary(dataset_types=[\"spatial\", \"document\"], cohorts=[\"ODP-Track1\", \"ODP-Track2\", \"ODP-Track3\", \"ODP-Track4\"])\n",
    "    df = df[df['cohort'].notna() & \n",
    "            (df['cohort'].str.strip() != \"\")]\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved ODP conformance summary to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
