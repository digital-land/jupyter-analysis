{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f3dc3a",
   "metadata": {},
   "source": [
    "This script generates a comprehensive **issue summary CSV** for datasets published under the **Open Digital Planning (ODP)** project. It works by querying two key sources from the [Datasette Planning instance](https://datasette.planning.data.gov.uk):\n",
    "\n",
    "1. **Expected dataset provisions** (what each organisation should supply).\n",
    "2. **Provision summary with issue counts** (including errors, warnings, and notices).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Functionality:\n",
    "\n",
    "- **Dataset Types**:\n",
    "  - `SPATIAL_DATASETS` (e.g., `tree`, `conservation-area`)\n",
    "  - `DOCUMENT_DATASETS` (e.g., `tree-preservation-order`)\n",
    "  - These can be queried individually or combined (`ALL_DATASETS`).\n",
    "\n",
    "- **Reliable Datasette Queries**:\n",
    "  - A `requests` session with retry logic ensures resilient HTTP querying from the Datasette API.\n",
    "\n",
    "- **Provision Metadata**:\n",
    "  - `get_provisions()` collects the expected datasets per organisation and cohort.\n",
    "  \n",
    "- **Issue Summaries**:\n",
    "  - The script paginates through the `provision_summary` table, fetching key statistics like:\n",
    "    - `active_endpoint_count`\n",
    "    - `error_endpoint_count`\n",
    "    - `count_issue_error_*`, `warning_*`, and `notice_*` by internal/external source\n",
    "\n",
    "- **Merging and Reshaping**:\n",
    "  - The script merges expected provisions with issue statistics and restructures the data into a clean per-dataset format.\n",
    "\n",
    "- **Output**:\n",
    "  - A single CSV file, `odp-issue-summary.csv`, is generated in the provided output directory, detailing the issue breakdown for each organisation and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65d9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching provisions...\n",
      "[INFO] Fetching detailed issue-level data...\n",
      "[INFO] Merging data...\n",
      "[INFO] Saving CSV...\n",
      "[SUCCESS] CSV saved: C:/Users/DanielGodden/Documents/MCHLG/collecting_and_managing_data\\odp-issue.csv (575 rows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import argparse\n",
    "\n",
    "SPATIAL_DATASETS = [\n",
    "    \"article-4-direction-area\",\n",
    "    \"conservation-area\",\n",
    "    \"listed-building-outline\",\n",
    "    \"tree-preservation-zone\",\n",
    "    \"tree\",\n",
    "]\n",
    "DOCUMENT_DATASETS = [\n",
    "    \"article-4-direction\",\n",
    "    \"conservation-area-document\",\n",
    "    \"tree-preservation-order\",\n",
    "]\n",
    "ALL_DATASETS = SPATIAL_DATASETS + DOCUMENT_DATASETS\n",
    "\n",
    "def get_datasette_http():\n",
    "    retry_strategy = Retry(total=3, status_forcelist=[400], backoff_factor=0.2)\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    return http\n",
    "\n",
    "def get_datasette_query(db: str, sql: str, url=\"https://datasette.planning.data.gov.uk\") -> pd.DataFrame:\n",
    "    full_url = f\"{url}/{db}.json\"\n",
    "    params = {\"sql\": sql, \"_shape\": \"array\", \"_size\": \"max\"}\n",
    "    try:\n",
    "        http = get_datasette_http()\n",
    "        response = http.get(full_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return pd.DataFrame(response.json())\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Datasette query failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_provisions():\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "            p.cohort,\n",
    "            p.organisation,\n",
    "            c.start_date AS cohort_start_date,\n",
    "            o.name AS organisation_name\n",
    "        FROM provision p\n",
    "        INNER JOIN cohort c ON c.cohort = p.cohort\n",
    "        INNER JOIN organisation o ON o.organisation = p.organisation\n",
    "        WHERE p.provision_reason = 'expected'\n",
    "          AND p.project = 'open-digital-planning'\n",
    "        GROUP BY p.organisation, p.cohort\n",
    "    \"\"\"\n",
    "    return get_datasette_query(\"digital-land\", sql)\n",
    "\n",
    "def get_issue_type_chunk(dataset_clause, offset):\n",
    "    sql = f\"\"\"\n",
    "        SELECT\n",
    "            edits.*,\n",
    "            eds.endpoint_end_date,\n",
    "            eds.endpoint_entry_date,\n",
    "            eds.latest_status,\n",
    "            eds.latest_exception\n",
    "        FROM endpoint_dataset_issue_type_summary edits\n",
    "        LEFT JOIN (\n",
    "            SELECT endpoint, end_date as endpoint_end_date,\n",
    "                   entry_date as endpoint_entry_date,\n",
    "                   latest_status, latest_exception\n",
    "            FROM endpoint_dataset_summary\n",
    "        ) eds ON edits.endpoint = eds.endpoint\n",
    "        {dataset_clause}\n",
    "        LIMIT 1000 OFFSET {offset}\n",
    "    \"\"\"\n",
    "    return get_datasette_query(\"performance\", sql)\n",
    "\n",
    "def get_full_issue_type_summary(datasets):\n",
    "    dataset_clause = \"WHERE \" + \" OR \".join(f\"edits.dataset = '{ds}'\" for ds in datasets)\n",
    "    df_list = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        chunk = get_issue_type_chunk(dataset_clause, offset)\n",
    "        if chunk.empty:\n",
    "            break\n",
    "        df_list.append(chunk)\n",
    "        if len(chunk) < 1000:\n",
    "            break\n",
    "        offset += 1000\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def generate_detailed_issue_csv(output_dir: str, dataset_type=\"all\") -> str:\n",
    "    datasets = {\n",
    "        \"spatial\": SPATIAL_DATASETS,\n",
    "        \"document\": DOCUMENT_DATASETS,\n",
    "        \"all\": ALL_DATASETS\n",
    "    }.get(dataset_type, ALL_DATASETS)\n",
    "\n",
    "    print(\"[INFO] Fetching provisions...\")\n",
    "    provisions = get_provisions()\n",
    "\n",
    "    print(\"[INFO] Fetching detailed issue-level data...\")\n",
    "    issues = get_full_issue_type_summary(datasets)\n",
    "\n",
    "    print(\"[INFO] Merging data...\")\n",
    "    merged = provisions.merge(issues.drop(columns=[\"organisation_name\"], errors=\"ignore\"), on=[\"organisation\", \"cohort\"], how=\"inner\")\n",
    "\n",
    "    print(\"[INFO] Saving CSV...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"odp-issue.csv\")\n",
    "    merged[\n",
    "        [\n",
    "            \"organisation\",\n",
    "            \"cohort\",\n",
    "            \"organisation_name\",\n",
    "            \"pipeline\",\n",
    "            \"issue_type\",\n",
    "            \"severity\",\n",
    "            \"responsibility\",\n",
    "            \"count_issues\",\n",
    "            \"collection\",\n",
    "            \"endpoint\",\n",
    "            \"endpoint_url\",\n",
    "            \"latest_status\",\n",
    "            \"latest_exception\",\n",
    "            \"resource\",\n",
    "            \"latest_log_entry_date\",\n",
    "            \"endpoint_entry_date\",\n",
    "            \"endpoint_end_date\",\n",
    "            \"resource_start_date\",\n",
    "            \"resource_end_date\",\n",
    "        ]\n",
    "    ].to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"[SUCCESS] CSV saved: {output_path} ({len(merged)} rows)\")\n",
    "    return output_path\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Generate detailed ODP issue-level CSV\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save the output CSV\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse arguments from CLI\n",
    "    #args = parse_args()\n",
    "    output_dir = \"C:/Users/DanielGodden/Documents/MCHLG/collecting_and_managing_data\"#args.output_dir\n",
    "    generate_detailed_issue_csv(output_dir, dataset_type=\"all\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
