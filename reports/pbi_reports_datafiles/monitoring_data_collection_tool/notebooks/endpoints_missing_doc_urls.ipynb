{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11b53be",
   "metadata": {},
   "source": [
    "This Python script is designed to audit data publishing endpoints listed in the Digital Land platform by identifying which endpoints are missing associated documentation URLs. It does this by connecting to the Datasette API, running a SQL query to fetch endpoint metadata, analyzing the results, and saving the findings to a CSV file.\n",
    "\n",
    "Here's how the script works in detail:\n",
    "\n",
    "1. **Command-Line Interface**:\n",
    "   The script uses `argparse` to accept an `--output-dir` argument, which determines where the resulting CSV file will be saved.\n",
    "\n",
    "2. **Data Fetching with Pagination**:\n",
    "   A paginated SQL query is constructed and repeatedly executed against the Datasette API endpoint (`https://datasette.planning.data.gov.uk/digital-land.json`) to retrieve all records from the `endpoint`, `source`, `source_pipeline`, and `organisation` tables. These are joined together to build a comprehensive view of each endpoint, including:\n",
    "   - Organisation name and ID\n",
    "   - Dataset (pipeline)\n",
    "   - Endpoint URL\n",
    "   - Documentation URL\n",
    "   - Entry and end dates\n",
    "\n",
    "   The SQL uses `LIMIT 1000 OFFSET {offset}` pagination to work around any row limits imposed by the API.\n",
    "\n",
    "3. **Data Analysis**:\n",
    "   The fetched dataset is analyzed to identify missing documentation:\n",
    "   - A `documentation_missing` column is created, where `True` indicates the `documentation_url` is either `null` or an empty string.\n",
    "   - A breakdown of statistics is printed to the console:\n",
    "     - Total number of endpoints\n",
    "     - Number and percentage of endpoints missing documentation\n",
    "     - Top 10 datasets (pipelines) most affected by missing documentation\n",
    "     - A breakdown of missing documentation for **active** (no `end_date`) vs. **ended** endpoints\n",
    "     - The most recent entry date among endpoints missing documentation\n",
    "\n",
    "4. **Data Enrichment**:\n",
    "   - A boolean column `is_active` is created to flag whether an endpoint is still active (i.e., `end_date` is blank).\n",
    "   - The `entry_date` is parsed into a `datetime` object for more accurate filtering and reporting.\n",
    "\n",
    "5. **Output**:\n",
    "   The entire dataset—including both documented and undocumented endpoints—is saved as a CSV file named `endpoints_missing_documentation_urls.csv` in the directory specified by the `--output-dir` argument. The file includes both metadata and computed flags (`documentation_missing`, `is_active`, etc.).\n",
    "\n",
    "This script is useful for assessing data publishing completeness across planning authorities and helps ensure that all data sources are properly documented for transparency, traceability, and usability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03076ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Constants\n",
    "DATASSETTE_URL = \"https://datasette.planning.data.gov.uk/digital-land.json\"\n",
    "\n",
    "# Base SQL query to retrieve endpoint metadata\n",
    "BASE_SQL = \"\"\"\n",
    "SELECT \n",
    "    o.name,\n",
    "    s.organisation, \n",
    "    sp.pipeline AS \"pipeline/dataset\", \n",
    "    e.endpoint_url, \n",
    "    s.documentation_url,\n",
    "    s.entry_date,\n",
    "    s.end_date,\n",
    "    e.endpoint\n",
    "FROM \n",
    "    endpoint e\n",
    "    INNER JOIN source s ON e.endpoint = s.endpoint\n",
    "    INNER JOIN source_pipeline sp ON s.source = sp.source\n",
    "    INNER JOIN organisation o ON o.organisation = s.organisation\n",
    "ORDER BY s.entry_date DESC\n",
    "LIMIT 1000 OFFSET {offset}\n",
    "\"\"\"\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command-line arguments for the output directory.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Export endpoints with missing documentation URLs.\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save the output CSV\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def fetch_endpoint_data():\n",
    "    \"\"\"\n",
    "    Fetches all endpoint metadata using paginated SQL from Datasette API.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined result of all pages as a DataFrame.\n",
    "    \"\"\"\n",
    "    all_rows, offset = [], 0\n",
    "    columns = []\n",
    "\n",
    "    while True:\n",
    "        paginated_sql = BASE_SQL.format(offset=offset)\n",
    "        response = requests.get(DATASSETTE_URL, params={\"sql\": paginated_sql, \"_size\": 1000})\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data from Datasette.\")\n",
    "            break\n",
    "\n",
    "        json_data = response.json()\n",
    "        rows = json_data.get(\"rows\", [])\n",
    "\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        if offset == 0:\n",
    "            columns = json_data.get(\"columns\", [])\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "        offset += 1000\n",
    "\n",
    "    return pd.DataFrame(all_rows, columns=columns) if all_rows else pd.DataFrame()\n",
    "\n",
    "def analyze_missing_docs(df):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to flag missing documentation URLs and report stats.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw endpoint metadata.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added helper columns.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    df[\"documentation_missing\"] = df[\"documentation_url\"].fillna(\"\").str.strip() == \"\"\n",
    "    missing_count = df[\"documentation_missing\"].sum()\n",
    "    percent_missing = (missing_count / total) * 100\n",
    "\n",
    "    print(f\"Total endpoints: {total}\")\n",
    "    print(f\"Missing documentation_url: {missing_count}\")\n",
    "    print(f\"Percent missing: {percent_missing:.2f}%\")\n",
    "\n",
    "    top_missing = (\n",
    "        df[df[\"documentation_missing\"]]\n",
    "        .groupby(\"pipeline/dataset\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\nTop affected pipelines:\")\n",
    "    print(top_missing.to_string())\n",
    "\n",
    "    df[\"is_active\"] = df[\"end_date\"].fillna(\"\").str.strip() == \"\"\n",
    "    active_missing = df.query(\"documentation_missing and is_active\").shape[0]\n",
    "    ended_missing = df.query(\"documentation_missing and not is_active\").shape[0]\n",
    "\n",
    "    print(f\"\\nActive endpoints missing documentation: {active_missing}\")\n",
    "    print(f\"Ended endpoints missing documentation: {ended_missing}\")\n",
    "\n",
    "    df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"], errors=\"coerce\")\n",
    "    recent_missing = df[df[\"documentation_missing\"]][\"entry_date\"].max()\n",
    "    recent_str = recent_missing.date() if pd.notnull(recent_missing) else \"N/A\"\n",
    "    print(f\"\\nMost recent entry with missing documentation: {recent_str}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_results(df, output_dir):\n",
    "    \"\"\"\n",
    "    Filters endpoints missing documentation and saves to CSV.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The analyzed DataFrame.\n",
    "        output_dir (str): Output directory path.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    #filtered = df.query(\"documentation_missing and is_active\")\n",
    "    output_path = os.path.join(output_dir, \"endpoints_missing_documentation_urls.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"CSV saved: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main workflow to fetch, analyze, and save data.\n",
    "    \"\"\"\n",
    "    args = parse_args()\n",
    "    df = fetch_endpoint_data()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No data found to process.\")\n",
    "        return\n",
    "\n",
    "    df = analyze_missing_docs(df)\n",
    "    save_results(df, args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
