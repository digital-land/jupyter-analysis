{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce82c64",
   "metadata": {},
   "source": [
    "This script generates a detailed **ODP (Open Digital Planning) status summary CSV**. It compares expected dataset provisions (from local authorities) against the actual endpoints they've published, helping track conformance and gaps across key planning datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### What It Does\n",
    "\n",
    "The script connects to the **Datasette Planning instance** and pulls together:\n",
    "\n",
    "- **Expected provisions** for each organisation and cohort (`provision` table).\n",
    "- **Latest endpoint data** for each dataset pipeline (`reporting_latest_endpoints` table).\n",
    "\n",
    "It checks if each local authority has published endpoints for a given dataset pipeline, and captures associated metadata (e.g. HTTP status, log dates, exceptions, etc.). If an expected pipeline is not found, it's flagged with placeholder values (e.g. `\"No endpoint added\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Collections Tracked\n",
    "\n",
    "Using the `ALL_PIPELINES` dictionary, the script checks the following collections:\n",
    "\n",
    "| Collection                | Pipelines                                                                |\n",
    "|---------------------------|--------------------------------------------------------------------------|\n",
    "| `article-4-direction`     | `article-4-direction`, `article-4-direction-area`                        |\n",
    "| `conservation-area`       | `conservation-area`, `conservation-area-document`                        |\n",
    "| `listed-building`         | `listed-building-outline`                                                |\n",
    "| `tree-preservation-order` | `tree-preservation-order`, `tree-preservation-zone`, `tree`              |\n",
    "\n",
    "Each collection maps to one or more dataset pipelines used to evaluate if the local authority has met its expected publication duties.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Retrieve expected provisions** (organisation, cohort, start date).\n",
    "2. **Retrieve latest endpoint metadata** from the performance DB.\n",
    "3. For each organisation and pipeline:\n",
    "   - Check if an endpoint exists.\n",
    "   - If yes → extract its metadata.\n",
    "   - If not → create a row indicating it’s missing.\n",
    "4. All results are compiled into a single DataFrame.\n",
    "5. The CSV `odp-status.csv` is saved to the specified `--output-dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "import argparse\n",
    "\n",
    "ALL_PIPELINES = {\n",
    "    \"article-4-direction\": [\"article-4-direction\", \"article-4-direction-area\"],\n",
    "    \"conservation-area\": [\"conservation-area\", \"conservation-area-document\"],\n",
    "    \"listed-building\": [\"listed-building-outline\"],\n",
    "    \"tree-preservation-order\": [\n",
    "        \"tree-preservation-order\",\n",
    "        \"tree-preservation-zone\",\n",
    "        \"tree\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def get_datasette_http():\n",
    "    \"\"\"\n",
    "    Returns a requests session with retry logic to handle larger datasette queries.\n",
    "    \"\"\"\n",
    "    retry_strategy = Retry(total=3, status_forcelist=[400], backoff_factor=0.2)\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    return http\n",
    "\n",
    "\n",
    "def get_datasette_query(db: str, sql: str, url=\"https://datasette.planning.data.gov.uk\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executes SQL against the given datasette database and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    full_url = f\"{url}/{db}.json\"\n",
    "    params = {\"sql\": sql, \"_shape\": \"array\", \"_size\": \"max\"}\n",
    "\n",
    "    try:\n",
    "        http = get_datasette_http()\n",
    "        response = http.get(full_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return pd.DataFrame.from_dict(response.json())\n",
    "    except Exception as e:\n",
    "        print(f\"Datasette query failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_provisions():\n",
    "    \"\"\"\n",
    "    Retrieves cohort provisions (expected organisations and their cohorts).\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "            p.cohort,\n",
    "            p.organisation,\n",
    "            c.start_date as cohort_start_date,\n",
    "            org.name as name\n",
    "        FROM provision p\n",
    "        INNER JOIN cohort c ON c.cohort = p.cohort\n",
    "        INNER JOIN organisation org ON org.organisation = p.organisation\n",
    "        WHERE p.provision_reason = \"expected\"\n",
    "          AND p.project = \"open-digital-planning\"\n",
    "        GROUP BY p.organisation, p.cohort\n",
    "    \"\"\"\n",
    "    return get_datasette_query(\"digital-land\", sql)\n",
    "\n",
    "\n",
    "def get_endpoints():\n",
    "    \"\"\"\n",
    "    Retrieves latest reporting endpoint data.\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "            rle.organisation,\n",
    "            rle.collection,\n",
    "            rle.pipeline,\n",
    "            rle.endpoint,\n",
    "            rle.endpoint_url,\n",
    "            rle.licence,\n",
    "            rle.latest_status as status,\n",
    "            rle.days_since_200,\n",
    "            rle.latest_exception as exception,\n",
    "            rle.resource,\n",
    "            rle.latest_log_entry_date,\n",
    "            rle.endpoint_entry_date,\n",
    "            rle.endpoint_end_date,\n",
    "            rle.resource_start_date,\n",
    "            rle.resource_end_date\n",
    "        FROM reporting_latest_endpoints rle\n",
    "    \"\"\"\n",
    "    df = get_datasette_query(\"performance\", sql)\n",
    "    df[\"organisation\"] = df[\"organisation\"].str.replace(\"-eng\", \"\", regex=False)\n",
    "    return df\n",
    "\n",
    "def generate_odp_summary_csv(output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a CSV file showing provision status by dataset and saves it to output_dir.\n",
    "    \"\"\"\n",
    "    provisions = get_provisions()\n",
    "    endpoints = get_endpoints()\n",
    "\n",
    "    output_rows = []\n",
    "\n",
    "    for _, row in provisions.iterrows():\n",
    "        organisation = row[\"organisation\"]\n",
    "        cohort = row[\"cohort\"]\n",
    "        name = row[\"name\"]\n",
    "        cohort_start_date = row[\"cohort_start_date\"]\n",
    "\n",
    "        for collection, pipelines in ALL_PIPELINES.items():\n",
    "            for pipeline in pipelines:\n",
    "                match = endpoints[\n",
    "                    (endpoints[\"organisation\"] == organisation) &\n",
    "                    (endpoints[\"pipeline\"] == pipeline)\n",
    "                ]\n",
    "\n",
    "                if not match.empty:\n",
    "                    for _, ep in match.iterrows():\n",
    "                        output_rows.append({\n",
    "                            \"organisation\": organisation,\n",
    "                            \"cohort\": cohort,\n",
    "                            \"name\": name,\n",
    "                            \"collection\": collection,\n",
    "                            \"pipeline\": pipeline,\n",
    "                            \"endpoint\": ep[\"endpoint\"],\n",
    "                            \"endpoint_url\": ep[\"endpoint_url\"],\n",
    "                            \"licence\": ep[\"licence\"],\n",
    "                            \"status\": ep[\"status\"],\n",
    "                            \"days_since_200\": ep[\"days_since_200\"],\n",
    "                            \"exception\": ep[\"exception\"],\n",
    "                            \"resource\": ep[\"resource\"],\n",
    "                            \"latest_log_entry_date\": ep[\"latest_log_entry_date\"],\n",
    "                            \"endpoint_entry_date\": ep[\"endpoint_entry_date\"],\n",
    "                            \"endpoint_end_date\": ep[\"endpoint_end_date\"],\n",
    "                            \"resource_start_date\": ep[\"resource_start_date\"],\n",
    "                            \"resource_end_date\": ep[\"resource_end_date\"],\n",
    "                            \"cohort_start_date\": cohort_start_date,\n",
    "                        })\n",
    "                else:\n",
    "                    output_rows.append({\n",
    "                        \"organisation\": organisation,\n",
    "                        \"cohort\": cohort,\n",
    "                        \"name\": name,\n",
    "                        \"collection\": collection,\n",
    "                        \"pipeline\": pipeline,\n",
    "                        \"endpoint\": \"No endpoint added\",\n",
    "                        \"endpoint_url\": \"\",\n",
    "                        \"licence\": \"\",\n",
    "                        \"status\": \"\",\n",
    "                        \"days_since_200\": \"\",\n",
    "                        \"exception\": \"\",\n",
    "                        \"resource\": \"\",\n",
    "                        \"latest_log_entry_date\": \"\",\n",
    "                        \"endpoint_entry_date\": \"\",\n",
    "                        \"endpoint_end_date\": \"\",\n",
    "                        \"resource_start_date\": \"\",\n",
    "                        \"resource_end_date\": \"\",\n",
    "                        \"cohort_start_date\": cohort_start_date,\n",
    "                    })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_final = pd.DataFrame(output_rows)\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"odp-status.csv\")\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"CSV generated at {output_path} with {len(df_final)} rows\")\n",
    "    return output_path\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command-line arguments for the output directory.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed arguments containing the output directory path.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Datasette batch exporter\")\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Directory to save exported CSVs\"\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# Run Script\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse arguments from CLI\n",
    "    args = parse_args()\n",
    "\n",
    "    # Set your desired output path here\n",
    "    output_directory = args.output_dir\n",
    "    \n",
    "    generate_odp_summary_csv(output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
