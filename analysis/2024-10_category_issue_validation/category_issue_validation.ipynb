{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date**:  8th October 2024 <br>\n",
    "**Dataset Scope**: all <br>\n",
    "**Report Type**: Ad-hoc analysis <br>\n",
    "\n",
    "## Purpose\n",
    "Check the new [categorical value data quality issue](https://datasette.planning.data.gov.uk/digital-land/issue_type?_sort=issue_type&issue_type__exact=invalid+category+value) is working as expected. \n",
    "\n",
    "We want to:  \n",
    "\n",
    "* Check the issues being raised are valid\n",
    "* Identify any invalid category values which we think should be added to the [category datasets](https://www.planning.data.gov.uk/dataset/#category) by the Data Design team\n",
    "* Identify any invalid category values for statutory datasets which should be patched (i.e. they are close to a valid value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import spatialite\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "data_dir = \"../../data/db_downloads/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasette_query(db, sql_string):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": sql_string,\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"https://datasette.planning.data.gov.uk/{db}.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def query_sqlite(db_path, query_string):\n",
    "\n",
    "    with spatialite.connect(db_path) as con:\n",
    "            \n",
    "        cursor = con.execute(query_string)\n",
    "        cols = [column[0] for column in cursor.description]\n",
    "        results_df = pd.DataFrame.from_records(data=cursor.fetchall(), columns=cols)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "FILES_URL = 'https://datasette.planning.data.gov.uk/'\n",
    "\n",
    "def download_dataset(dataset, output_dir_path, overwrite=False):\n",
    "    dataset_file_name = f'{dataset}.db'\n",
    "    \n",
    "    if not os.path.exists(output_dir_path):\n",
    "        os.makedirs(output_dir_path)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir_path, dataset_file_name)\n",
    "\n",
    "    if overwrite is False and os.path.exists(output_file_path):\n",
    "        return\n",
    "    \n",
    "    final_url = os.path.join(FILES_URL, dataset_file_name)\n",
    "    print(f'downloading data from {final_url}')\n",
    "    print(f'to: {output_file_path}')\n",
    "    urllib.request.urlretrieve(final_url, os.path.join(output_dir_path, dataset_file_name))\n",
    "    print('download complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the invalid category value issues happening\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT dataset, resource, field, value, count(*) as n_issues\n",
    "    FROM issue \n",
    "    WHERE issue_type = \"invalid category value\"\n",
    "    GROUP BY dataset, resource, field, value \n",
    "    ORDER BY dataset\n",
    "\"\"\"\n",
    "\n",
    "cat_issue_sum = datasette_query(\"digital-land\", q)\n",
    "\n",
    "print(len(cat_issue_sum))\n",
    "cat_issue_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select * from patch\"\"\"\n",
    "\n",
    "patches = datasette_query(\"digital-land\", q)\n",
    "\n",
    "print(len(patches))\n",
    "# patches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get endpoint to resource lookup - we want the endpoint hash for creating new patches where we need them\n",
    "\n",
    "download_dataset(\"performance\", data_dir, overwrite=True)\n",
    "perf_path = os.path.join(data_dir, \"performance.db\")\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT distinct endpoint, resource\n",
    "    FROM reporting_historic_endpoints\n",
    "    WHERE resource != \"\"\n",
    "\"\"\"\n",
    "\n",
    "ep_res = query_sqlite(perf_path, q)\n",
    "\n",
    "print(len(ep_res))\n",
    "ep_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_issue_sum[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ODP datasets from list\n",
    "ODP_datasets = [\n",
    "    \"article-4-direction\", \"article-4-direction-area\", \"conservation-area\", \"conservation-area-document\", \"listed-building-outline\", \"tree\", \"tree-preservation-order\", \"tree-preservation-zone\"  \n",
    "]\n",
    "\n",
    "issues_to_check = cat_issue_sum[~cat_issue_sum[\"dataset\"].isin(ODP_datasets)]\n",
    "\n",
    "print(len(cat_issue_sum))\n",
    "print(len(issues_to_check))\n",
    "\n",
    "# join on endpoint\n",
    "issues_to_check = issues_to_check.merge(\n",
    "    ep_res,\n",
    "    how = \"left\",\n",
    "    on = \"resource\"\n",
    ")\n",
    "\n",
    "print(len(issues_to_check))\n",
    "\n",
    "# export - going to endpoint level rather than resource, for patching purposes\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "issues_to_check[[\"dataset\", \"field\", \"value\", \"n_issues\"]].drop_duplicates().to_csv(f\"cat_issues_to_check_{td}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step here is to manually review the issues output into `cat_issues_to_check_[yyyy-mm-dd].csv`. Add in a `needs_patching` field to flag required patches as \"yes\" and put the corrected value in a `patch_value` field. Then read back in below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in annotated results to create new required patches\n",
    "issues_validated = pd.read_csv(\"cat_issues_to_check_2024-10-14_noted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_patch = issues_validated[issues_validated[\"needs_patching\"] == \"yes\"][\n",
    "    [\"dataset\", \"field\", \"value\", \"patch_value\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "# join back on endpoint hash\n",
    "to_patch = to_patch.merge(\n",
    "    issues_to_check[[\"endpoint\", \"dataset\", \"field\", \"value\"]], \n",
    "    how = \"left\",\n",
    "    on = [\"dataset\", \"field\", \"value\"]\n",
    ")\n",
    "\n",
    "to_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print output to copy into patch.csv\n",
    "\n",
    "\n",
    "print(\"dataset,resource,field,pattern,value,entry-number,start-date,end-date,entry-date,endpoint\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "for i, r in to_patch.iterrows():\n",
    "\n",
    "    print(r[\"dataset\"] + \",,\" + r[\"field\"] + \",\" + r[\"value\"] + \",\" + r[\"patch_value\"] + \",,,,,\" + r[\"endpoint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category dataset to get full list of valid values\n",
    "\n",
    "cat_dataset = \"contribution-purpose\"\n",
    "\n",
    "import json\n",
    "with urllib.request.urlopen(f\"https://www.planning.data.gov.uk/entity.json?dataset={cat_dataset}&limit=100\") as url:\n",
    "    data = json.load(url)\n",
    "\n",
    "cont_purpose = pd.DataFrame.from_records(data[\"entities\"])\n",
    "cont_purpose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
