{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date**:  13 Jan 2025 <br>\n",
    "**Dataset Scope**: `dataset` <br>\n",
    "**Report Type**: Ad-hoc analysis <br>\n",
    "\n",
    "## Purpose\n",
    "[Jira ticket](https://mhclgdigital.atlassian.net/browse/DATA-1199)   \n",
    "[Mural board](https://app.mural.co/t/mhclg2837/m/mhclg2837/1706786112750/e94eeff744863fe859b2d22a3ccb2dac1e442270?wid=0-1736957406208) mapping out problem and approach.   \n",
    "\n",
    "Analysis to identify BFL entities which can be given an end-date on the basis that their references do not appear on the latest resource for a provision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "data_dir = \"../../data/db_downloads/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "out_dir = \"../../data/deleted_entities/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "## Problem organisations to exclude\n",
    "# This is a list of organisations who we know supply multiple endpoints for multiple areas - we need to exclude these as this process only works for \n",
    "# provisions where there is one dataset updated over subsequent endpoints and resources, rather than multiple datasets.\n",
    "\n",
    "orgs_to_exclude = [\"local-authority:BUC\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasette_query(db, sql_string):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": sql_string,\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"https://datasette.planning.data.gov.uk/{db}.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def get_all_organisations():\n",
    "    q = \"\"\"\n",
    "        select organisation, name, entity as organisation_entity\n",
    "        from organisation\n",
    "        where end_date = \"\"\n",
    "        \"\"\"\n",
    "    return datasette_query(\"digital-land\", q)\n",
    "\n",
    "FILES_URL = 'https://datasette.planning.data.gov.uk/'\n",
    "\n",
    "def download_dataset(dataset, output_dir_path, overwrite=False):\n",
    "    dataset_file_name = f'{dataset}.db'\n",
    "    \n",
    "    if not os.path.exists(output_dir_path):\n",
    "        os.makedirs(output_dir_path)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir_path, dataset_file_name)\n",
    "\n",
    "    if overwrite is False and os.path.exists(output_file_path):\n",
    "        return\n",
    "    \n",
    "    final_url = os.path.join(FILES_URL, dataset_file_name)\n",
    "    print(f'downloading data from {final_url}')\n",
    "    print(f'to: {output_file_path}')\n",
    "    urllib.request.urlretrieve(final_url, os.path.join(output_dir_path, dataset_file_name))\n",
    "    print('download complete')\n",
    "\n",
    "def query_sqlite(db_path, query_string):\n",
    "\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "            \n",
    "        cursor = con.execute(query_string)\n",
    "        cols = [column[0] for column in cursor.description]\n",
    "        results_df = pd.DataFrame.from_records(data=cursor.fetchall(), columns=cols)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get historic endpoints and resources for a provision, along with incrementing resource key\n",
    "def get_historic_resources(dataset, organisation):\n",
    "\n",
    "    q = f\"\"\"\n",
    "\n",
    "        WITH endpoint_count as (\n",
    "            SELECT \n",
    "                distinct organisation,\n",
    "                latest_status, \n",
    "                endpoint, \n",
    "                endpoint_entry_date, \n",
    "                endpoint_end_date, \n",
    "                resource, \n",
    "                resource_start_date, \n",
    "                resource_end_date,\n",
    "                dense_rank() over (order by endpoint_entry_date, endpoint) as endpoint_no\n",
    "\n",
    "            FROM reporting_historic_endpoints\n",
    "            WHERE pipeline = '{dataset}'\n",
    "            AND organisation = '{organisation}'\n",
    "            AND (latest_status = 200 OR latest_status = \"\")\n",
    "            ORDER BY endpoint_entry_date, resource_start_date\n",
    "        ),\n",
    "\n",
    "        endpoint_resource_count as (\n",
    "            SELECT \n",
    "                *,\n",
    "                CAST(endpoint_no as string) || \".\" || dense_rank() over (partition by endpoint_no order by resource_start_date, latest_status) as endpoint_resource_no\n",
    "            FROM endpoint_count\n",
    "        )\n",
    "\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE WHEN resource_end_date = \"\" then \n",
    "                LEAD(endpoint_entry_date) over (order by endpoint_resource_no) \n",
    "                ELSE resource_end_date end\n",
    "                as entity_end_date\n",
    "\n",
    "        FROM endpoint_resource_count\n",
    "\n",
    "\"\"\"\n",
    "    r = datasette_query(\"performance\", q)\n",
    "\n",
    "    return r \n",
    "\n",
    "# get_historic_resources(\"brownfield-land\", \"local-authority:BST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resource_references(resources, db_path):\n",
    "\n",
    "    # take list of resources and db path to get all reference values which have appeared on each resource\n",
    "\n",
    "    q = \"\"\" \n",
    "\n",
    "        SELECT distinct fr.resource, f.value \n",
    "        FROM fact_resource fr\n",
    "        INNER JOIN fact f on fr.fact = f.fact\n",
    "        WHERE 1=1\n",
    "            AND f.field = \"reference\"\n",
    "            AND fr.resource in ({})\n",
    "    \"\"\".format(', '.join(f\"'{r}'\" for r in resources))\n",
    "\n",
    "    df = query_sqlite(db_path, q)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download performance db\n",
    "download_dataset(\"brownfield-land\", data_dir, overwrite=True)\n",
    "bfl_path = os.path.join(data_dir, \"brownfield-land.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lookup = get_all_organisations()\n",
    "\n",
    "org_dict = dict(zip(org_lookup[\"organisation\"], org_lookup[\"organisation_entity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_bfl_all = datasette_query(\n",
    "    \"digital-land\",\n",
    "    \"\"\"\n",
    "    SELECT organisation\n",
    "    FROM provision\n",
    "    WHERE dataset = \"brownfield-land\"\n",
    "    AND end_date = \"\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# remove organisations to exclude\n",
    "prov_bfl = prov_bfl_all[~prov_bfl_all[\"organisation\"].isin(orgs_to_exclude)].copy()\n",
    "\n",
    "print(len(prov_bfl_all))\n",
    "print(len(prov_bfl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis steps\n",
    "\n",
    "Expected steps:\n",
    "* For each organisation, get table of: organisation, resource, start-date, reference\n",
    "* Take the latest (non-200, with more than 0 entities) resource for each organisation\n",
    "* For any references from not latest resources, use end-date as end-date for entities with that reference\n",
    "\n",
    "\n",
    "NOTE - we should only run this for organisations supplying authoritative data, as otherwise it will get confusing factoring in orgs like GLA who can supply data for multiple organisations. This method of working out latest entities relies on a model of on organisation supplying successive data about the same things through one or multiple endpoints.\n",
    "\n",
    "NOTE - some organisations are using end-date (e.g. Bristol), we should make sure to only apply this calculated end date to entities which don't already have one from the org.\n",
    "\n",
    "Questions:\n",
    "* Is taking resource with latest start date robust enough to identify latest snapshot of data? What if an endpoint for 2022 data has a mistake corrected which generates a newer resource than an endpoint with 2024 data has? Should we use the latest resource from latest endpoint?\n",
    "\n",
    "Suggest here we actually begin giving old endpoints for orgs an end date, it will make the logic of working out what end-date to give old entities SO much easier.\n",
    "\n",
    "* What about orgs like Bucks who have multiple active endpoints to supply data for multiple regions? Can't reliably identify latest data between this model and single dataset model. May have to manually exclude orgs like Bucks, or maybe restrict method to only orgs with single active endpoint?\n",
    "\n",
    "* how to deal with entities with data from old resources that don't appear in reporting or collection tables? e.g. Bristol entity [1725409](https://www.planning.data.gov.uk/entity/1725409) has reference facts from resource `01307bca0ecc5c950ef9b35c02ee7b3378cd9b7d29078229e4984a88d4f04ed1`. This appears in the `resource.csv` with an end-date, but isn't in `old-resource.csv`, or in the BFL `dataset_resource` or `fact_resource` tables. So not easy to work out an end-date for it from just the sqlite file. Will probably need to separately check for any more of these and then they can just be retired.\n",
    "\n",
    "\n",
    "Risks:\n",
    "* Using endpoint_entry_date as a proxy for the data lineage will be wrong in some cases, e.g. [Brighton and Hove](https://datasette.planning.data.gov.uk/performance?sql=SELECT+*%2C%0D%0A++++CAST%28endpoint_no+as+string%29+%7C%7C+%22.%22+%7C%7C+dense_rank%28%29+over+%28partition+by+endpoint_no+order+by+resource_start_date%29+as+endpoint_resource_no%0D%0A%0D%0AFROM+%28%0D%0ASELECT+distinct+organisation%2C+organisation_name%2C+dataset%2C+latest_status%2C+endpoint_url%2C+endpoint%2C+endpoint_entry_date%2C+endpoint_end_date%2C+resource%2C+resource_start_date%2C+resource_end_date%2C%0D%0A+++dense_rank%28%29+over+%28order+by+endpoint_entry_date%2C+endpoint%29+as+endpoint_no%0D%0AFROM+reporting_historic_endpoints%0D%0AWHERE+pipeline+%3D+%22brownfield-land%22%0D%0AAND+organisation+%3D+%22local-authority%3ABNH%22%0D%0AAND+latest_status+%3D+200%0D%0AORDER+BY+endpoint_entry_date%2C+resource_start_date%0D%0A++%29) got 2017 and 2018 data added on the same day in 2019. So references which appeared in one and not the other may get an unusual end-date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing steps with one organisation for POC - Bristol\n",
    "bs_test = get_historic_resources(\"brownfield-land\", \"local-authority:BST\")\n",
    "\n",
    "# get historic resources and the endpoint_resource count key for Bristol\n",
    "bs_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all reference values for each historic resource\n",
    "res_refs = get_resource_references(bs_test[\"resource\"].drop_duplicates(), \"../../data/db_downloads/brownfield-land.db\")\n",
    "\n",
    "res_refs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join bristol resources to list of reference values per resource\n",
    "org_res_refs = bs_test.merge(\n",
    "    res_refs,\n",
    "    how = \"left\",\n",
    "    on = \"resource\"\n",
    ")\n",
    "\n",
    "# get the endpoint_resource_no for the most recent resource the reference value appeared in \n",
    "ref_latest_res = org_res_refs.groupby([\"value\"], as_index = False).agg(\n",
    "    endpoint_resource_no = (\"endpoint_resource_no\", \"max\")\n",
    ")\n",
    "\n",
    "# get the endpoint_resource_no for the latest bristol resource\n",
    "max_ep_res = org_res_refs[\"endpoint_resource_no\"].max()\n",
    "\n",
    "# old refs are those where the most recent resource they appeared on is not the latest resource\n",
    "old_refs = ref_latest_res[ref_latest_res[\"endpoint_resource_no\"] != max_ep_res]\n",
    "\n",
    "# join back to res_refs table to get the end date of the resource each old ref last appeared on - this will be the entity end-date\n",
    "old_refs_dated = org_res_refs.merge(\n",
    "    old_refs,\n",
    "    how = \"inner\",\n",
    "    on = [\"value\", \"endpoint_resource_no\"]\n",
    ")[[\"organisation\", \"value\", \"entity_end_date\"]].drop_duplicates()\n",
    "\n",
    "old_refs_dated.rename(\n",
    "    inplace = True,\n",
    "    columns = {\"value\" : \"reference\"}\n",
    ")\n",
    "\n",
    "old_refs_dated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_entity = query_sqlite(\"../../data/db_downloads/brownfield-land.db\", \n",
    "             \"\"\"\n",
    "                SELECT * \n",
    "                FROM entity\n",
    "                WHERE organisation_entity = 66\n",
    "             \"\"\")\n",
    "\n",
    "\n",
    "# find entities that have a reference which doens't appear in any valid resources at all\n",
    "old_ents = bst_entity[[\"entity\", \"entry_date\", \"reference\"]].merge(\n",
    "                old_refs_dated,\n",
    "                how = \"inner\",\n",
    "                on = \"reference\"\n",
    "            )\n",
    "\n",
    "print(f\"No of bristol entities: {len(bst_entity)}\")\n",
    "print(f\"No of old bristol entities: {len(old_ents)}\")\n",
    "old_ents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystery entities:\n",
    "# find entities that have a reference which doens't appear in any valid resources at all\n",
    "mystery_ents = bst_entity[~bst_entity[\"reference\"].isin(res_refs[\"value\"].drop_duplicates())]\n",
    "print(f\"No of bristol mystery entities: {len(mystery_ents)}\")\n",
    "mystery_ents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sqlite(\"../../data/db_downloads/brownfield-land.db\", \n",
    "            f\"\"\"\n",
    "                SELECT * \n",
    "                FROM entity\n",
    "                WHERE organisation_entity = {org_dict[\"local-authority:BST\"]}\n",
    "                and end_date = \"\"\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale: Apply to all BFL provisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_out = []\n",
    "\n",
    "for org in prov_bfl[\"organisation\"]:\n",
    "    \n",
    "    # get all valid resources\n",
    "    res_hist = get_historic_resources(\"brownfield-land\", org)\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(org)\n",
    "    print(f\"Number of endpoint & resource records found: {len(res_hist)}\")\n",
    "\n",
    "    if len(res_hist) > 1:\n",
    "\n",
    "        # get entities\n",
    "        org_entities = query_sqlite(\"../../data/db_downloads/brownfield-land.db\", \n",
    "            f\"\"\"\n",
    "                SELECT * \n",
    "                FROM entity\n",
    "                WHERE organisation_entity = {org_dict[org]}\n",
    "                and end_date = \"\"\n",
    "            \"\"\")\n",
    "\n",
    "        print(f\"Number of live entities: {len(org_entities)}\")\n",
    "\n",
    "        # get all reference values for resources\n",
    "        res_refs = get_resource_references(\n",
    "            res_hist[\"resource\"].drop_duplicates(), \n",
    "            \"../../data/db_downloads/brownfield-land.db\")\n",
    "        \n",
    "        # join resources to list of reference values per resource\n",
    "        org_res_refs = res_hist.merge(\n",
    "            res_refs,\n",
    "            how = \"left\",\n",
    "            on = \"resource\"\n",
    "        )\n",
    "\n",
    "        # get the endpoint-resource count code for the most recent resource each reference has appeared in \n",
    "        ref_latest_res = org_res_refs.groupby([\"value\"], as_index = False).agg(\n",
    "            endpoint_resource_no = (\"endpoint_resource_no\", \"max\")\n",
    "        )\n",
    "\n",
    "        print(len(ref_latest_res))\n",
    "        \n",
    "        # join back to the full resource table using the endpoint_resource_no to get the end-date of the most recent\n",
    "        ref_latest_res_dated = ref_latest_res.merge(\n",
    "            org_res_refs[[\"resource\", \"endpoint_resource_no\", \"value\", \"entity_end_date\"]],\n",
    "            how = \"left\",\n",
    "            on = [\"value\", \"endpoint_resource_no\"]\n",
    "        )\n",
    "\n",
    "        # join from entity table to our reference and resource calcs\n",
    "        entity_ref_latest_res_dated = org_entities[[\"entity\", \"organisation_entity\", \"reference\", \"end_date\"]].merge(\n",
    "            ref_latest_res_dated,\n",
    "            how = \"left\",\n",
    "            left_on = \"reference\",\n",
    "            right_on = \"value\"\n",
    "        )\n",
    "\n",
    "        entity_ref_latest_res_dated.drop(\"value\", axis=1, inplace=True)\n",
    "\n",
    "        # add flags for old and mystery entities\n",
    "        max_ep_res = org_res_refs[\"endpoint_resource_no\"].max()\n",
    "        entity_ref_latest_res_dated[\"old_entity\"] = np.where(entity_ref_latest_res_dated[\"endpoint_resource_no\"] != max_ep_res, True, False)\n",
    "        entity_ref_latest_res_dated[\"mystery_entity\"] = np.where(entity_ref_latest_res_dated[\"endpoint_resource_no\"].isnull(), True, False)\n",
    "\n",
    "        all_out.append(entity_ref_latest_res_dated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out_df = pd.concat(all_out)\n",
    "print(len(all_out_df))\n",
    "\n",
    "all_out_df[\"organisation_entity\"] = all_out_df[\"organisation_entity\"].astype(int)\n",
    "all_out_df = all_out_df.merge(\n",
    "    org_lookup[[\"organisation_entity\", \"organisation\"]],\n",
    "    on = \"organisation_entity\"\n",
    ")\n",
    "\n",
    "unique_ent_test = len(all_out_df) == len(all_out_df[\"entity\"].drop_duplicates())\n",
    "print(f\"Is the output table unique by entity: {unique_ent_test}\")\n",
    "\n",
    "# all_out_df[\"mystery_entity\"] = np.where(all_out_df[\"endpoint_resource_no\"].isnull(), True, False)\n",
    "# all_out_df[\"old_entity\"] = np.where(all_out_df[\"entity_end_date\"].notnull(), True, False)\n",
    "\n",
    "all_out_df.sort_values([\"organisation\", \"endpoint_resource_no\"], inplace=True, ascending= [True, False])\n",
    "\n",
    "all_out_df.to_csv(os.path.join(out_dir, f\"test - old_entity_results_all_{td}.csv\"), index = False)\n",
    "\n",
    "print(len(all_out_df))\n",
    "all_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_out_df[all_out_df[\"old_entity\"]]))\n",
    "print(len(all_out_df[all_out_df[\"mystery_entity\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(org_lookup))\n",
    "print(len(org_lookup[\"organisation_entity\"].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out_df[all_out_df[\"organisation\"] == \"local-authority:BUC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check entity ranges\n",
    "\n",
    "This is to make sure that all of the entities we're adding end-dates for have an entry in `entity-organisation.csv`; this ensures that the organisation won't be updated to MHCLG when the newer facts come through from our github endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_org = pd.read_csv(f\"https://raw.githubusercontent.com/digital-land/config/refs/heads/main/pipeline/brownfield-land/entity-organisation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_range = all_out_df[\"entity\"]\n",
    "\n",
    "print(f\"checking ranges for {len(e_range)} entities\")\n",
    "\n",
    "# check how many ranges in range table each entity has\n",
    "range_checks = [len(ent_org[(ent_org[\"entity-minimum\"] <= e) & (ent_org[\"entity-maximum\"] >= e)]) for e in e_range]\n",
    "\n",
    "# df for results\n",
    "check_df = pd.DataFrame(\n",
    "    {\n",
    "        \"entity\" : e_range,\n",
    "        \"n_ranges\" : range_checks\n",
    "    }\n",
    ")\n",
    "\n",
    "# test if any with > 1 range\n",
    "entities_no_range = check_df[check_df[\"n_ranges\"] == 0]\n",
    "entities_no_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df[\"n_ranges\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out_df[~all_out_df[\"organisation\"].isin(ent_orgs)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
