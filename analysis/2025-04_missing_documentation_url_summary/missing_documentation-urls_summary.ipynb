{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2a6abe-864f-4980-bd03-c68669b387d4",
   "metadata": {},
   "source": [
    "# Selecting Endpoints **with or without** Documentation URLs\n",
    "\n",
    "**Author**:  Thiruni K <br>\n",
    "**Date**:  24th April 2025 <br>\n",
    "**Dataset Scope**: `dataset` <br>\n",
    "**Report Type**: Ad-hoc analysis <br>\n",
    "**Purpose**: This notebook loads all endpoint records and performs analysis to determine:\n",
    "   - Total number of endpoints\n",
    "   - Count and percentage of endpoints missing `documentation_url`\n",
    "   - Datasets most affected by missing documentation URLs\n",
    "   - Whether those endpoints are still active or have ended\n",
    "   - The most recent `entry_date` for missing documentation_url values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3619a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a4fcf-e683-40e2-8420-ab203e6bfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASSETTE_URL = \"https://datasette.planning.data.gov.uk/digital-land.json\"\n",
    "\n",
    "# --- Step 1: Load all endpoint-related data using SQL OFFSET pagination\n",
    "BASE_SQL = \"\"\"\n",
    "SELECT \n",
    "    o.name,\n",
    "    s.organisation, \n",
    "    sp.pipeline AS \"pipeline/dataset\", \n",
    "    e.endpoint_url, \n",
    "    s.documentation_url,\n",
    "    s.entry_date,\n",
    "    s.end_date,\n",
    "    e.endpoint\n",
    "FROM \n",
    "    endpoint e\n",
    "    INNER JOIN source s on e.endpoint = s.endpoint\n",
    "    INNER JOIN source_pipeline sp on s.source = sp.source\n",
    "    INNER JOIN organisation o on o.organisation = s.organisation\n",
    "ORDER BY s.entry_date DESC\n",
    "LIMIT 1000 OFFSET {offset}\n",
    "\"\"\"\n",
    "\n",
    "all_rows = []\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    paginated_sql = BASE_SQL.format(offset=offset)\n",
    "    params = {\n",
    "        \"sql\": paginated_sql,\n",
    "        \"_size\": 1000\n",
    "    }\n",
    "\n",
    "    response = requests.get(DATASSETTE_URL, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve data from Datasette.\")\n",
    "        break\n",
    "\n",
    "    data = response.json()\n",
    "    rows = data.get(\"rows\", [])\n",
    "    columns = data.get(\"columns\", [])\n",
    "\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    all_rows.extend(rows)\n",
    "    offset += 1000\n",
    "\n",
    "# Convert all rows into a DataFrame\n",
    "if all_rows:\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- Step 2: Analysis\n",
    "\n",
    "if not df.empty:\n",
    "    total_records = len(df)\n",
    "    df['documentation_missing'] = df['documentation_url'].isnull() | (df['documentation_url'].astype(str).str.strip() == '')\n",
    "    missing_count = df['documentation_missing'].sum()\n",
    "    percent_missing = (missing_count / total_records) * 100\n",
    "\n",
    "    print(f\"Total endpoints: {total_records}\")\n",
    "    print(f\"Missing documentation_url: {missing_count}\")\n",
    "    print(f\"Percent missing: {percent_missing:.2f}%\")\n",
    "\n",
    "    # --- Step 3: Most affected datasets\n",
    "    affected_by_pipeline = df[df['documentation_missing']].groupby('pipeline/dataset').size().sort_values(ascending=False)\n",
    "    print(\"\\nTop affected pipelines (by missing documentation_url):\")\n",
    "    print(affected_by_pipeline.head(10).to_string())\n",
    "\n",
    "    # --- Step 4: Are missing ones active or ended?\n",
    "    df['is_active'] = df['end_date'].isnull() | (df['end_date'].astype(str).str.strip() == '')\n",
    "    active_missing = df[df['documentation_missing'] & df['is_active']].shape[0]\n",
    "    ended_missing = df[df['documentation_missing'] & ~df['is_active']].shape[0]\n",
    "\n",
    "    print(f\"\\nEndpoints with missing documentation_url that are still active: {active_missing}\")\n",
    "    print(f\"Endpoints with missing documentation_url that are ended: {ended_missing}\")\n",
    "\n",
    "    # --- Step 5: Are we still adding missing ones?\n",
    "    df['entry_date'] = pd.to_datetime(df['entry_date'], errors='coerce')\n",
    "    most_recent_missing = df[df['documentation_missing']]['entry_date'].max()\n",
    "    print(f\"\\nMost recent entry with missing documentation_url: {most_recent_missing.date() if pd.notnull(most_recent_missing) else 'N/A'}\")\n",
    "else:\n",
    "    print(\"No data available to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not df.empty and not affected_by_pipeline.empty:\n",
    "    top_20 = affected_by_pipeline.head(10)\n",
    "    ax = top_20.plot(\n",
    "        kind='barh',\n",
    "        figsize=(10, 6),\n",
    "        title='Top 20 Pipelines with Missing Documentation URLs'\n",
    "    )\n",
    "    plt.xlabel('Count of Missing documentation_url')\n",
    "    plt.ylabel('Pipeline / Dataset')\n",
    "    plt.gca().invert_yaxis()  # Highest count on top\n",
    "\n",
    "    # Add value labels to the bars\n",
    "    for i, v in enumerate(top_20):\n",
    "        ax.text(v + 0.5, i, str(v), va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
