{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date**:  24th September 2024 <br>\n",
    "**Dataset Scope**: `dataset` <br>\n",
    "**Report Type**: Ad-hoc analysis <br>\n",
    "\n",
    "## Purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import spatialite\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "data_dir = \"../../data/db_downloads/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasette_query(db, sql_string):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": sql_string,\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"https://datasette.planning.data.gov.uk/{db}.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def get_all_organisations():\n",
    "    q = \"\"\"\n",
    "        select organisation, name, entity as organisation_entity\n",
    "        from organisation\n",
    "        \"\"\"\n",
    "    return datasette_query(\"digital-land\", q)\n",
    "\n",
    "FILES_URL = 'https://datasette.planning.data.gov.uk/'\n",
    "\n",
    "def download_dataset(dataset, output_dir_path, overwrite=False):\n",
    "    dataset_file_name = f'{dataset}.db'\n",
    "    \n",
    "    if not os.path.exists(output_dir_path):\n",
    "        os.makedirs(output_dir_path)\n",
    "    \n",
    "    output_file_path = os.path.join(output_dir_path, dataset_file_name)\n",
    "\n",
    "    if overwrite is False and os.path.exists(output_file_path):\n",
    "        return\n",
    "    \n",
    "    final_url = os.path.join(FILES_URL, dataset_file_name)\n",
    "    print(f'downloading data from {final_url}')\n",
    "    print(f'to: {output_file_path}')\n",
    "    urllib.request.urlretrieve(final_url, os.path.join(output_dir_path, dataset_file_name))\n",
    "    print('download complete')\n",
    "\n",
    "def query_sqlite(db_path, query_string):\n",
    "\n",
    "    with spatialite.connect(db_path) as con:\n",
    "            \n",
    "        cursor = con.execute(query_string)\n",
    "        cols = [column[0] for column in cursor.description]\n",
    "        results_df = pd.DataFrame.from_records(data=cursor.fetchall(), columns=cols)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_vs_resource(dataset, organisation_entity, resources):\n",
    "\n",
    "    # needs organisation_entity and a list of active resources for that org\n",
    "    # will return a count of current entities, and no. of reference values from active resources\n",
    "\n",
    "    q = \"\"\" \n",
    "\n",
    "    with latest_res as (\n",
    "        select distinct f.value \n",
    "        from fact_resource fr\n",
    "        inner join fact f on fr.fact = f.fact\n",
    "        where 1=1\n",
    "            and f.field = \"reference\"\n",
    "            and resource in ({})\n",
    "    )\n",
    "\n",
    "    select \n",
    "        e.dataset as pipeline, \n",
    "        count(*) as count_entities, \n",
    "        count(distinct e.reference) as count_entity_unique_refs, \n",
    "        count(distinct lr.value) as count_active_res_unique_refs\n",
    "    from entity e\n",
    "    full outer join latest_res lr on e.reference = lr.value\n",
    "    where e.organisation_entity = {}\n",
    "    \"\"\".format(', '.join(f\"'{r}'\" for r in resources), organisation_entity)\n",
    "\n",
    "    try:\n",
    "        df = datasette_query(dataset, q)\n",
    "    \n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deleted_entities(dataset, organisation_entity, resources):\n",
    "\n",
    "    # needs organisation_entity and a list of active resources for that org\n",
    "    # will return the entities which have a reference value that doesn't appear \n",
    "    # on active resources\n",
    "    \n",
    "    q = \"\"\" \n",
    "\n",
    "    with latest_res as (\n",
    "        select distinct f.value \n",
    "        from fact_resource fr\n",
    "        inner join fact f on fr.fact = f.fact\n",
    "        where 1=1\n",
    "            and f.field = \"reference\"\n",
    "            and resource in ({})\n",
    "    )\n",
    "\n",
    "    select e.dataset as pipeline, e.entity, e.organisation_entity, e.reference\n",
    "    from entity e\n",
    "    full outer join latest_res lr on e.reference = lr.value\n",
    "    where e.organisation_entity = {}\n",
    "    and lr.value is null\n",
    "    \"\"\".format(', '.join(f\"'{r}'\" for r in resources), organisation_entity)\n",
    "    \n",
    "    return datasette_query(dataset, q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download performance db\n",
    "download_dataset(\"performance\", data_dir, overwrite=False)\n",
    "perf_path = os.path.join(data_dir, \"performance.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unknown entity issues from performance db\n",
    "q = \"\"\"\n",
    "    SELECT distinct resource, count_issues as count_unknown_entities\n",
    "    FROM endpoint_dataset_issue_type_summary\n",
    "    WHERE issue_type = \"unknown entity\"\n",
    "\"\"\"\n",
    "\n",
    "uk_ents = query_sqlite(perf_path, q)\n",
    "# uk_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lookup = get_all_organisations()\n",
    "\n",
    "dataset = \"article-4-direction-area\"\n",
    "\n",
    "# get active resources for dataset\n",
    "q = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM reporting_historic_endpoints\n",
    "    WHERE pipeline = '{dataset}'\n",
    "    AND latest_status = 200\n",
    "    AND resource_end_date = \"\"\n",
    "\"\"\"\n",
    "historic_ep_df = query_sqlite(perf_path, q)\n",
    "\n",
    "print(len(historic_ep_df))\n",
    "\n",
    "# join on org_entity and unknown entity counts\n",
    "historic_ep_df = historic_ep_df.merge(\n",
    "    org_lookup[[\"name\", \"organisation_entity\"]],\n",
    "    how = \"left\",\n",
    "    on = \"name\"\n",
    ").merge(\n",
    "    uk_ents,\n",
    "    how = \"left\",\n",
    "    on = \"resource\"\n",
    ")\n",
    "\n",
    "print(len(historic_ep_df))\n",
    "historic_ep_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group to provision level - get count and list of active resources for each, plus unknown entities\n",
    "grouped = historic_ep_df.groupby(\n",
    "    [\"pipeline\", \"organisation\", \"name\", \"organisation_entity\"],\n",
    "    as_index=False ).agg(\n",
    "        resources = (\"resource\", list),\n",
    "        n_active_resources = (\"resource\", \"count\"),\n",
    "        count_unknown_entities = (\"count_unknown_entities\", \"sum\")\n",
    "        )\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run comparison query for each provision and store results\n",
    "count_comp = []\n",
    "\n",
    "for i, r in grouped.iterrows():\n",
    "    \n",
    "    results = count_entity_vs_resource(r[\"pipeline\"] ,r[\"organisation_entity\"], r[\"resources\"])\n",
    "    results[\"organisation_entity\"] = r[\"organisation_entity\"]\n",
    "\n",
    "    count_comp.append(results)\n",
    "\n",
    "count_comp_all = pd.concat(count_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results = grouped.merge(\n",
    "    count_comp_all,\n",
    "    how = \"left\",\n",
    "    on = [\"pipeline\", \"organisation_entity\"]\n",
    ")\n",
    "\n",
    "# get results where count entities != either source of unique refs (entity or active resources)\n",
    "mis_matches = comp_results[\n",
    "    (comp_results[\"count_entities\"] != comp_results[\"count_entity_unique_refs\"]) |\n",
    "    (comp_results[\"count_entities\"] != comp_results[\"count_active_res_unique_refs\"])]\n",
    "\n",
    "mis_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_ents = []\n",
    "\n",
    "for i, r in grouped.iterrows():\n",
    "    \n",
    "    results = get_deleted_entities(r[\"pipeline\"] ,r[\"organisation_entity\"], r[\"resource\"])\n",
    "    results[\"name\"] = r[\"name\"]\n",
    "\n",
    "    deleted_ents.append(results)\n",
    "\n",
    "deleted_ents_all = pd.concat(deleted_ents)\n",
    "deleted_ents_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lookup = get_all_organisations()\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT * \n",
    "    FROM reporting_historic_endpoints\n",
    "    WHERE pipeline = \"article-4-direction-area\"\n",
    "    AND organisation in (\"local-authority-eng:BUC\", \"local-authority-eng:BIR\", \"local-authority-eng:HOR\")\n",
    "    AND latest_status = 200\n",
    "    and resource_end_date = \"\"\n",
    "\"\"\"\n",
    "\n",
    "historic_ep_df = datasette_query(\"performance\", q)\n",
    "historic_ep_df = historic_ep_df.merge(\n",
    "    org_lookup[[\"name\", \"organisation_entity\"]],\n",
    "    how = \"left\",\n",
    "    on = \"name\"\n",
    ")\n",
    "\n",
    "historic_ep_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
