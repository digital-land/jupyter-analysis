{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a42c70-3f28-4d36-94a4-4c5ac5dfc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc1a35-a677-4c5c-bccc-f1b404207645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_input=['article-4-direction', 'article-4-direction-area', 'brownfield-land', 'conservation-area',  'listed-building-outline', 'tree', 'tree-preservation-order', 'tree-preservation-zone']\n",
    "severity_input=[] # list of issue severities you want to get e.g [\"error\", \"warning\", \"info\", \"notice\"]\n",
    "issue_type_input= input(\"Please enter a single issue type or list of issue types to search for, or leave blank for all issue types. \\n List should seperated by commas with no spaces (e.g: unknown entity,invalid geometry - fixed): \") # list of issue types you want to get e.g [\"unknown entity\", \"invalid geometry\"]\n",
    "if issue_type_input == '':\n",
    "    issue_type_input = []\n",
    "else:\n",
    "    issue_type_input = issue_type_input.split(\",\")\n",
    "    print(\"Issue types chosen: \", issue_type_input)\n",
    "line_number_input=''\n",
    "\n",
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "# Collect list of organisations\n",
    "params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select organisation, name, entity as organisation_entity\n",
    "        from organisation\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "url = f\"https://datasette.planning.data.gov.uk/digital-land.csv?{params}\"\n",
    "organisations_df = pd.read_csv(url)\n",
    "\n",
    "dataset_dfs=[]\n",
    "output_dataset_names=[]\n",
    "for dataset in dataset_input:\n",
    "    dataset_organisations_dfs=[]\n",
    "    for organisation in organisations_df.itertuples():\n",
    "        params = urllib.parse.urlencode({\n",
    "            \"sql\": f\"\"\"\n",
    "            select x.resource, x.endpoint, x.organisation, x.name, x.entry_date, x.endpoint_url, x.collection\n",
    "            from (\n",
    "                select re.resource, re.endpoint, s.organisation, o.name, s.collection, s.entry_date, e.endpoint_url,\n",
    "                    row_number() over (partition by s.organisation order by s.entry_date desc) as row_number\n",
    "                from resource_endpoint re\n",
    "                inner join endpoint e\n",
    "                on re.endpoint = e.endpoint\n",
    "                inner join source s\n",
    "                on e.endpoint = s.endpoint\n",
    "                inner join resource_organisation ro\n",
    "                on re.resource = ro.resource\n",
    "                inner join organisation o\n",
    "                on ro.organisation = o.organisation\n",
    "                inner join source_pipeline sp\n",
    "                on s.source = sp.source\n",
    "                inner join resource r\n",
    "                on re.resource = r.resource\n",
    "                where sp.pipeline = '{dataset}'\n",
    "                and s.organisation = '{organisation[1]}'\n",
    "                and s.end_date = ''                \n",
    "            ) x\n",
    "            where x.row_number = 1\n",
    "            \"\"\",\n",
    "            \"_size\": \"max\"\n",
    "        })\n",
    "\n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        endpoint_df = pd.read_csv(url)\n",
    "        \n",
    "        if (not endpoint_df.empty):\n",
    "            params = urllib.parse.urlencode({\n",
    "            \"sql\": f\"\"\"\n",
    "            select\n",
    "                r.resource\n",
    "            from\n",
    "                endpoint e\n",
    "                inner join resource_endpoint re on e.endpoint = re.endpoint\n",
    "                inner join resource r on re.resource = r.resource\n",
    "            where\n",
    "                e.endpoint_url='{endpoint_df.iloc[0]['endpoint_url']}'\n",
    "            order by\n",
    "                r.entry_date desc\n",
    "            limit 1\n",
    "            \"\"\",\n",
    "            \"_size\": \"max\"\n",
    "            })\n",
    "    \n",
    "            url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "            resource_df = pd.read_csv(url)\n",
    "            endpoint_df.iloc[0]['resource'] = resource_df.iloc[0]['resource']\n",
    "            dataset_organisations_dfs.append(endpoint_df)\n",
    "    # Check if dataset_organisations_dfs is empty\n",
    "    if (not dataset_organisations_dfs):\n",
    "        print(\"\\033[1m No results found for \", dataset)\n",
    "    else:\n",
    "        dataset_organisations_dfs = pd.concat(dataset_organisations_dfs)\n",
    "        dataset_dfs.append(dataset_organisations_dfs)\n",
    "        output_dataset_names.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = urllib.parse.urlencode({\n",
    "    \"sql\": f\"\"\"\n",
    "    select description, issue_type, severity, responsibility\n",
    "    from issue_type\n",
    "    \"\"\",\n",
    "    \"_size\": \"max\"\n",
    "})\n",
    "\n",
    "url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "issue_type_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f26e54-a995-455c-9d2c-8e902fd9117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issues_by_issue_type(resource, issue_type, query):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select field,issue_type,dataset,resource,value, line_number\n",
    "        from issue\n",
    "        where resource = '{resource}'\n",
    "        and issue_type = '{issue_type}'\n",
    "        {query}\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"{datasette_url}{dataset_input[idx]}.csv?{params}\"\n",
    "    issues_by_type_df = pd.read_csv(url)\n",
    "    return issues_by_type_df\n",
    "\n",
    "\n",
    "issues_dfs=[]\n",
    "for idx, dataset_df in enumerate(dataset_dfs):\n",
    "    query=\"\"\n",
    "    if line_number_input:\n",
    "        query = f\" and line_number = '{line_number_input}'\"\n",
    "\n",
    "    resources = dataset_df['resource'].tolist()\n",
    "    issues = []\n",
    "    for resource in resources:\n",
    "        if issue_type_input == []:\n",
    "            issue_type_input = issue_type_df['issue_type'].tolist()\n",
    "        for issue_type in issue_type_input:\n",
    "            # print(dataset_df[dataset_df['resource'] == resource]['dataset'].iloc[0], dataset_df[dataset_df['resource'] == resource]['organisation'].iloc[0], resource, issue_type )\n",
    "            issues_by_type_df = get_issues_by_issue_type(resource, issue_type, query)\n",
    "            issues.append(issues_by_type_df)\n",
    "    df1 = pd.concat(issues, ignore_index=True)\n",
    "    issues_with_type = df1.merge(issue_type_df, left_on='issue_type', right_on='issue_type')\n",
    "    issues_dfs.append(issues_with_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cace5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dfs=[]\n",
    "for idx, dataset_df in enumerate(dataset_dfs):\n",
    "    dataset_issues_df = dataset_df.merge(issues_dfs[idx], left_on='resource', right_on='resource')\n",
    "    dataset_issues_df = dataset_issues_df.reindex(columns=['resource', 'organisation', 'name', 'dataset', 'entry_date', 'field', 'line_number', 'issue_type', 'value', 'severity', 'responsibility', 'description', 'endpoint', 'endpoint_url']).reset_index(drop=True)\n",
    "    if (severity_input):\n",
    "        dataset_issues_df = dataset_issues_df.loc[dataset_issues_df['severity'].isin(severity_input)]\n",
    "    output_dfs.append(dataset_issues_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87741790-b5ae-4037-9e5b-a53e63fc5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def filter_column_by_value(df, column, value):\n",
    "    return df.loc[df[column].isin(value)]\n",
    "\n",
    "def compute_output_df(dataset, error, warning, info, notice, search):\n",
    "    index = output_dataset_names.index(dataset)\n",
    "    selected_data = []\n",
    "    output_df = output_dfs[index]\n",
    "    if error or warning or info or notice:\n",
    "        selected_data = []\n",
    "        for i in range(0, len(severity_checkboxes)):\n",
    "            if severity_checkboxes[i].value == True:\n",
    "                selected_data = selected_data + [severity_checkboxes[i].description]\n",
    "        output_df = filter_column_by_value(output_dfs[index], 'severity', selected_data)\n",
    "    if search:\n",
    "        mask = np.column_stack([output_df[col].astype('str').str.contains(search, na=False) for col in output_df])\n",
    "        output_df = output_df.loc[mask.any(axis=1)]\n",
    "    return output_df\n",
    "\n",
    "def display_output_df(dataset, error, warning, info, notice, search):\n",
    "    output_df = compute_output_df(dataset, error, warning, info, notice, search)\n",
    "    display(output_df.head(1000))\n",
    "\n",
    "def download_df(dataset, error, warning, info, notice, search):\n",
    "    output_df = compute_output_df(dataset, error, warning, info, notice, search)\n",
    "    output_df.to_csv(dataset + \"-issues.csv\")\n",
    "\n",
    "severity_options = [\"error\", \"warning\", \"info\", \"notice\"]\n",
    "severity_checkboxes = [widgets.Checkbox(value=False, description=severity) for severity in severity_options]\n",
    "\n",
    "dataset_selector = widgets.RadioButtons(\n",
    "    options=output_dataset_names,\n",
    "    description='Select dataset to display:',\n",
    "    disabled=False\n",
    ")\n",
    "download_button = widgets.Button(\n",
    "    description = \"Download output table\",\n",
    "    layout=widgets.Layout(width='200px'),\n",
    ")\n",
    "download_button.on_click(lambda b: download_df(dataset_selector.value, severity_checkboxes[0].value, severity_checkboxes[1].value, severity_checkboxes[2].value, severity_checkboxes[3].value, search_box.value))\n",
    "search_box = widgets.Text(placeholder=\"Search table\", layout=widgets.Layout(width='200px'))\n",
    "\n",
    "severity_filter = widgets.VBox(severity_checkboxes, layout = widgets.Layout(flex_flow='row wrap'))\n",
    "ui = widgets.VBox([dataset_selector, search_box, download_button, severity_filter])\n",
    "out = widgets.interactive_output(display_output_df, {'dataset': dataset_selector, \"error\": severity_checkboxes[0], \"warning\": severity_checkboxes[1], \"info\": severity_checkboxes[2], \"notice\": severity_checkboxes[3], \"search\": search_box})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03822532-f1aa-454f-b35b-212f2a91b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_output_df(dataset, error, warning, info, notice, search):\n",
    "    output_df = compute_output_df(dataset, error, warning, info, notice, search)\n",
    "    fields = output_df['field']\n",
    "    field_count = {}\n",
    "    \n",
    "    for field in fields:\n",
    "        if field in field_count:\n",
    "            field_count[field] += 1\n",
    "        else:\n",
    "            field_count[field] = 1\n",
    "    x_values = list(field_count.keys())\n",
    "    y_values = list(field_count.values())\n",
    "\n",
    "    print(\"Issue count for\",dataset,\"is\",sum(y_values))\n",
    "    figsize = (20, 5) if dataset == \"brownfield-land\" else (5, 5)\n",
    "\n",
    "    fig,ax1 = plt.subplots(1,1,figsize=figsize)\n",
    "    \n",
    "    for i, value in enumerate(y_values):\n",
    "        ax1.text(i, value, str(value),ha='center', va='bottom')\n",
    "\n",
    "    ax1.bar(x_values, y_values)\n",
    "    ax1.set_xlabel('issue field')\n",
    "    ax1.set_ylabel('count')\n",
    "    ax1.set_title('Issues Type')\n",
    "    ax1.set_xticks(x_values)\n",
    "    ax1.tick_params(axis='x', rotation=35)\n",
    "    \n",
    "ui = widgets.VBox([dataset_selector, severity_filter])\n",
    "out = widgets.interactive_output(display_output_df, {'dataset': dataset_selector, \"error\": severity_checkboxes[0], \"warning\": severity_checkboxes[1], \"info\": severity_checkboxes[2], \"notice\": severity_checkboxes[3], \"search\": search_box})\n",
    "\n",
    "display(ui,out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
