{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75aa7cf7-84e8-4b2d-9b2f-40ef543d8d0a",
   "metadata": {},
   "source": [
    "# Compare Entity Count\n",
    "**Author**:  Kena Vyas <br>\n",
    "**Date**:  2nd May 2024 <br>\n",
    "**Data Scope**: First four datasets <br>\n",
    "**Report Type**: Recurring daily <br>\n",
    "\n",
    "## Purpose\n",
    "This report provides a comparative analysis of the entity counts for each endpoint against the corresponding counts on the platform per Organisation per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752d7b3a-3ee7-45b2-9ac6-e9470abada00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c51819-c013-4473-a08d-140fe69d6bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a348e3757a9644c3a584728063135cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select Dataset:', options={'Article 4 direction': 'article-4-direc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def get_endpoint_resource_info(dataset):\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select p.organisation,p.start_date,re.name,re.collection,re.pipeline,re.endpoint,\n",
    "        re.endpoint_url,re.status,re.resource,re.latest_log_entry_date,re.endpoint_entry_date\n",
    "        from provision p join reporting_latest_endpoints re \n",
    "        on p.organisation= replace(re.organisation, '-eng', '') \n",
    "        where cohort in ('ODP-Track1','ODP-Track2','ODP-Track3','ODP-Track4','RIPA-Beta','RIPA-BOPS') and \n",
    "        re.pipeline = '{dataset}' and status='200' \n",
    "        group by endpoint\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    " \n",
    "dataset_options = {\n",
    "    \"Article 4 direction\": \"article-4-direction\",\"Article 4 direction area\": \"article-4-direction-area\",\"Conservation area\": \"conservation-area\",\n",
    "    \"Listed building outline\": \"listed-building-outline\",\"Tree\": \"tree\",\"Tree preservation order\": \"tree-preservation-order\",\"Tree preservation zone\": \"tree-preservation-zone\",    \n",
    "}\n",
    "\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=dataset_options,\n",
    "    description=\"Select Dataset:\",\n",
    ")\n",
    "\n",
    "widgets.interact(get_endpoint_resource_info, dataset=dataset_dropdown)\n",
    "initial_dataset = dataset_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b26fdd-2643-45a5-929a-b8ecac1b534a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ea04865c31456393e0df3cb3727d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select Dataset:', options={'Article 4 direction': 'article-4-direc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_endpoint_resource_info_(dataset):\n",
    "    result_df = get_endpoint_resource_info(dataset)\n",
    "    resource_list=result_df['resource']\n",
    "    dataset_input=result_df['pipeline'][0]\n",
    "    info={}\n",
    "    latest_res_list=[]\n",
    "    platform_list=[]\n",
    "    \n",
    "    for res in resource_list:\n",
    "        info[res] = []\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select count(*) from ( \n",
    "                    select rowid, end_date, fact, entry_date, entry_number, resource, start_date \n",
    "                    from fact_resource \n",
    "                    where \"resource\" ='{res}' group by entry_number\n",
    "                );\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        \n",
    "        url = f\"{datasette_url}{dataset_input}.csv?{params}\"\n",
    "        df = pd.read_csv(url)\n",
    "        info[res].append(df.iloc[0, 0])\n",
    "\n",
    "    updated_dict={}\n",
    "    for index, row in result_df.iterrows():\n",
    "        resource, organisation = row['resource'], row['organisation']\n",
    "        if resource in info:\n",
    "            updated_dict[organisation] = info[resource]\n",
    "\n",
    "    org_entity={}\n",
    "    org_list=result_df['organisation']\n",
    "    \n",
    "    for org in org_list:\n",
    "        org_entity[org] = []\n",
    "        params = urllib.parse.urlencode({\n",
    "                    \"sql\": f\"\"\"\n",
    "                    select entity from organisation where organisation = '{org}'\n",
    "                    \"\"\",\n",
    "                    \"_size\": \"max\"\n",
    "                })\n",
    "            \n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        df = pd.read_csv(url)\n",
    "        org_entity[org]=df.iloc[0, 0]\n",
    "\n",
    "    entity_count={}\n",
    "    for key,value in org_entity.items():\n",
    "        entity_count[key]=[]\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select count(*) from (select * from entity where \"organisation_entity\" = '{value}')\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        \n",
    "        url = f\"{datasette_url}{dataset_input}.csv?{params}\"\n",
    "        df = pd.read_csv(url)\n",
    "        if key in updated_dict:\n",
    "            updated_dict[key].append(df.iloc[0, 0])\n",
    "\n",
    "    filtered_data = {key: value for key, value in updated_dict.items() if value[0] != value[1]}\n",
    "\n",
    "    res_df = pd.DataFrame(filtered_data).transpose()\n",
    "    if res_df.empty:\n",
    "        return \"Entities Match for all endpoints\"\n",
    "    res_df.columns = ['Latest resource entity count', 'Platform entity count']\n",
    "    print(\"Dataset : \",dataset_input)\n",
    "\n",
    "    for key,value in updated_dict.items():\n",
    "        params = urllib.parse.urlencode({\n",
    "                    \"sql\": f\"\"\"\n",
    "                    select entity from organisation where organisation = '{key}'\n",
    "                    \"\"\",\n",
    "                    \"_size\": \"max\"\n",
    "                })\n",
    "            \n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        df = pd.read_csv(url)\n",
    "        \n",
    "        o_entity=df.iloc[0,0]\n",
    "        resource = result_df[result_df['organisation'] == key]['resource'].values[0]\n",
    "       \n",
    "        Bool=True\n",
    "        value=0\n",
    "        while Bool:\n",
    "            params = urllib.parse.urlencode({\n",
    "                    \"sql\": f\"\"\"\n",
    "                    select fe.end_date, fe.fact, fe.entry_date, fe.entry_number, fe.resource, fe.start_date,f.entity,e.reference,e.organisation_entity\n",
    "                    from fact_resource fe join fact f on fe.fact=f.fact join entity e on f.entity=e.entity\n",
    "                    where resource=\"{resource}\" and fe.entry_number>{value}\n",
    "                    group by entry_number\n",
    "                    \"\"\",\n",
    "                    \"_size\": \"max\"\n",
    "            })\n",
    "                \n",
    "            url = f\"{datasette_url}{dataset_input}.csv?{params}\"\n",
    "            df1 = pd.read_csv(url)\n",
    "            list_new=df1[['entity','reference','organisation_entity']].values.tolist()  \n",
    "            list_entity=df1['entry_number'].tolist()\n",
    "            latest_res_list.extend(list_new)\n",
    "            if list_entity:\n",
    "                value = list_entity[-1]\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "                    \n",
    "            if len(df1)<1000:\n",
    "                Bool=False\n",
    "    \n",
    "        Bool=True\n",
    "        value=0\n",
    "        while Bool:\n",
    "            params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select dataset, end_date, entity, entry_date, geojson, geometry, json, name, organisation_entity, point, prefix, reference, start_date, typology from entity where organisation_entity = \"{o_entity}\"\n",
    "                and entity>{value}\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "            \n",
    "            url1 = f\"{datasette_url}{dataset_input}.csv?{params}\"\n",
    "            df2 = pd.read_csv(url1)\n",
    "            list_new=df2[['entity','reference','organisation_entity']].values.tolist()\n",
    "            list_entity=df2['entity'].tolist()\n",
    "            platform_list.extend(list_new)\n",
    "            if list_entity:\n",
    "                value =list_entity[-1]\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            if len(df2)<1000:\n",
    "                Bool=False\n",
    "    \n",
    "    ele = {item[0] for item in latest_res_list}\n",
    "    filtered_list = [item for item in platform_list if item[0] not in ele]\n",
    "    result_df = pd.DataFrame(filtered_list, columns=['entity', 'reference', 'organisation_entity'])\n",
    "    result_df.to_csv('result.csv', index=False)\n",
    "    \n",
    "    print(\"File result.csv downloaded\")\n",
    "    return res_df\n",
    "\n",
    "widgets.interact(get_endpoint_resource_info_, dataset=dataset_dropdown)\n",
    "initial_dataset = dataset_dropdown.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
