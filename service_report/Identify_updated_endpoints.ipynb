{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afb3c9d-3a41-4fda-9035-03decda8a609",
   "metadata": {},
   "source": [
    "# Identify Updated Endpoints\n",
    "**Author**:  Kena Vyas <br>\n",
    "**Date**:  23rd Feb 2024 <br>\n",
    "**Data Scope**: First four datasets <br>\n",
    "**Report Type**: Recurring daily <br>\n",
    "\n",
    "## Purpose\n",
    "This Report gets a list of endpoints that have updated along with the entity count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6712b7b-71e9-4ab9-bcbd-42981a5fdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f092d394-ecee-4141-91fe-b5c4daf7192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914708f7c84e4ee78ac9136703b9a317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options={'Article 4 Direction': 'article-4-direction', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def get_all_endpoints(dataset):\n",
    "    global result_df\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select t1.endpoint,\n",
    "        t3.pipeline from(select endpoint from resource_endpoint \n",
    "        group by endpoint having count(endpoint)>1) t1 \n",
    "        join source t2 on t1.endpoint=t2.endpoint \n",
    "        join source_pipeline t3 on t2.source=t3.source \n",
    "        where t3.pipeline='{dataset}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    result_df = df\n",
    "    return df\n",
    "   \n",
    "dataset_options = {\n",
    "    \"Article 4 Direction\":\"article-4-direction\",\"Article 4 Direction Area\": \"article-4-direction-area\",\"Conservation Area\": \"conservation-area\",\"Listed Building Outline\": \"listed-building-outline\",\n",
    "    \"Tree\":\"tree\",\"Tree Preservation Zone\": \"tree-preservation-zone\",\"Tree\":\"tree\",\"Tree Preservation Order\":\"tree-preservation-order\"\n",
    "}\n",
    "\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=dataset_options,\n",
    ")\n",
    "\n",
    "widgets.interact(get_all_endpoints, dataset=dataset_options)\n",
    "initial_dataset = dataset_dropdown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2be13-11d0-4b29-9d40-427a180a3b2d",
   "metadata": {},
   "source": [
    "This table lists the endpoints along with its endpoint URL and the number of entities updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4f2d6b-f64c-48a2-8d47-a8c6a16d0c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint</th>\n",
       "      <th>update</th>\n",
       "      <th>endpoint_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03b17776d0e7707bdb98768bdcac82cf7d01595a353603...</td>\n",
       "      <td>20 entities added</td>\n",
       "      <td>https://services-eu1.arcgis.com/xk4RA36G57mVH7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            endpoint             update  \\\n",
       "0  03b17776d0e7707bdb98768bdcac82cf7d01595a353603...  20 entities added   \n",
       "\n",
       "                                        endpoint_url  \n",
       "0  https://services-eu1.arcgis.com/xk4RA36G57mVH7...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_dataset=result_df['pipeline'][0]\n",
    "get_resource={}\n",
    "\n",
    "for index,row in result_df.iterrows():\n",
    "    resource_endpoint=''\n",
    "    params = urllib.parse.urlencode({\n",
    "            \"sql\": f\"\"\"\n",
    "            select re.resource, re.endpoint, r.start_date from resource_endpoint re inner join resource r on re.resource=r.resource\n",
    "            where re.endpoint = '{row['endpoint']}'\n",
    "            \"\"\",\n",
    "            \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    resource_endpoint = pd.read_csv(url)\n",
    "\n",
    "    resource_endpoint.sort_values(by='start_date', ascending=False, inplace=True)\n",
    "    resource_endpoint.reset_index(drop=True, inplace=True)\n",
    "    df=resource_endpoint.head(2)\n",
    "    grouped_data = df.groupby('endpoint').apply(lambda group: [dict(zip(['resource', 'start_date'], values)) for values in group[['resource', 'start_date']].values]).reset_index(name='resource_start_date_list')\n",
    "    get_resource.update(dict(zip(grouped_data['endpoint'], grouped_data['resource_start_date_list'])))\n",
    "\n",
    "all_resource_count={}\n",
    "for key,list_resource in get_resource.items():\n",
    "    per_resource_count=[]\n",
    "    for ele in list_resource:\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM ( select rowid, end_date, fact, entry_date, entry_number, resource, start_date \n",
    "                from fact_resource \n",
    "                where \"resource\" ='{ele['resource']}' group by entry_number);\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        url = f\"{datasette_url}{selected_dataset}.csv?{params}\"\n",
    "        count = pd.read_csv(url)\n",
    "        per_resource_count.append(count.iloc[0, 0])\n",
    "    all_resource_count[key] = per_resource_count\n",
    "\n",
    "endpoints_with_diff_count={}\n",
    "for key,value in all_resource_count.items():\n",
    "    flag = True if value[0] != value[1] else False\n",
    "    if flag:\n",
    "        diff=value[0]-value[1]\n",
    "        if diff < 0:\n",
    "            if diff==-1:\n",
    "                msg = str(abs(diff)) + ' entity deleted'\n",
    "            else:\n",
    "                msg = str(abs(diff)) + ' entities deleted'\n",
    "        elif diff == 1:\n",
    "            msg = str(diff) + ' entity added'\n",
    "        else:\n",
    "            msg = str(diff) + ' entities added'\n",
    "        endpoints_with_diff_count[key]=msg\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "if len(endpoints_with_diff_count)>1:\n",
    "    endpoint_tuple=tuple(endpoints_with_diff_count)\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select endpoint, endpoint_url from endpoint where endpoint in {endpoint_tuple}\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "else:\n",
    "    endpoint_tuple=list(endpoints_with_diff_count.keys())[0]\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select endpoint, endpoint_url from endpoint where endpoint =='{endpoint_tuple}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "to_df = pd.DataFrame.from_dict(endpoints_with_diff_count, orient='index').reset_index()\n",
    "to_df = to_df.rename(columns={'index': 'endpoint',0:'update'})\n",
    "updated_endpoints = pd.merge(to_df, df, on='endpoint', how='left')\n",
    "updated_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f787555-65e6-4e08-868c-6999491ce3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to download the result? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result downloaded as 'updated_endpoints.csv'\n"
     ]
    }
   ],
   "source": [
    "download = input(\"Do you want to download the result? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    updated_endpoints.to_csv(\"updated_endpoints.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'updated_endpoints.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
