{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2afb3c9d-3a41-4fda-9035-03decda8a609",
   "metadata": {},
   "source": [
    "# Identify Updated Endpoints\n",
    "**Author**:  Kena Vyas <br>\n",
    "**Date**:  23rd Feb 2024 <br>\n",
    "**Data Scope**: First four datasets <br>\n",
    "**Report Type**: Recurring daily <br>\n",
    "\n",
    "## Purpose\n",
    "This Report gets a list of endpoints that have updated along with the entity count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6712b7b-71e9-4ab9-bcbd-42981a5fdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f092d394-ecee-4141-91fe-b5c4daf7192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def get_all_endpoints(dataset):\n",
    "    global result_df\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select \n",
    "            t1.endpoint,\n",
    "            t3.pipeline \n",
    "        from(\n",
    "            select \n",
    "                endpoint \n",
    "            from \n",
    "                resource_endpoint \n",
    "            group by \n",
    "                endpoint \n",
    "            having \n",
    "                count(endpoint)>1\n",
    "            ) t1 \n",
    "            join source t2 on t1.endpoint=t2.endpoint \n",
    "            join source_pipeline t3 on t2.source=t3.source \n",
    "        where \n",
    "            t3.pipeline='{dataset}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    result_df = df\n",
    "    return df\n",
    "   \n",
    "dataset_options = {\n",
    "    \"Article 4 Direction\":\"article-4-direction\",\"Article 4 Direction Area\": \"article-4-direction-area\",\"Conservation Area\": \"conservation-area\",\"Listed Building Outline\": \"listed-building-outline\",\n",
    "    \"Tree\":\"tree\",\"Tree Preservation Zone\": \"tree-preservation-zone\",\"Tree\":\"tree\",\"Tree Preservation Order\":\"tree-preservation-order\"\n",
    "}\n",
    "\n",
    "global dataset_dropdown\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=dataset_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2be13-11d0-4b29-9d40-427a180a3b2d",
   "metadata": {},
   "source": [
    "This table lists the endpoints along with its endpoint URL and the number of entities updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4f2d6b-f64c-48a2-8d47-a8c6a16d0c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7926c62e49c4e5bb0433a6c58dc6366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options={'Article 4 Direction': 'article-4-direction', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=pd.DataFrame()\n",
    "\n",
    "def get_all_endpoints_result(dataset):\n",
    "    global result\n",
    "    result_df=get_all_endpoints(dataset)\n",
    "    selected_dataset=result_df['pipeline'][0]\n",
    "    get_resource={}\n",
    "    for index,row in result_df.iterrows():\n",
    "        resource_endpoint=''\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select \n",
    "                    re.resource, re.endpoint, r.start_date \n",
    "                from \n",
    "                    resource_endpoint re \n",
    "                inner join \n",
    "                    resource r on re.resource=r.resource\n",
    "                where \n",
    "                    re.endpoint = '{row['endpoint']}'\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        resource_endpoint = pd.read_csv(url)\n",
    "    \n",
    "        resource_endpoint.sort_values(by='start_date', ascending=False, inplace=True)\n",
    "        resource_endpoint.reset_index(drop=True, inplace=True)\n",
    "        df=resource_endpoint.head(2)\n",
    "        grouped_data = df.groupby('endpoint').apply(lambda group: [dict(zip(['resource', 'start_date'], values)) for values in group[['resource', 'start_date']].values]).reset_index(name='resource_start_date_list')\n",
    "        get_resource.update(dict(zip(grouped_data['endpoint'], grouped_data['resource_start_date_list'])))\n",
    "    \n",
    "    all_resource_count={}\n",
    "    for key,list_resource in get_resource.items():\n",
    "        per_resource_count=[]\n",
    "        for ele in list_resource:\n",
    "            params = urllib.parse.urlencode({\n",
    "                    \"sql\": f\"\"\"\n",
    "                    SELECT COUNT(*)\n",
    "                    FROM ( \n",
    "                        select rowid, end_date, fact, entry_date, entry_number, resource, start_date \n",
    "                        from fact_resource \n",
    "                        where \"resource\" ='{ele['resource']}' group by entry_number\n",
    "                         );\n",
    "                    \"\"\",\n",
    "                    \"_size\": \"max\"\n",
    "                })\n",
    "            url = f\"{datasette_url}{selected_dataset}.csv?{params}\"\n",
    "            count = pd.read_csv(url)\n",
    "            per_resource_count.append(count.iloc[0, 0])\n",
    "        all_resource_count[key] = per_resource_count\n",
    "    \n",
    "    endpoints_with_diff_count={}\n",
    "    for key,value in all_resource_count.items():\n",
    "        flag = True if value[0] != value[1] else False\n",
    "        if flag:\n",
    "            diff=value[0]-value[1]\n",
    "            if diff < 0:\n",
    "                if diff==-1:\n",
    "                    msg = str(abs(diff)) + ' entity deleted'\n",
    "                else:\n",
    "                    msg = str(abs(diff)) + ' entities deleted'\n",
    "            elif diff == 1:\n",
    "                msg = str(diff) + ' entity added'\n",
    "            else:\n",
    "                msg = str(diff) + ' entities added'\n",
    "            endpoints_with_diff_count[key]=msg\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    if len(endpoints_with_diff_count)>1:\n",
    "        endpoint_tuple=tuple(endpoints_with_diff_count)\n",
    "        params = urllib.parse.urlencode({\n",
    "            \"sql\": f\"\"\"\n",
    "            select \n",
    "                e.endpoint, e.endpoint_url, o.name from endpoint e inner join source s\n",
    "                on e.endpoint = s.endpoint\n",
    "                inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "            where \n",
    "                e.endpoint in {endpoint_tuple}\n",
    "            \"\"\",\n",
    "            \"_size\": \"max\"\n",
    "        })\n",
    "    else:\n",
    "        endpoint_tuple=list(endpoints_with_diff_count.keys())[0]\n",
    "        params = urllib.parse.urlencode({\n",
    "            \"sql\": f\"\"\"\n",
    "            select \n",
    "                e.endpoint, e.endpoint_url, o.name from endpoint e inner join source s\n",
    "                on e.endpoint = s.endpoint\n",
    "                inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "            where \n",
    "                e.endpoint =='{endpoint_tuple}'\n",
    "            \"\"\",\n",
    "            \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    to_df = pd.DataFrame.from_dict(endpoints_with_diff_count, orient='index').reset_index()\n",
    "    to_df = to_df.rename(columns={'index': 'endpoint',0:'update'})\n",
    "    updated_endpoints = pd.merge(to_df, df, on='endpoint', how='left')\n",
    "    result=updated_endpoints\n",
    "    print(\"Dataset : \",dataset)\n",
    "    return updated_endpoints\n",
    "\n",
    "widgets.interact(get_all_endpoints_result, dataset=dataset_dropdown)\n",
    "initial_dataset = dataset_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f787555-65e6-4e08-868c-6999491ce3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eda4049190460d8f83f5fb67bc4f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options={'Article 4 Direction': 'article-4-direction', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=pd.DataFrame()\n",
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "def track_all_endpoints(dataset):\n",
    "    global output_df\n",
    "    result_df=get_all_endpoints(dataset)\n",
    "    selected_dataset=result_df['pipeline'][0]\n",
    "    get_resource={}\n",
    "    \n",
    "    for index,row in result_df.iterrows():\n",
    "        resource_endpoint=''\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select \n",
    "                    re.resource, re.endpoint, r.start_date \n",
    "                from \n",
    "                    resource_endpoint re \n",
    "                inner join \n",
    "                    resource r on re.resource=r.resource\n",
    "                where \n",
    "                    re.endpoint = '{row['endpoint']}'\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        resource_endpoint = pd.read_csv(url)\n",
    "        resource_endpoint.sort_values(by='start_date', ascending=False, inplace=True)\n",
    "        resource_endpoint.reset_index(drop=True, inplace=True)\n",
    "        grouped_data = resource_endpoint.groupby('endpoint').apply(lambda group: [dict(zip(['resource', 'start_date'], values)) for values in group[['resource', 'start_date']].values]).reset_index(name='resource_start_date_list')\n",
    "        get_resource.update(dict(zip(grouped_data['endpoint'], grouped_data['resource_start_date_list'])))\n",
    "\n",
    "    all_resource_count={}\n",
    "    for key,list_resource in get_resource.items():\n",
    "        per_resource_count={}\n",
    "        for ele in list_resource:\n",
    "            params = urllib.parse.urlencode({\n",
    "                    \"sql\": f\"\"\"\n",
    "                    SELECT COUNT(*)\n",
    "                    FROM ( \n",
    "                        select rowid, end_date, fact, entry_date, entry_number, resource, start_date \n",
    "                        from fact_resource \n",
    "                        where \"resource\" ='{ele['resource']}' group by entry_number\n",
    "                         );\n",
    "                    \"\"\",\n",
    "                    \"_size\": \"max\"\n",
    "                })\n",
    "            url = f\"{datasette_url}{selected_dataset}.csv?{params}\"\n",
    "            count = pd.read_csv(url)\n",
    "            if count.iloc[0, 0] != 0:\n",
    "                per_resource_count[ele['start_date']]=count.iloc[0, 0]\n",
    "        if per_resource_count and len(per_resource_count) > 1:\n",
    "            value=list(per_resource_count.values())\n",
    "            if not all(element == value[0] for element in value):\n",
    "                all_resource_count[key] = per_resource_count\n",
    "\n",
    "    dfs = []\n",
    "    for endpoint, values in all_resource_count.items():\n",
    "        df_endpoint = pd.DataFrame(list(values.items()), columns=['start_date', 'value'])\n",
    "        df_endpoint['start_date'] = pd.to_datetime(df_endpoint['start_date'])\n",
    "        df_endpoint.sort_values('start_date', inplace=True)\n",
    "        df_endpoint['value_diff'] = df_endpoint['value'].diff().fillna(0).astype(int)\n",
    "        df_endpoint['Endpoint'] = endpoint\n",
    "        dfs.append(df_endpoint)\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    endpoint_list = result_df['Endpoint'].drop_duplicates().tolist()\n",
    "    lpa_endpoint=[]\n",
    "    for endpoint in endpoint_list:\n",
    "        params = urllib.parse.urlencode({\n",
    "                \"sql\": f\"\"\"\n",
    "                select \n",
    "                    e.endpoint, o.name \n",
    "                from \n",
    "                    endpoint e \n",
    "                    inner join source s on e.endpoint = s.endpoint\n",
    "                    inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "                where \n",
    "                    e.endpoint='{endpoint}'\n",
    "                \"\"\",\n",
    "                \"_size\": \"max\"\n",
    "            })\n",
    "        url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "        df = pd.read_csv(url)\n",
    "        lpa_endpoint.append(df)\n",
    "    lpa_reshaped = np.reshape(lpa_endpoint, (-1, 2))\n",
    "    endpoint_df = pd.DataFrame(lpa_reshaped, columns=['Endpoint', 'Name'])\n",
    "    output_df = pd.merge(result_df, endpoint_df, on='Endpoint')\n",
    "    \n",
    "    ordering=['Endpoint','Name','start_date','value','value_diff']\n",
    "    output_df=output_df[ordering]\n",
    "    print(\"Dataset : \",dataset)\n",
    "    return output_df\n",
    "\n",
    "widgets.interact(track_all_endpoints, dataset=dataset_dropdown)\n",
    "initial_dataset = dataset_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28a2664-bdad-4887-aae5-0cb8543f0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to download the result? (yes/no):  no\n"
     ]
    }
   ],
   "source": [
    "download = input(\"Do you want to download the result? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    output_df.to_csv(\"record_updates.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'record_updates.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
