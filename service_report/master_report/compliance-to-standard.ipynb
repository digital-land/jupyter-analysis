{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides compliance to specification information on the most latest endpoints for a hardcoded list of prioritised list of LPAs, or organisations from an input.\n",
    "\n",
    "The column 'structure_score' tells us how much data an endpoint is giving us as a fraction of what we ask for. The column 'column_name_score' tells us how many columns are correctly named.\n",
    "\n",
    "Example: a column name that is incorrect (e.g 'area' instead of 'geometry') but the data in it has been detected as correct data will score in the 'structure_score' column but not the 'column_name' column\n",
    "\n",
    "The input should be called 'organisation_input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wget\n",
    "import wget\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download helper utility files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default prioritised LPAs are used unless a specific set of LPAs is detected using an 'organisation_input.csv' file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './organisation_input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    organisation_list = [\n",
    "    'local-authority-eng:BUC', \n",
    "    'local-authority-eng:DAC', 'local-authority-eng:DNC',\n",
    "    'local-authority-eng:GLO', 'local-authority-eng:CMD', 'local-authority-eng:LBH', 'local-authority-eng:SWK',\n",
    "    'local-authority-eng:MDW', 'local-authority-eng:NET', 'local-authority-eng:BIR', 'local-authority-eng:CAT',\n",
    "    'local-authority-eng:EPS', 'local-authority-eng:BNE', 'local-authority-eng:GAT', 'local-authority-eng:GRY',\n",
    "    'local-authority-eng:KTT', 'local-authority-eng:SAL', 'local-authority-eng:TEW', 'local-authority-eng:WBK',\n",
    "    'local-authority-eng:DST', 'local-authority-eng:DOV', 'local-authority-eng:LIV', 'local-authority-eng:RDB',\n",
    "    'local-authority-eng:WFT', 'local-authority-eng:NLN', 'local-authority-eng:NSM', 'local-authority-eng:SLF',\n",
    "    'local-authority-eng:WRL' ]\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_in_endpoint(fields, dataset_field_df, column_field_df, dataset):\n",
    "    dataset_columns = dataset_field_df['field'].tolist()\n",
    "    # Remove automatically assigned columns by the pipeline from scoring\n",
    "    dataset_columns = remove_assigned_columns(dataset, dataset_columns)\n",
    "    \n",
    "    missing_columns = []\n",
    "    present_columns = []\n",
    "    # Count whether columns in the specification are present in the endpoint\n",
    "    for column in dataset_columns:\n",
    "        if column not in fields:\n",
    "            missing_columns.append(column)\n",
    "        else:\n",
    "            present_columns.append(column)\n",
    "    structure_score = f\"{len(dataset_columns) - len(missing_columns)}/{len(dataset_columns)}\"\n",
    "    structure_percentage = (len(dataset_columns) - len(missing_columns)) / len(dataset_columns) * 100\n",
    "\n",
    "    # The WKT column is removed from the column_field mapping as it is autogenerated by the pipeline for some file formats (e.g geojson)\n",
    "    filtered_columns = [\"WKT\"]\n",
    "    column_field_df = column_field_df[-column_field_df['column'].isin(filtered_columns)]\n",
    "\n",
    "    mapped_fields = column_field_df['field'].tolist()\n",
    "    # print(\"present columns: \", present_columns)\n",
    "    # print(\"missing columns: \", missing_columns)\n",
    "    # print(\"column field mapping: \\n\", column_field_df)\n",
    "    correct_column_names = 0\n",
    "    for field in present_columns:\n",
    "        # If a field isn't present in the mapped fields it is correctly named\n",
    "        # Or if the column name is the same as the field name it is correctly named\n",
    "        if field not in mapped_fields or column_field_df[column_field_df['field'] == field]['column'].tolist()[0] == field:\n",
    "            correct_column_names += 1\n",
    "    \n",
    "    column_score = f\"{correct_column_names}/{len(dataset_columns)}\"\n",
    "    column_percentage = (correct_column_names)/ len(dataset_columns)*100\n",
    "   \n",
    "    return structure_score, structure_percentage, column_score, column_percentage\n",
    "\n",
    "\n",
    "def get_fields_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select f.field \n",
    "        from \n",
    "            fact_resource fr\n",
    "            inner join fact f on fr.fact = f.fact\n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        group by\n",
    "            f.field\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    facts_df = pd.read_csv(url)\n",
    "    facts_list = facts_df['field'].tolist()\n",
    "    return facts_list\n",
    "\n",
    "def get_column_mappings_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select column, field\n",
    "        from \n",
    "          column_field  \n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    column_field_df = pd.read_csv(url)\n",
    "    return column_field_df\n",
    "\n",
    "def remove_assigned_columns(dataset, dataset_columns):\n",
    "    # These columns are auto generated by the pipeline therefore not used in the scoring\n",
    "    dataset_columns.remove('entity')\n",
    "    dataset_columns.remove('organisation')\n",
    "    dataset_columns.remove('prefix')\n",
    "    if dataset != \"tree\" and \"point\" in dataset_columns:\n",
    "        dataset_columns.remove('point')\n",
    "    return dataset_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of organisation names, to be displayed in the output table. This is gathered separately from the main data, to ensure that if an organisation has not provided any endpoints, it is still included in the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get organisation names for output table\n",
    "organisation_info_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/organisation-collection/main/data/local-authority.csv')\n",
    "organisation_info_df.head()\n",
    "organisation_name_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    organisation_code = organisation.split(':')[1]\n",
    "    organisation_name = organisation_info_df.loc[organisation_info_df['reference'] == organisation_code].iloc[0]['name']\n",
    "    organisation_name_dict[organisation] = organisation_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest endpoints are collected for each of the organisations, for the first 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "# Collect latest endpoints for each organisation\n",
    "dataset_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "pipelines_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree', 'tree,tree-preservation-order', 'tree-preservation-order,tree-preservation-zone']\n",
    "all_orgs_latest_endpoints={}\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = get_latest_endpoints(organisation)\n",
    "    latest_endpoints_df = latest_endpoints_df[latest_endpoints_df['pipelines'].isin(pipelines_list)]\n",
    "    all_orgs_latest_endpoints[organisation] = latest_endpoints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these endpoints, the relevant schema for the dataset is downloaded to compare the endpoint columns against.\n",
    "\n",
    "'Structure score' is the number of columns in the processed data that match the schema, divided by the number of columns in the schema. Note that if there is no data at all in a field, it cannot be detected as a structure match.\n",
    "\n",
    "'Column name score' is the number of columns in the processed data that had matching column names to the schema before any processing happened (ie no column mapping had to take place). Note that if there is no data at all in a field, it cannot be detected as a column name match.\n",
    "\n",
    "If an endpoint contributes to two datasets, it will only be considered for a dataset if it is the newest endpoint for that dataset, calculated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_colour(value):\n",
    "    if \"%\" in value:\n",
    "        value = int(value.replace(\"%\", \"\"))\n",
    "        if value >= 75:\n",
    "            return 'background-color: green'\n",
    "        elif value < 75 and value >= 50:\n",
    "            return 'background-color: orange'\n",
    "        elif 0 <= value < 50:\n",
    "            return 'background-color: red'\n",
    "        else:\n",
    "            return 'background-color: brown'\n",
    "\n",
    "organisation_dataset_compliance_dict={}\n",
    "rows_list = []\n",
    "csv_rows_list = []\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = all_orgs_latest_endpoints[organisation]\n",
    "    dataset_compliance_dict = {}\n",
    "    for index, row in latest_endpoints_df.iterrows():\n",
    "        resource = row['resource']\n",
    "        if ',' in row['pipelines']:\n",
    "            datasets = row['pipelines'].split(',')\n",
    "        else:\n",
    "            datasets = [row['pipelines']]\n",
    "        for dataset in datasets:\n",
    "            same_datasets_df = latest_endpoints_df[latest_endpoints_df[\"pipelines\"].apply(lambda x: dataset in x.split(','))]\n",
    "            if len(same_datasets_df) > 1:\n",
    "                skip_dataset = handle_skip_dataset(same_datasets_df, dataset, row)\n",
    "            else:\n",
    "                skip_dataset = False\n",
    "            # print(organisation, dataset, resource)\n",
    "            \n",
    "\n",
    "            dataset_field_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "            dataset_field_df = dataset_field_df[dataset_field_df['dataset'] == dataset]\n",
    "\n",
    "            if not skip_dataset:\n",
    "                column_field_df = get_column_mappings_for_resource(resource, dataset)\n",
    "                fields = get_fields_for_resource(resource, dataset)\n",
    "                structure_score, structure_percentage, column_score, column_percentage = check_columns_in_endpoint(fields, dataset_field_df, column_field_df, dataset)\n",
    "                overall_percentage = (structure_percentage + column_percentage) / 2\n",
    "                dataset_compliance_dict[dataset] = {\"structure_score\": structure_score, \"structure_percentage\": structure_percentage, \"column_score\": column_score, \"column_name_percentage\": column_percentage}\n",
    "                new_row = {'organisation': organisation_name_dict[organisation], 'dataset': dataset, 'structure_score': structure_score, 'structure_percentage': f\"{int(structure_percentage)}%\" , 'column_name_score': column_score, 'column_name_percentage': f\"{int(column_percentage)}%\", 'overall_percentage': f\"{int(overall_percentage)}%\"}\n",
    "                rows_list.append(new_row)\n",
    "                csv_row = new_row.copy()\n",
    "                csv_row['endpoint_url'] = row['endpoint_url']\n",
    "                csv_row['resource'] = row['resource']\n",
    "                csv_rows_list.append(csv_row)\n",
    "    \n",
    "    organisation_dataset_compliance_dict[organisation] = dataset_compliance_dict\n",
    "\n",
    "\n",
    "compliance_df = pd.DataFrame(rows_list)\n",
    "output_df = pd.DataFrame(csv_rows_list)\n",
    "output_df.to_csv('compliance.csv', index=False)\n",
    "compliance_df.style.applymap(compute_cell_colour, subset=[\"structure_percentage\", \"column_name_percentage\", \"overall_percentage\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
