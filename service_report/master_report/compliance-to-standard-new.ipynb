{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides compliance to specification information on the most latest endpoints for a hardcoded list of prioritised list of LPAs, or organisations from an input.\n",
    "\n",
    "The column 'structure_score' tells us how much data an endpoint is giving us as a fraction of what we ask for. The column 'column_name_score' tells us how many columns are correctly named.\n",
    "\n",
    "Example: a column name that is incorrect (e.g 'area' instead of 'geometry') but the data in it has been detected as correct data will score in the 'structure_score' column but not the 'column_name' column\n",
    "\n",
    "The input should be called 'organisation_input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wget\n",
    "import wget\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download helper utility files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default prioritised LPAs are used unless a specific set of LPAs is detected using an 'organisation_input.csv' file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './organisation_input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    provision_df = get_provisions()\n",
    "    organisation_list = provision_df[\"organisation\"].str.replace(\":\",\"-eng:\")\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_resource_data():\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "            e.endpoint_url,\n",
    "            l.endpoint,\n",
    "            l.status,\n",
    "            l.exception,\n",
    "            s.collection,\n",
    "            l.resource,\n",
    "            sp.pipeline,\n",
    "            s.organisation,\n",
    "            o.name,\n",
    "            l.entry_date as log_entry_date,\n",
    "            e.entry_date as endpoint_entry_date,\n",
    "            e.end_date as endpoint_end_date,\n",
    "            r.start_date as resource_start_date,\n",
    "            r.end_date as resource_end_date\n",
    "        from\n",
    "            most_recent_log l\n",
    "            inner join source s on l.endpoint = s.endpoint\n",
    "            inner join endpoint e on l.endpoint = e.endpoint\n",
    "            inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "            inner join source_pipeline sp on s.source = sp.source\n",
    "            left join resource r on l.resource = r.resource\n",
    "        where\n",
    "            sp.pipeline IN ('article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree')\n",
    "\n",
    "        order by s.organisation, sp.pipeline, log_entry_date desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def get_fields_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select f.field, fr.resource\n",
    "        from \n",
    "            fact_resource fr\n",
    "            inner join fact f on fr.fact = f.fact\n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        group by\n",
    "            f.field\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    facts_df = pd.read_csv(url)\n",
    "    # facts_list = facts_df['field'].tolist()\n",
    "    return facts_df\n",
    "\n",
    "def get_column_mappings_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select column, field\n",
    "        from \n",
    "          column_field  \n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    column_field_df = pd.read_csv(url)\n",
    "    return column_field_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get endpoint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from datasette\n",
    "endpoint_resource_df = get_endpoint_resource_data()\n",
    "\n",
    "# filter to org_list, valid, active endpoints and resources\n",
    "endpoint_resource_filtered_df = endpoint_resource_df[\n",
    "    (endpoint_resource_df[\"organisation\"].isin(organisation_list)) &\n",
    "    (endpoint_resource_df[\"status\"] == 200) &\n",
    "    (endpoint_resource_df[\"endpoint_end_date\"].isnull()) &\n",
    "    (endpoint_resource_df[\"resource_end_date\"].isnull())\n",
    "].copy()\n",
    "\n",
    "print(len(endpoint_resource_df))\n",
    "print(len(endpoint_resource_filtered_df))\n",
    "\n",
    "print(len(endpoint_resource_filtered_df[[\"endpoint\", \"pipeline\"]].drop_duplicates()))\n",
    "print(len(endpoint_resource_filtered_df[[\"resource\"]].drop_duplicates()))\n",
    "print(len(endpoint_resource_filtered_df[[\"endpoint\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get field and col mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of unique resources and pipelines\n",
    "resource_df = endpoint_resource_filtered_df[[\"pipeline\", \"resource\"]].drop_duplicates().dropna(axis = 0)\n",
    "print(len(resource_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function to try the resource datasette queries \n",
    "# will return a df with resource and dataset fields as keys, and query results as other fields\n",
    "def try_results(function, resource, dataset):\n",
    "\n",
    "    # try grabbing results\n",
    "    try:\n",
    "        df = function(resource, dataset)\n",
    "\n",
    "        # if empty response give NaNs\n",
    "        if len(df) == 0:\n",
    "            df = pd.DataFrame({\"field\" : [np.nan]\n",
    "            })\n",
    "\n",
    "        df[\"resource\"] = resource\n",
    "        df[\"dataset\"] = dataset\n",
    "\n",
    "    # if error record resource and dataset\n",
    "    except:\n",
    "        df = pd.DataFrame({\"resource\" : [resource],\n",
    "                           \"dataset\" : [dataset]\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# get results for col mappings and fields in arrays\n",
    "results_col_map = [try_results(get_column_mappings_for_resource, r[\"resource\"], r[\"pipeline\"]) for index, r in resource_df.iterrows()]\n",
    "results_field_resource = [try_results(get_fields_for_resource, r[\"resource\"], r[\"pipeline\"]) for index, r in resource_df.iterrows()]\n",
    "\n",
    "# concat the results, resources which errored with have NaNs in query results fields\n",
    "results_col_map_df = pd.concat(results_col_map)\n",
    "results_field_resource_df = pd.concat(results_field_resource)\n",
    "\n",
    "# no. of resources in each query response array\n",
    "print(len(results_col_map))\n",
    "print(len(results_field_resource))\n",
    "\n",
    "# no of records in each results df\n",
    "print(len(results_col_map_df))\n",
    "print(len(results_field_resource_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in match field for column mappings \n",
    "results_col_map_df[\"field_matched\"] = np.where(\n",
    "        (results_col_map_df[\"field\"].isin([\"geometry\", \"point\"])) |\n",
    "        (results_col_map_df[\"field\"] == results_col_map_df[\"column\"]),\n",
    "        1, \n",
    "        0\n",
    ")\n",
    "\n",
    "# add in flag for fields supplied (i.e. they're in the mapping table)\n",
    "results_col_map_df[\"field_supplied\"] = 1\n",
    "\n",
    "# add in flag for fields present\n",
    "results_field_resource_df[\"field_loaded\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating match rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_field_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "\n",
    "# remove the pipeline-created fields from the spec field table\n",
    "# (\"entity\", \"organisation\", \"prefix\", \"point\" for all but tree, and \"entity\", \"organisation\", \"prefix\" for tree)\n",
    "dataset_field_subset_df = dataset_field_df[\n",
    "    ((dataset_field_df[\"dataset\"] != \"tree\") & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\", \"point\"])) |\n",
    "     (dataset_field_df[\"dataset\"] == \"tree\") & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\"])))\n",
    "]\n",
    "\n",
    "dataset_field_subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename pipeline to dataset in endpoint_resource table\n",
    "endpoint_resource_filtered_df.rename(columns={\"pipeline\":\"dataset\"}, inplace=True)\n",
    "\n",
    "# left join from endpoint resource table to all the fields that each dataset should have\n",
    "resource_spec_fields_df = endpoint_resource_filtered_df[\n",
    "    [\"organisation\", \"name\", \"dataset\", \"endpoint\", \"status\", \"log_entry_date\", \"endpoint_entry_date\", \"resource\"]\n",
    "    ].merge(\n",
    "        dataset_field_subset_df[[\"dataset\", \"field\"]],\n",
    "        on = \"dataset\"\n",
    ")\n",
    "\n",
    "print(len(resource_spec_fields_df))\n",
    "resource_spec_fields_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on field present flag for each resource\n",
    "resource_fields_match = resource_spec_fields_df.merge(\n",
    "    results_field_resource_df[[\"dataset\", \"resource\", \"field\", \"field_loaded\"]],\n",
    "    how = \"left\",\n",
    "    on = [\"dataset\", \"resource\", \"field\"]\n",
    ")\n",
    "\n",
    "print(len(resource_fields_match))\n",
    "resource_fields_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on field present flag for each resource\n",
    "resource_fields_map_match = resource_fields_match.merge(\n",
    "    results_col_map_df[[\"dataset\", \"resource\", \"field\", \"field_supplied\", \"field_matched\"]],\n",
    "    how = \"left\",\n",
    "    on = [\"dataset\", \"resource\", \"field\"]\n",
    ")\n",
    "\n",
    "print(len(resource_fields_map_match))\n",
    "resource_fields_map_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_fields_map_match.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "final_count = resource_fields_map_match.groupby(\n",
    "    [\"organisation\", \"name\", \"dataset\", \"endpoint\", \"resource\", \"status\", \"log_entry_date\", \"endpoint_entry_date\"]\n",
    "    ).agg(\n",
    "        {\"field\":\"count\",\n",
    "         \"field_supplied\" : \"sum\",\n",
    "         \"field_matched\" : \"sum\",\n",
    "         \"field_loaded\" : \"sum\"}\n",
    "         ).reset_index(\n",
    "         ).sort_values([\"name\"])\n",
    "\n",
    "# add a field for the endpoint number (so that orgs and datasets with multiple endpoints are split out and in index)\n",
    "final_count[\"endpoint_number\"] = final_count.groupby([\"organisation\", \"name\", \"dataset\"]).cumcount() + 1\n",
    "# create % columns\n",
    "final_count[\"field_supplied_pct\"] = final_count[\"field_supplied\"] / final_count[\"field\"] \n",
    "final_count[\"field_matched_pct\"] = final_count[\"field_matched\"] / final_count[\"field\"] \n",
    "final_count[\"field_loaded_pct\"] = final_count[\"field_loaded\"] / final_count[\"field\"] \n",
    "\n",
    "# final_count.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to make to this report:\n",
    "\n",
    "* Make sure list of orgs and datasets is exhaustive in report table\n",
    "* Sense-check metric results\n",
    "* Sort index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretty(styler):\n",
    "    styler.relabel_index([\"Fields Supplied\", \"Fields Loaded\", \"Field Names Matched\"], axis=1)\n",
    "    styler.format(\"{:.0%}\")\n",
    "    styler.background_gradient(axis=None, vmin=0, vmax=1, cmap=\"PiYG\")\n",
    "    return styler\n",
    "\n",
    "final_count_out = final_count[\n",
    "    [\"name\", \"dataset\", \"endpoint_number\", \"field\", \"field_supplied_pct\", \"field_loaded_pct\", \"field_matched_pct\"]\n",
    "].copy()\n",
    "\n",
    "final_count_out.sort_values([\"name\", \"dataset\", \"endpoint_number\"])\n",
    "final_count_out.set_index([\"name\", \"dataset\", \"field\", \"endpoint_number\"], inplace=True)\n",
    "final_count_out.style.pipe(make_pretty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
