{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides compliance to specification information on the most latest endpoints for a hardcoded list of prioritised list of LPAs, or organisations from an input.\n",
    "\n",
    "The column 'structure_score' tells us how much data an endpoint is giving us as a fraction of what we ask for. The column 'column_name_score' tells us how many columns are correctly named.\n",
    "\n",
    "Example: a column name that is incorrect (e.g 'area' instead of 'geometry') but the data in it has been detected as correct data will score in the 'structure_score' column but not the 'column_name' column\n",
    "\n",
    "The input should be called 'organisation_input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wget\n",
    "# import wget\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download helper utility files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default prioritised LPAs are used unless a specific set of LPAs is detected using an 'organisation_input.csv' file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './organisation_input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    provision_df = get_provisions()\n",
    "    organisation_list = provision_df[\"organisation\"].str.replace(\":\",\"-eng:\")\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "def get_datasette_results(sql):\n",
    "  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": \"{}\".format(sql),\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    resource_df = pd.read_csv(url)\n",
    "    return resource_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_in_endpoint(fields, dataset_field_df, column_field_df, dataset):\n",
    "    dataset_columns = dataset_field_df['field'].tolist()\n",
    "    # Remove automatically assigned columns by the pipeline from scoring\n",
    "    dataset_columns = remove_assigned_columns(dataset, dataset_columns)\n",
    "    \n",
    "    missing_columns = []\n",
    "    present_columns = []\n",
    "    # Count whether columns in the specification are present in the endpoint\n",
    "    for column in dataset_columns:\n",
    "        if column not in fields:\n",
    "            missing_columns.append(column)\n",
    "        else:\n",
    "            present_columns.append(column)\n",
    "    structure_score = f\"{len(dataset_columns) - len(missing_columns)}/{len(dataset_columns)}\"\n",
    "    structure_percentage = (len(dataset_columns) - len(missing_columns)) / len(dataset_columns) * 100\n",
    "\n",
    "    # The WKT column is removed from the column_field mapping as it is autogenerated by the pipeline for some file formats (e.g geojson)\n",
    "    filtered_columns = [\"WKT\"]\n",
    "    column_field_df = column_field_df[-column_field_df['column'].isin(filtered_columns)]\n",
    "\n",
    "    mapped_fields = column_field_df['field'].tolist()\n",
    "    # print(\"present columns: \", present_columns)\n",
    "    # print(\"missing columns: \", missing_columns)\n",
    "    # print(\"column field mapping: \\n\", column_field_df)\n",
    "    correct_column_names = 0\n",
    "    for field in present_columns:\n",
    "        # If a field isn't present in the mapped fields it is correctly named\n",
    "        # Or if the column name is the same as the field name it is correctly named\n",
    "        if field not in mapped_fields or column_field_df[column_field_df['field'] == field]['column'].tolist()[0] == field:\n",
    "            correct_column_names += 1\n",
    "    \n",
    "    column_score = f\"{correct_column_names}/{len(dataset_columns)}\"\n",
    "    column_percentage = (correct_column_names)/ len(dataset_columns)*100\n",
    "   \n",
    "    return structure_score, structure_percentage, column_score, column_percentage\n",
    "\n",
    "\n",
    "# def get_fields_for_resource(resource, dataset):\n",
    "#     datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "#     params = urllib.parse.urlencode({\n",
    "#         \"sql\": f\"\"\"\n",
    "#         select f.field \n",
    "#         from \n",
    "#             fact_resource fr\n",
    "#             inner join fact f on fr.fact = f.fact\n",
    "#         where \n",
    "#             resource = '{resource}'\n",
    "#         group by\n",
    "#             f.field\n",
    "#         \"\"\",\n",
    "#         \"_size\": \"max\"\n",
    "#     })\n",
    "#     url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "#     facts_df = pd.read_csv(url)\n",
    "#     facts_list = facts_df['field'].tolist()\n",
    "#     return facts_list\n",
    "\n",
    "# alternate version which returns df\n",
    "def get_fields_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select f.field, fr.resource\n",
    "        from \n",
    "            fact_resource fr\n",
    "            inner join fact f on fr.fact = f.fact\n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        group by\n",
    "            f.field\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    facts_df = pd.read_csv(url)\n",
    "    # facts_list = facts_df['field'].tolist()\n",
    "    return facts_df\n",
    "\n",
    "def get_column_mappings_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select column, field\n",
    "        from \n",
    "          column_field  \n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    column_field_df = pd.read_csv(url)\n",
    "    return column_field_df\n",
    "\n",
    "def remove_assigned_columns(dataset, dataset_columns):\n",
    "    # These columns are auto generated by the pipeline therefore not used in the scoring\n",
    "    dataset_columns.remove('entity')\n",
    "    dataset_columns.remove('organisation')\n",
    "    dataset_columns.remove('prefix')\n",
    "    if dataset != \"tree\" and \"point\" in dataset_columns:\n",
    "        dataset_columns.remove('point')\n",
    "    return dataset_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of organisation names, to be displayed in the output table. This is gathered separately from the main data, to ensure that if an organisation has not provided any endpoints, it is still included in the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get organisation names for output table\n",
    "organisation_info_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/organisation-collection/main/data/local-authority.csv')\n",
    "organisation_info_df.head()\n",
    "organisation_name_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    organisation_code = organisation.split(':')[1]\n",
    "    organisation_name = organisation_info_df.loc[organisation_info_df['reference'] == organisation_code].iloc[0]['name']\n",
    "    organisation_name_dict[organisation] = organisation_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organisation_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organisation_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latest endpoints table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest endpoints are collected for each of the organisations, for the first 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "# Collect latest endpoints for each organisation\n",
    "dataset_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "pipelines_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree', 'tree,tree-preservation-order', 'tree-preservation-order,tree-preservation-zone']\n",
    "all_orgs_latest_endpoints={}\n",
    "for organisation in organisation_list:\n",
    "\n",
    "    try:\n",
    "        latest_endpoints_df = get_latest_endpoints(organisation)\n",
    "        latest_endpoints_df = latest_endpoints_df[latest_endpoints_df['pipelines'].isin(pipelines_list)]\n",
    "        all_orgs_latest_endpoints[organisation] = latest_endpoints_df\n",
    "\n",
    "    except:\n",
    "        all_orgs_latest_endpoints[organisation] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick dictionary in df\n",
    "endpoint_latest_df = pd.concat([all_orgs_latest_endpoints[v] for v in all_orgs_latest_endpoints])\n",
    "\n",
    "print(len(endpoint_latest_df))\n",
    "endpoint_latest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_df.groupby(\"status\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_df[endpoint_latest_df[\"status\"] == 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_df[endpoint_latest_df[\"organisation\"] == \"local-authority-eng:GLO\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_df[endpoint_latest_df[\"organisation\"] == \"local-authority-eng:BNE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode out the \n",
    "endpoint_latest_df[\"dataset\"] = endpoint_latest_df[\"pipelines\"].str.split(\",\")\n",
    "endpoint_latest_long_df = endpoint_latest_df.explode(\"dataset\", ignore_index=True)\n",
    "\n",
    "print(len(endpoint_latest_df))\n",
    "print(len(endpoint_latest_long_df))\n",
    "endpoint_latest_long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count datasets which have multiple endpoints\n",
    "org_dataset_count = endpoint_latest_long_df.groupby([\"organisation\", \"name\", \"dataset\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "org_dataset_count[org_dataset_count[\"count\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find datasets which have multiple resources, or resources used in multiple datasets (basically, any instances of resource duplication)\n",
    "resource_count = endpoint_latest_long_df.groupby([\"resource\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "resource_dupes = resource_count[resource_count[\"count\"] > 1]\n",
    "\n",
    "# look at records which have resource dupes\n",
    "endpoint_latest_long_df[endpoint_latest_long_df[\"resource\"].isin(resource_dupes[\"resource\"])][\n",
    "    [\"status\", \"collection\", \"dataset\", \"name\", \"resource\", \"entrydate\", \"maxentrydate\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking original last updated handling\n",
    "\n",
    "This is an example of some data which has the `date_last_status_200` populated because the most recent endpoint log doesn't have a 200 status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_long_df[endpoint_latest_long_df[\"date_last_status_200\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how the data looks in the original table before last updated logic applied - endpoint 404 is kept and just date field added to capture when the last 200 status record was\n",
    "temp_cat = get_endpoints(\"local-authority-eng:CAT\")\n",
    "\n",
    "temp_cat[temp_cat[\"pipelines\"] == \"tree-preservation-order,tree-preservation-zone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however, the resource is still incorrect for this record\n",
    "get_latest_resource_for_endpoint(\"https://opendata.arcgis.com/datasets/a4ddbb5114274ba89e33a33545c407c8_0.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than grabbing all logs and doing this extra logic, it may be easier just to use lastest_logs as the base, then for all non-200 records just grab the last resource which was 200. \n",
    "\n",
    "\n",
    "[Example datasette query](https://datasette.planning.data.gov.uk/digital-land?sql=select%0D%0A++++l.endpoint%2C%0D%0A++++l.status%2C%0D%0A++++l.exception%2C%0D%0A++++s.collection%2C%0D%0A++++l.resource%2C%0D%0A++++l.entry_date+as+log_entry_date%2C%0D%0A++++e.entry_date+as+endpoint_entry_date%0D%0Afrom%0D%0A++++log+l%0D%0A++++inner+join+source+s+on+l.endpoint+%3D+s.endpoint%0D%0A%0D%0A++++inner+join+endpoint+e+on+l.endpoint+%3D+e.endpoint%0D%0Awhere%0D%0A++++s.organisation+%3D+%22local-authority-eng%3ACAT%22+and+collection%3D%22tree-preservation-order%22+%0D%0A++++%0D%0Aorder+by+log_entry_date+desc) to get this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreating the get_endpoints function, but using latest_log table instead\n",
    "\n",
    "def get_endpoints_new(organisation):\n",
    "    if organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "select\n",
    "    e.endpoint_url,\n",
    "    l.endpoint,\n",
    "    l.status,\n",
    "    l.exception,\n",
    "    s.collection,\n",
    "    l.resource,\n",
    "    sp.pipeline,\n",
    "    s.organisation,\n",
    "    o.name,\n",
    "    l.entry_date as log_entry_date,\n",
    "    e.entry_date as endpoint_entry_date\n",
    "from\n",
    "    most_recent_log l\n",
    "    inner join source s on l.endpoint = s.endpoint\n",
    "    inner join endpoint e on l.endpoint = e.endpoint\n",
    "    inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "    inner join source_pipeline sp on s.source = sp.source\n",
    "where\n",
    "    {query} and not s.collection=\"brownfield-land\" \n",
    "\n",
    "order by log_entry_date desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "\n",
    "    try:\n",
    "        endpoints_df = pd.read_csv(url)\n",
    "    except:\n",
    "        endpoints_df = pd.DataFrame({\"organisation\":[organisation]})\n",
    "    \n",
    "    return endpoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "\n",
    "# dictionary of results, with org name as key\n",
    "results_dict = {org_name : get_endpoints_new(org_name) for org_name in organisation_list}\n",
    "\n",
    "# record orgs which didn't return any results\n",
    "no_result_orgs = [v for v in organisation_list if len(results_dict[v]) == 0]\n",
    "# concat results into df\n",
    "endpoint_resource_df = pd.concat([results_dict[v] for v in organisation_list if len(results_dict[v]) > 0])\n",
    "\n",
    "# filter to only records in pipelines we want\n",
    "endpoint_resource_df = endpoint_resource_df[endpoint_resource_df[\"pipeline\"].isin(dataset_list)].reset_index(drop=True)\n",
    "\n",
    "print(len(endpoint_resource_df))\n",
    "endpoint_resource_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing no. of results between approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of endpoints across tables\n",
    "print(len(endpoint_resource_df))\n",
    "print(len(endpoint_latest_long_df))\n",
    "\n",
    "print(len(endpoint_resource_df[endpoint_resource_df[\"endpoint_url\"].isin(endpoint_latest_long_df[\"endpoint_url\"])]))\n",
    "print(len(endpoint_resource_df[~endpoint_resource_df[\"endpoint_url\"].isin(endpoint_latest_long_df[\"endpoint_url\"])]))\n",
    "\n",
    "print(len(endpoint_latest_long_df[endpoint_latest_long_df[\"endpoint_url\"].isin(endpoint_resource_df[\"endpoint_url\"])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the old approach endpoints are captured in the new table, but the new table also has some endpoints which aren't in the old table.\n",
    "\n",
    "Looking into this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoints in new that aren't in old approach\n",
    "endpoint_resource_df[~endpoint_resource_df[\"endpoint_url\"].isin(endpoint_latest_long_df[\"endpoint_url\"])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new appraoch captures 2 endpoints for barnet TPO collection, across three pipelines\n",
    "endpoint_resource_df[(endpoint_resource_df[\"organisation\"] == \"local-authority-eng:BNE\") & (endpoint_resource_df[\"collection\"] == \"tree-preservation-order\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but old approach only has one of them, for TPO pipeline\n",
    "endpoint_latest_long_df[(endpoint_latest_long_df[\"organisation\"] == \"local-authority-eng:BNE\") & (endpoint_latest_long_df[\"collection\"] == \"tree-preservation-order\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how the data looks in the original get_endpoints table - both endpoints were captured, but grouped into pipeline groups. Not sure why one is lost \n",
    "\n",
    "temp_bne = get_endpoints(\"local-authority-eng:BNE\")\n",
    "temp_bne[temp_bne[\"collection\"] == \"tree-preservation-order\"][[\"endpoint_url\", \"status\", \"pipelines\", \"resource\", \"maxentrydate\", \"entrydate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find datasets which have multiple resources, or resources used in multiple datasets (basically, any instances of resource duplication)\n",
    "resource_count = endpoint_resource_df.groupby([\"resource\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "resource_dupes = resource_count[resource_count[\"count\"] > 1]\n",
    "\n",
    "# look at records which have resource dupes\n",
    "endpoint_resource_df[endpoint_resource_df[\"resource\"].isin(resource_dupes[\"resource\"])][\n",
    "    [\"status\", \"pipeline\", \"organisation\", \"name\", \"endpoint\", \"resource\", \"log_entry_date\", \"endpoint_entry_date\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicate endpoints\n",
    "ep_count = endpoint_resource_df.groupby([\"endpoint\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "endpoint_dupes = ep_count[ep_count[\"count\"] > 1]\n",
    "\n",
    "# look at records which have resource dupes\n",
    "endpoint_resource_df[endpoint_resource_df[\"endpoint\"].isin(endpoint_dupes[\"endpoint\"])][\n",
    "    [\"status\", \"pipeline\", \"name\", \"endpoint\", \"resource\", \"log_entry_date\", \"endpoint_entry_date\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find org datasets which are getting data from multiple endpoints\n",
    "ep_count = endpoint_resource_df.groupby([\"organisation\", \"name\", \"pipeline\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "endpoint_dupes = ep_count[ep_count[\"count\"] > 1]\n",
    "endpoint_dupes\n",
    "# look at records which have resource dupes\n",
    "# endpoint_resource_df[endpoint_resource_df[\"endpoint\"].isin(endpoint_dupes[\"endpoint\"])][\n",
    "#     [\"status\", \"pipeline\", \"name\", \"endpoint\", \"resource\", \"log_entry_date\", \"endpoint_entry_date\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to answer\n",
    "* what happens when the same resource is duplicated across different endpoints and processed with the same pipeline? As is the case for Yarmouth and Newcastle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find instances where the same resource is being used multiple times in the same dataset\n",
    "resource_count = endpoint_resource_df.groupby([\"resource\", \"pipeline\"]).size().reset_index(name = \"count\")\n",
    "\n",
    "resource_dupes = resource_count[resource_count[\"count\"] > 1]\n",
    "\n",
    "# look at records which have resource dupes\n",
    "endpoint_resource_df[endpoint_resource_df[\"resource\"].isin(resource_dupes[\"resource\"])][\n",
    "    [\"status\", \"pipeline\", \"name\", \"endpoint\", \"resource\", \"log_entry_date\", \"endpoint_entry_date\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_df.groupby(\"status\", dropna=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total records with bad status\n",
    "bad_status = endpoint_resource_df[endpoint_resource_df[\"status\"] != 200]\n",
    "\n",
    "print(len(bad_status))\n",
    "\n",
    "# join to main table to check whether there are any more recent resources in there - length is the same so no\n",
    "print(len(bad_status[[\"endpoint\", \"pipeline\"]].merge(\n",
    "    endpoint_resource_df,\n",
    "    how = \"inner\",\n",
    "    on = [\"endpoint\", \"pipeline\"]\n",
    ")))\n",
    "\n",
    "# If we want to get the latest active resources for the bad endpoints will have to process that separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing latest resource between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_resource_for_endpoint_new(endpoint_url):\n",
    "  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          r.resource\n",
    "        from\n",
    "          endpoint e\n",
    "          inner join resource_endpoint re on e.endpoint = re.endpoint\n",
    "          inner join resource r on re.resource = r.resource\n",
    "        where\n",
    "          e.endpoint_url='{endpoint_url}'\n",
    "        order by\n",
    "          r.start_date desc\n",
    "        limit 1\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    resource_df = pd.read_csv(url)\n",
    "    return resource_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latest_resource_for_endpoint_new(\"https://maps.birmingham.gov.uk/server/rest/services/planx/PlanX/FeatureServer/4/query?where=1=1&outfields=*&f=geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_distinct = endpoint_resource_df[[\"endpoint\", \"status\", \"endpoint_url\", \"resource\"]].drop_duplicates()\n",
    "\n",
    "# endpoint_resource_distinct = endpoint_resource_distinct[endpoint_resource_distinct[\"status\"] == 200]\n",
    "\n",
    "endpoint_resource_distinct[\"resource_orig\"] = [get_latest_resource_for_endpoint(e)[\"resource\"].values[0] for e in endpoint_resource_distinct[\"endpoint_url\"].values]\n",
    "endpoint_resource_distinct[\"resource_new\"] = [get_latest_resource_for_endpoint_new(e)[\"resource\"].values[0] for e in endpoint_resource_distinct[\"endpoint_url\"].values]\n",
    "\n",
    "endpoint_resource_distinct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(endpoint_resource_distinct))\n",
    "\n",
    "print(len(endpoint_resource_distinct[endpoint_resource_distinct[\"resource\"] == endpoint_resource_distinct[\"resource_orig\"]]))\n",
    "\n",
    "print(len(endpoint_resource_distinct[endpoint_resource_distinct[\"resource\"] == endpoint_resource_distinct[\"resource_new\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_distinct[endpoint_resource_distinct[\"resource\"] != endpoint_resource_distinct[\"resource_new\"]].reset_index(drop=True).iloc[[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether just one big sql query for all orgs will give same results, and also checking resource end dates\n",
    "\n",
    "ep_all_df = get_datasette_results(\n",
    "    \"\"\"\n",
    "select\n",
    "    e.endpoint_url,\n",
    "    l.endpoint,\n",
    "    l.status,\n",
    "    l.exception,\n",
    "    s.collection,\n",
    "    l.resource,\n",
    "    sp.pipeline,\n",
    "    s.organisation,\n",
    "    o.name,\n",
    "    l.entry_date as log_entry_date,\n",
    "    e.entry_date as endpoint_entry_date,\n",
    "    e.end_date as endpoint_end_date,\n",
    "    r.start_date as resource_start_date,\n",
    "    r.end_date as resource_end_date\n",
    "from\n",
    "    most_recent_log l\n",
    "    inner join source s on l.endpoint = s.endpoint\n",
    "    inner join endpoint e on l.endpoint = e.endpoint\n",
    "    inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "    inner join source_pipeline sp on s.source = sp.source\n",
    "    left join resource r on l.resource = r.resource\n",
    "where\n",
    "    sp.pipeline IN ('article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree')\n",
    "\n",
    "order by s.organisation, sp.pipeline, log_entry_date desc\n",
    "\"\"\")\n",
    "\n",
    "print(len(ep_all_df))\n",
    "ep_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(endpoint_resource_df))\n",
    "\n",
    "match_all_check = endpoint_resource_df[[\"endpoint\", \"status\", \"pipeline\", \"collection\"]].merge(\n",
    "    ep_all_df,\n",
    "    how = \"inner\",\n",
    "    on = [\"endpoint\", \"status\", \"pipeline\", \"collection\"]\n",
    ")\n",
    "\n",
    "# check all records matched\n",
    "print(len(match_all_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking records which have an endpoint or resource end date\n",
    "match_all_check[match_all_check[\"resource_end_date\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking those orgs and collections in main table - in all instances those with a non-active resource have a more recent active resource\n",
    "# so I think we can just filter to endpoints and resources without an end date in the main SQL query\n",
    "match_all_check[(match_all_check[\"organisation\"] == \"local-authority-eng:DNC\") & (match_all_check[\"collection\"] == \"listed-building\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_df[(endpoint_resource_df[\"organisation\"] == \"local-authority-eng:DOV\") & (endpoint_resource_df[\"collection\"] == \"tree-preservation-order\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(endpoint_resource_df))\n",
    "print(len(endpoint_resource_df[[\"endpoint\", \"pipeline\"]].drop_duplicates()))\n",
    "print(len(endpoint_resource_df[[\"resource\"]].drop_duplicates()))\n",
    "print(len(endpoint_resource_df[[\"endpoint\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_all_filtered_df = ep_all_df[\n",
    "    (ep_all_df[\"organisation\"].isin(organisation_list)) &\n",
    "    (ep_all_df[\"status\"] == 200) &\n",
    "    (ep_all_df[\"endpoint_end_date\"].isnull()) &\n",
    "    (ep_all_df[\"resource_end_date\"].isnull())\n",
    "]\n",
    "\n",
    "print(len(ep_all_df))\n",
    "print(len(ep_all_filtered_df))\n",
    "\n",
    "print(len(ep_all_filtered_df[[\"endpoint\", \"pipeline\"]].drop_duplicates()))\n",
    "print(len(ep_all_filtered_df[[\"resource\"]].drop_duplicates()))\n",
    "print(len(ep_all_filtered_df[[\"endpoint\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_all_check[match_all_check[\"resource_end_date\"].notnull()][\"endpoint\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_all_df[ep_all_df[\"endpoint\"] == \"351fdbd179616dcf25ce0c4498cbd7fd5a917c5bbedcbc6af9f3f23d546b484d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_df[endpoint_resource_df[\"endpoint\"].isin(\n",
    "    match_all_check[match_all_check[\"organisation\"].isnull()][\"endpoint\"].values\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource fields and mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_df = endpoint_resource_df[[\"pipeline\", \"resource\"]].drop_duplicates().dropna(axis = 0)\n",
    "print(len(resource_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function to try the resource datasette queries \n",
    "# will return a df with resource and dataset fields as keys, and query results as other fields\n",
    "def try_results(function, resource, dataset):\n",
    "\n",
    "    # try grabbing results\n",
    "    try:\n",
    "        df = function(resource, dataset)\n",
    "\n",
    "        # if empty response give NaNs\n",
    "        if len(df) == 0:\n",
    "            df = pd.DataFrame({\"column\" : [np.nan],\n",
    "                           \"field\" : [np.nan]\n",
    "            })\n",
    "\n",
    "        df[\"resource\"] = resource\n",
    "        df[\"dataset\"] = dataset\n",
    "\n",
    "    # if error record resource and dataset\n",
    "    except:\n",
    "        df = pd.DataFrame({\"resource\" : [resource],\n",
    "                           \"dataset\" : [dataset]\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "results_col_map = [try_results(get_column_mappings_for_resource, r[\"resource\"], r[\"pipeline\"]) for index, r in resource_df.iterrows()]\n",
    "results_field_resource = [try_results(get_fields_for_resource, r[\"resource\"], r[\"pipeline\"]) for index, r in resource_df.iterrows()]\n",
    "\n",
    "# concat the results, resources which errored with have NaNs in query results fields\n",
    "results_col_map_df = pd.concat(results_col_map)\n",
    "results_field_resource_df = pd.concat(results_field_resource)\n",
    "\n",
    "# no. of resources in each query response array\n",
    "print(len(results_col_map))\n",
    "print(len(results_field_resource))\n",
    "\n",
    "# no of records in each results df\n",
    "print(len(results_col_map_df))\n",
    "print(len(results_field_resource_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct resources in each table\n",
    "print(len(results_col_map_df[[\"resource\"]].drop_duplicates()))\n",
    "print(len(results_field_resource_df[[\"resource\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_field_resource_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_col_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources which are in the column mapping df but not in the fields one\n",
    "\n",
    "results_col_valid = results_col_map_df[results_col_map_df[\"field\"].notnull()]\n",
    "results_field_valid = results_field_resource_df[results_field_resource_df[\"field\"].notnull()]\n",
    "\n",
    "results_col_valid[~results_col_valid[\"resource\"].isin(results_field_valid[\"resource\"].drop_duplicates())].sort_values(\"resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for tomorrow - why would a resource be in the field mapping table but not in the fields table..??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in match field for column mappings \n",
    "results_col_map_df[\"field_matched\"] = np.where(\n",
    "        (results_col_map_df[\"field\"].isin([\"geometry\", \"point\"])) |\n",
    "        (results_col_map_df[\"field\"] == results_col_map_df[\"column\"]),\n",
    "        1, \n",
    "        0\n",
    ")\n",
    "\n",
    "# add in flag for fields supplied (i.e. they're in the mapping table)\n",
    "results_col_map_df[\"field_supplied\"] = 1\n",
    "\n",
    "# add in flag for fields present\n",
    "results_field_resource_df[\"field_loaded\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_col_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how geometry fields are mapped\n",
    "results_col_map_df[results_col_map_df[\"field\"] == \"geometry\"][[\"column\", \"field\", \"match\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data in fields vs mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for differences in the number of fields in the col_map response vs. field response\n",
    "results_col_count = results_col_map_df.groupby([\"resource\"]).size().reset_index(name = \"col_map_count\")\n",
    "results_field_count = results_field_resource_df.groupby([\"resource\"]).size().reset_index(name = \"field_count\")\n",
    "\n",
    "col_field_comp_df = results_col_count.merge(\n",
    "    results_field_count,\n",
    "    how = \"left\", \n",
    "    on = \"resource\"\n",
    ")\n",
    "\n",
    "# col_field_comp_df.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "col_field_comp_df[\"difference\"] = col_field_comp_df[\"col_map_count\"] - col_field_comp_df[\"field_count\"]\n",
    "\n",
    "col_field_comp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking an example of a single resource - 7a937605655b895bf9ebfbe29f8e35af8d3f606fd811b42867251d61ff15b693\n",
    "# the column mapping table contains 11 fields\n",
    "\n",
    "results_col_map_df[results_col_map_df[\"resource\"]== \"7a937605655b895bf9ebfbe29f8e35af8d3f606fd811b42867251d61ff15b693\"].sort_values(\"field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the field table only contains 8 fields\n",
    "results_field_resource_df[results_field_resource_df[\"resource\"] == \"7a937605655b895bf9ebfbe29f8e35af8d3f606fd811b42867251d61ff15b693\"].sort_values(\"field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the endpoint itself we can see that the 8 fields above are the 6 mapped fields which are populated, plus organisation and prefix\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "bm_af_df = gpd.read_file(\"https://maps.birmingham.gov.uk/server/rest/services/planx/PlanX/FeatureServer/0/query?where=1=1&outfields=*&f=geojson\")\n",
    "\n",
    "bm_af_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_long_df[endpoint_latest_long_df[\"resource\"] == \"0519df49c2ecc3c53948b4283704bfd5b905ac4db6e4b5a0ae709c1fc495bc81\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_latest_long_df[endpoint_latest_long_df[\"organisation\"] == \"local-authority-eng:DOV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dov_endpoints_df = get_endpoints(\"local-authority-eng:DOV\")\n",
    "\n",
    "print(len(dov_endpoints_df))\n",
    "dov_endpoints_df[dov_endpoints_df[\"collection\"] == \"tree-preservation-order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fields_for_resource(\"0519df49c2ecc3c53948b4283704bfd5b905ac4db6e4b5a0ae709c1fc495bc81\", \"tree-preservation-zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fields_for_resource(\"3327be0c6d46fb1ebb8c77a9b3344dd12be4bd59e9abf546966fcdc552c4282c\", \"tree-preservation-zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fields_for_resource(\"004e273e15af7f9c5ffe43cda70764da076e53c090c128f937031e63c7ce7a8d\", \"article-4-direction-area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating match rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_field_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "\n",
    "# remove the pipeline-created fields from the spec field table\n",
    "# (\"entity\", \"organisation\", \"prefix\", \"point\" for all but tree, and\n",
    "#  \"entity\", \"organisation\", \"prefix\" for tree)\n",
    "dataset_field_subset_df = dataset_field_df[((dataset_field_df[\"dataset\"] != \"tree\") & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\", \"point\"])) |\n",
    "                  (dataset_field_df[\"dataset\"] == \"tree\") & (~dataset_field_df[\"field\"].isin([\"entity\", \"organisation\", \"prefix\"])))]\n",
    "\n",
    "dataset_field_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join on all fields that each dataset should have\n",
    "\n",
    "endpoint_resource_df[\"dataset\"] = endpoint_resource_df[\"pipeline\"]\n",
    "\n",
    "resource_spec_fields_df = endpoint_resource_df[[\"organisation\", \"name\", \"dataset\", \"endpoint\", \"status\", \"log_entry_date\", \"endpoint_entry_date\", \"resource\"]].merge(\n",
    "    dataset_field_subset_df[[\"dataset\", \"field\"]],\n",
    "    how = \"left\",\n",
    "    on = \"dataset\"\n",
    ")\n",
    "\n",
    "print(len(resource_spec_fields_df))\n",
    "resource_spec_fields_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on field present flag for each resource\n",
    "resource_fields_match = resource_spec_fields_df.merge(\n",
    "    results_field_resource_df[[\"dataset\", \"resource\", \"field\", \"field_loaded\"]],\n",
    "    how = \"left\",\n",
    "    on = [\"dataset\", \"resource\", \"field\"]\n",
    ")\n",
    "\n",
    "print(len(resource_fields_match))\n",
    "resource_fields_match.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on field present flag for each resource\n",
    "resource_fields_map_match = resource_fields_match.merge(\n",
    "    results_col_map_df[[\"dataset\", \"resource\", \"field\", \"field_supplied\", \"field_matched\"]],\n",
    "    how = \"left\",\n",
    "    on = [\"dataset\", \"resource\", \"field\"]\n",
    ")\n",
    "\n",
    "print(len(resource_fields_map_match))\n",
    "resource_fields_map_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_fields_map_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_fields_map_match.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "final_count = resource_fields_map_match.groupby([\"organisation\", \"name\", \"endpoint\", \"resource\", \"dataset\", \"status\", \"log_entry_date\", \"endpoint_entry_date\"]).agg(\n",
    "    {\"field\":\"count\", \n",
    "     \"field_supplied\" : \"sum\",\n",
    "     \"field_matched\" : \"sum\",\n",
    "     \"field_loaded\" : \"sum\"}).reset_index().sort_values([\"name\"])\n",
    "\n",
    "final_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_count[final_count[\"name\"] == \"Birmingham City Council\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_resource_df[endpoint_resource_df[\"resource\"] == \"acb88aac41434c4cfccb9ee77f6471f5c682616617604cd0db502893e9c08579\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_fields_map_match[resource_fields_map_match[\"resource\"] == \"acb88aac41434c4cfccb9ee77f6471f5c682616617604cd0db502893e9c08579\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_field_resource_df[results_field_resource_df[\"resource\"] == \"9fea9a08d5717b319698f2871b7e5d9e635cb6381a3da08fbec731277c23dd26\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_col_map_df[results_col_map_df[\"resource\"] == \"9fea9a08d5717b319698f2871b7e5d9e635cb6381a3da08fbec731277c23dd26\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_issues_for_resource(\"acb88aac41434c4cfccb9ee77f6471f5c682616617604cd0db502893e9c08579\", \"conservation-area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these endpoints, the relevant schema for the dataset is downloaded to compare the endpoint columns against.\n",
    "\n",
    "'Structure score' is the number of columns in the processed data that match the schema, divided by the number of columns in the schema. Note that if there is no data at all in a field, it cannot be detected as a structure match.\n",
    "\n",
    "'Column name score' is the number of columns in the processed data that had matching column names to the schema before any processing happened (ie no column mapping had to take place). Note that if there is no data at all in a field, it cannot be detected as a column name match.\n",
    "\n",
    "If an endpoint contributes to two datasets, it will only be considered for a dataset if it is the newest endpoint for that dataset, calculated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_field_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_colour(value):\n",
    "    if \"%\" in value:\n",
    "        value = int(value.replace(\"%\", \"\"))\n",
    "        if value >= 75:\n",
    "            return 'background-color: green'\n",
    "        elif value < 75 and value >= 50:\n",
    "            return 'background-color: orange'\n",
    "        elif 0 <= value < 50:\n",
    "            return 'background-color: #ffaeb1'\n",
    "        else:\n",
    "            return 'background-color: brown'\n",
    "\n",
    "organisation_dataset_compliance_dict={}\n",
    "rows_list = []\n",
    "csv_rows_list = []\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = all_orgs_latest_endpoints[organisation]\n",
    "    dataset_compliance_dict = {}\n",
    "    for index, row in latest_endpoints_df.iterrows():\n",
    "        resource = row['resource']\n",
    "        if ',' in row['pipelines']:\n",
    "            datasets = row['pipelines'].split(',')\n",
    "        else:\n",
    "            datasets = [row['pipelines']]\n",
    "        for dataset in datasets:\n",
    "            same_datasets_df = latest_endpoints_df[latest_endpoints_df[\"pipelines\"].apply(lambda x: dataset in x.split(','))]\n",
    "            if len(same_datasets_df) > 1:\n",
    "                skip_dataset = handle_skip_dataset(same_datasets_df, dataset, row)\n",
    "            else:\n",
    "                skip_dataset = False\n",
    "            # print(organisation, dataset, resource)\n",
    "            \n",
    "\n",
    "            dataset_field_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "            dataset_field_df = dataset_field_df[dataset_field_df['dataset'] == dataset]\n",
    "\n",
    "            if not skip_dataset:\n",
    "                column_field_df = get_column_mappings_for_resource(resource, dataset)\n",
    "                fields = get_fields_for_resource(resource, dataset)\n",
    "                structure_score, structure_percentage, column_score, column_percentage = check_columns_in_endpoint(fields, dataset_field_df, column_field_df, dataset)\n",
    "                overall_percentage = (structure_percentage + column_percentage) / 2\n",
    "                dataset_compliance_dict[dataset] = {\"structure_score\": structure_score, \"structure_percentage\": structure_percentage, \"column_score\": column_score, \"column_name_percentage\": column_percentage}\n",
    "                new_row = {'organisation': organisation_name_dict[organisation], 'dataset': dataset, 'structure_score': structure_score, 'structure_percentage': f\"{int(structure_percentage)}%\" , 'column_name_score': column_score, 'column_name_percentage': f\"{int(column_percentage)}%\", 'overall_percentage': f\"{int(overall_percentage)}%\"}\n",
    "                rows_list.append(new_row)\n",
    "                csv_row = new_row.copy()\n",
    "                csv_row['endpoint_url'] = row['endpoint_url']\n",
    "                csv_row['resource'] = row['resource']\n",
    "                csv_rows_list.append(csv_row)\n",
    "    \n",
    "    organisation_dataset_compliance_dict[organisation] = dataset_compliance_dict\n",
    "\n",
    "\n",
    "compliance_df = pd.DataFrame(rows_list)\n",
    "output_df = pd.DataFrame(csv_rows_list)\n",
    "output_df.to_csv('compliance.csv', index=False)\n",
    "compliance_df.style.applymap(compute_cell_colour, subset=[\"structure_percentage\", \"column_name_percentage\", \"overall_percentage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def compute_cell_colour(status):\n",
    "    if status == \"200\":\n",
    "        return 'background-color: green'\n",
    "    elif status == 'No endpoint':\n",
    "        return 'background-color: orange'\n",
    "    else:\n",
    "        return 'background-color: red'\n",
    "    \n",
    "def cut_zeros(row):\n",
    "  if row[-2:]=='.0':\n",
    "    row=row[:-2]\n",
    "  return row\n",
    "\n",
    "# Only display non 200 statuses if they have been non 200 for more than 5 days\n",
    "def compute_displayed_status(row):\n",
    "    # Check if the most recent status isn't 200\n",
    "    if row[\"last_status\"] == 200:\n",
    "        last_200_date = pd.to_datetime(row[\"last_updated_date\"])\n",
    "    elif row[\"last_status\"] != None:\n",
    "        last_200_date = pd.to_datetime(row[\"date_last_status_200\"])\n",
    "    else:\n",
    "        # If the most recent status is 200 then we can return 200\n",
    "        return 200\n",
    "    \n",
    "    days_since_200 = (row[\"maxentrydate\"] - last_200_date).days\n",
    "    # Only show non 200 statuses if they have been non 200 for more than 5 days\n",
    "    if days_since_200 >= 5:\n",
    "        status = row['status']\n",
    "        # Handle cases where there is no status by looking at the exception\n",
    "        if not pd.isna(status):\n",
    "            status = int(status)\n",
    "        else:\n",
    "            status=latest_endpoints_df.loc[latest_endpoints_df['status'].isna(), 'exception'].values[0]\n",
    "            if status is None:\n",
    "                status=\"Unknown Error\"\n",
    "        return status\n",
    "    else:\n",
    "        return 200\n",
    "\n",
    "\n",
    "rows_list = []\n",
    "organisation_dataset_statuses_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = all_orgs_latest_endpoints[organisation]\n",
    "    latest_endpoints_df = latest_endpoints_df[pd.isna(latest_endpoints_df['end_date'])]\n",
    "    try:\n",
    "        name = organisation_name_dict[organisation]\n",
    "    except:\n",
    "        name = organisation\n",
    "    \n",
    "    dataset_statuses_dict = {}\n",
    "    for index, row in latest_endpoints_df.iterrows():\n",
    "        if 'WFS' in row['endpoint_url']:\n",
    "            response = requests.get(row['endpoint_url'], stream=True)\n",
    "            try:\n",
    "                content = next(response.iter_content(chunk_size=1024)).decode('utf-8')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                content = response.text\n",
    "            if 'Cannot find layer' in content:\n",
    "                row['status']='Cannot find layer'\n",
    "        resource = row['resource']\n",
    "        if ',' in row['pipelines']:\n",
    "            datasets = row['pipelines'].split(',')\n",
    "        else:\n",
    "            datasets = [row['pipelines']]\n",
    "        for dataset in datasets:\n",
    "            # Consider cases where a dataset is contributed to by multiple endpoints\n",
    "            same_datasets_df = latest_endpoints_df[latest_endpoints_df[\"pipelines\"].apply(lambda x: dataset in x.split(','))]\n",
    "            if len(same_datasets_df) > 1:\n",
    "                skip_dataset = handle_skip_dataset(same_datasets_df, dataset, row)\n",
    "            else:\n",
    "                skip_dataset = False\n",
    "\n",
    "            if not skip_dataset:\n",
    "                dataset_statuses_dict[dataset] = compute_displayed_status(row)\n",
    "    organisation_dataset_statuses_dict[organisation] = dataset_statuses_dict\n",
    "   \n",
    "    new_row = {'organisation': name}\n",
    "    new_row.update(dataset_statuses_dict)\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "output_df = pd.DataFrame(rows_list, columns=['organisation', *dataset_list])\n",
    "output_df = output_df.replace(np.nan, \"No endpoint\")\n",
    "\n",
    "output_df = output_df.astype(str)\n",
    "output_df = output_df.applymap(cut_zeros)\n",
    "\n",
    "output_df.to_csv('endpoint_status_master_report.csv', index=False)\n",
    "output_df.style.applymap(compute_cell_colour, subset=dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different query type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_mappings_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select column, field\n",
    "        from \n",
    "          column_field  \n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    column_field_df = pd.read_csv(url)\n",
    "    return column_field_df\n",
    "\n",
    "get_column_mappings_for_resource(\"81ed286e34b43d1f9f3053e463a6151224b182538ce98f9064f43ebd30dc2973\", \"conservation-area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_col_map = []\n",
    "\n",
    "for index, r in endpoint_latest_long_df.iterrows():\n",
    "    try:\n",
    "        df = get_column_mappings_for_resource(r[\"resource\"], r[\"dataset\"])\n",
    "        df[\"resource\"] = r[\"resource\"]\n",
    "        df[\"dataset\"] = r[\"dataset\"]\n",
    "\n",
    "    except:\n",
    "        df = pd.DataFrame({\"resource\" : [r[\"resource\"]],\n",
    "                           \"dataset\" : [r[\"dataset\"]]\n",
    "        })\n",
    "\n",
    "    results_col_map.append(df)\n",
    "\n",
    "results_col_map_df = pd.concat(results_col_map)\n",
    "\n",
    "print(len(results_col_map_df))\n",
    "results_col_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fields_for_resource(resource, dataset):\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select f.field, fr.resource\n",
    "        from \n",
    "            fact_resource fr\n",
    "            inner join fact f on fr.fact = f.fact\n",
    "        where \n",
    "            resource = '{resource}'\n",
    "        group by\n",
    "            f.field\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}{dataset}.csv?{params}\"\n",
    "    facts_df = pd.read_csv(url)\n",
    "    # facts_list = facts_df['field'].tolist()\n",
    "    return facts_df\n",
    "\n",
    "# get_fields_for_resource(\"81ed286e34b43d1f9f3053e463a6151224b182538ce98f9064f43ebd30dc2973\", \"conservation-area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_field_resource = []\n",
    "\n",
    "for index, r in endpoint_latest_long_df.iterrows():\n",
    "    try:\n",
    "        df = get_fields_for_resource(r[\"resource\"], r[\"dataset\"])\n",
    "        df[\"dataset\"] = r[\"dataset\"]\n",
    "\n",
    "    except:\n",
    "        df = pd.DataFrame({\"resource\" : [r[\"resource\"]],\n",
    "                           \"dataset\" : [r[\"dataset\"]],\n",
    "                           \"field\" : [np.nan]\n",
    "        })\n",
    "\n",
    "    results_field_resource.append(df)\n",
    "\n",
    "results_field_resource_df = pd.concat(results_field_resource)\n",
    "\n",
    "print(len(results_field_resource_df))\n",
    "\n",
    "results_field_resource_df[\"field_present\"] = 1\n",
    "results_field_resource_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_field_resource_df[results_field_resource_df[\"field\"].isnull()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
