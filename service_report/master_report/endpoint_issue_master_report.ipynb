{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides issue/data quality information on the most recent endpoints for a hardcoded list of prioritised list of LPAs, or organisations from an input.\n",
    "\n",
    "The input should be called 'organisation_input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%pip install wget\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download helper utility files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './organisation_input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    organisation_list = ['local-authority-eng:BUC', 'local-authority-eng:DAC', 'local-authority-eng:DNC',\n",
    "    'local-authority-eng:GLO', 'local-authority-eng:CMD', 'local-authority-eng:LBH', 'local-authority-eng:SWK',\n",
    "    'local-authority-eng:MDW', 'local-authority-eng:NET', 'local-authority-eng:BIR', 'local-authority-eng:CAT',\n",
    "    'local-authority-eng:EPS', 'local-authority-eng:BNE', 'local-authority-eng:GAT', 'local-authority-eng:GRY',\n",
    "    'local-authority-eng:KTT', 'local-authority-eng:SAL', 'local-authority-eng:TEW', 'local-authority-eng:WBK',\n",
    "    'local-authority-eng:DST', 'local-authority-eng:DOV', 'local-authority-eng:LIV', 'local-authority-eng:RDB',\n",
    "    'local-authority-eng:WFT', 'local-authority-eng:NLN', 'local-authority-eng:NSM', 'local-authority-eng:SLF',\n",
    "    'local-authority-eng:WRL' ]\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get organisation names\n",
    "organisation_info_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/organisation-collection/main/data/local-authority.csv')\n",
    "organisation_info_df.head()\n",
    "organisation_name_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    organisation_code = organisation.split(':')[1]\n",
    "    organisation_name = organisation_info_df.loc[organisation_info_df['reference'] == organisation_code].iloc[0]['name']\n",
    "    organisation_name_dict[organisation] = organisation_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "# Collect latest endpoints for each organisation\n",
    "collection_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "pipelines_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree', 'tree,tree-preservation-order', 'tree-preservation-order,tree-preservation-zone']\n",
    "all_orgs_recent_endpoints={}\n",
    "for organisation in organisation_list:\n",
    "    recent_endpoints_df = get_latest_endpoints(organisation)\n",
    "    recent_endpoints_df = recent_endpoints_df[recent_endpoints_df['pipelines'].isin(pipelines_list)]\n",
    "    all_orgs_recent_endpoints[organisation] = recent_endpoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organisation_dataset_issues_dict = {}\n",
    "info_issue_types = get_issues_with_severity_info()\n",
    "for organisation in organisation_list:\n",
    "    recent_endpoints_df = all_orgs_recent_endpoints[organisation]\n",
    "    dataset_issues_dict = {}\n",
    "    for index, row in recent_endpoints_df.iterrows():\n",
    "        resource = row['resource']\n",
    "        if ',' in row['pipelines']:\n",
    "            datasets = row['pipelines'].split(',')\n",
    "        else:\n",
    "            datasets = [row['pipelines']]\n",
    "        for dataset in datasets:\n",
    "          issues_df = get_issues_for_resource(resource, dataset)\n",
    "          issues_df.drop_duplicates(subset='issue_type', keep='first', inplace=True)\n",
    "          issues = issues_df['issue_type'].values.tolist()\n",
    "          if organisation_dataset_issues_dict.get(organisation, None) and (organisation_dataset_issues_dict.get(organisation, None)).get(dataset, None):\n",
    "            existing_issues = organisation_dataset_issues_dict[organisation][dataset]\n",
    "            issues_to_add = []\n",
    "            for issue in issues:\n",
    "              # Remove existing issues and severity=info issues from list \n",
    "              if issue not in (existing_issues or info_issue_types):\n",
    "                issues_to_add.append(issue)\n",
    "            dataset_issues_dict[dataset] = existing_issues.append(issues_to_add)\n",
    "          else:\n",
    "            # Remove info issues from list\n",
    "            for issue in info_issue_types:\n",
    "               if issue in issues:\n",
    "                issues.remove(issue)\n",
    "            dataset_issues_dict[dataset] = issues\n",
    "        organisation_dataset_issues_dict[organisation] = dataset_issues_dict\n",
    "for organisation, dataset_issues in organisation_dataset_issues_dict.items():\n",
    "  for dataset, issues in dataset_issues.items():\n",
    "    if issues is None or issues == []:\n",
    "       organisation_dataset_issues_dict[organisation][dataset] = 'No issues'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def compute_cell_colour(value):\n",
    "    if value == \"No issues\":\n",
    "        return 'background-color: green'\n",
    "    elif value == \"No endpoint\":\n",
    "        return 'background-color: orange'\n",
    "    else:\n",
    "        return 'background-color: red'\n",
    "\n",
    "rows_list = []\n",
    "for organisation in organisation_list:\n",
    "    df = all_orgs_recent_endpoints[organisation]\n",
    "    df = df[pd.isna(df['end_date'])]\n",
    "    try:\n",
    "        name = organisation_name_dict[organisation]\n",
    "    except:\n",
    "        name = organisation\n",
    "    issues = {}\n",
    "        \n",
    "    new_row = {'organisation': name}\n",
    "    if organisation_dataset_issues_dict.get(organisation, None) is not None:\n",
    "        for k, v in organisation_dataset_issues_dict[organisation].items():\n",
    "            if v != 'No issues':\n",
    "                new_row[k] = ', '.join(v)\n",
    "                organisation_dataset_issues_dict[organisation][k] = ', '.join(v)\n",
    "            else:\n",
    "                new_row[k] = v\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "output_df = pd.DataFrame(rows_list, columns=['organisation', *collection_list])\n",
    "output_df = output_df.replace(np.nan, \"No endpoint\")\n",
    "output_df.to_csv('endpoint_issues_master_report.csv', index=False)\n",
    "output_df = output_df.style.applymap(compute_cell_colour, subset=collection_list)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output csv containing endpoints with issues\n",
    "output_columns = ['name', 'pipelines', 'endpoint_url', 'organisation', 'collection', 'maxentrydate', 'entrydate', 'end_date', 'last_status', 'last_updated_date']\n",
    "\n",
    "output_df = produce_output_csv(all_orgs_recent_endpoints, organisation_dataset_issues_dict, \"issues\", \"No issues\", output_columns)\n",
    "output_df.to_csv('endpoint_issues_has_issues.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
