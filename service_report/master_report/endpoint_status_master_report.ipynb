{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides status information on the latest endpoints for a hardcoded list of prioritised LPAs, or organisations from an input.\n",
    "\n",
    "The input should be called 'organisation_input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.parse\n",
    "import os\n",
    "%pip install wget\n",
    "import wget\n",
    "import requests\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download helper utility files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default prioritised LPAs are used unless a specific set of LPAs is detected using an 'organisation_input.csv' file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './organisation_input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    organisation_list = ['local-authority-eng:BUC', 'local-authority-eng:DAC', 'local-authority-eng:DNC',\n",
    "    'local-authority-eng:GLO', 'local-authority-eng:CMD', 'local-authority-eng:LBH', 'local-authority-eng:SWK',\n",
    "    'local-authority-eng:MDW', 'local-authority-eng:NET', 'local-authority-eng:BIR', 'local-authority-eng:CAT',\n",
    "    'local-authority-eng:EPS', 'local-authority-eng:BNE', 'local-authority-eng:GAT', 'local-authority-eng:GRY',\n",
    "    'local-authority-eng:KTT', 'local-authority-eng:SAL', 'local-authority-eng:TEW', 'local-authority-eng:WBK',\n",
    "    'local-authority-eng:DST', 'local-authority-eng:DOV', 'local-authority-eng:LIV', 'local-authority-eng:RDB',\n",
    "    'local-authority-eng:WFT', 'local-authority-eng:NLN', 'local-authority-eng:NSM', 'local-authority-eng:SLF',\n",
    "    'local-authority-eng:WRL']\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of organisation names, to be displayed in the output table. This is gathered separately from the main data, to ensure that if an organisation has not provided any endpoints, it is still included in the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get organisation names for output table\n",
    "organisation_info_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/organisation-collection/main/data/local-authority.csv')\n",
    "organisation_info_df.head()\n",
    "organisation_name_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    organisation_code = organisation.split(':')[1]\n",
    "    organisation_name = organisation_info_df.loc[organisation_info_df['reference'] == organisation_code].iloc[0]['name']\n",
    "    organisation_name_dict[organisation] = organisation_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest endpoints are collected for each of the organisations, for the first 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "# Collect latest endpoints for each organisation\n",
    "dataset_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "pipelines_list = ['article-4-direction', 'article-4-direction-area', 'article-4-direction,article-4-direction-area', 'conservation-area', 'conservation-area-document', 'conservation-area,conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree', 'tree,tree-preservation-order','tree,tree-preservation-zone', 'tree-preservation-order,tree-preservation-zone']\n",
    "all_orgs_latest_endpoints={}\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = get_latest_endpoints(organisation)\n",
    "    latest_endpoints_df = latest_endpoints_df[latest_endpoints_df['pipelines'].isin(pipelines_list)]\n",
    "    all_orgs_latest_endpoints[organisation] = latest_endpoints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these endpoints, the latest status that the endpoints were hit are collected. If there is no status (e.g. connection error), the exception is used instead.\n",
    "If an endpoint contributes to two datasets, it will only be considered for a dataset if it is the newest endpoint for that dataset, calculated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def compute_cell_colour(status):\n",
    "    if status == \"200\":\n",
    "        return 'background-color: green'\n",
    "    elif status == 'No endpoint':\n",
    "        return 'background-color: orange'\n",
    "    else:\n",
    "        return 'background-color: red'\n",
    "    \n",
    "def cut_zeros(row):\n",
    "  if row[-2:]=='.0':\n",
    "    row=row[:-2]\n",
    "  return row\n",
    "\n",
    "# Only display non 200 statuses if they have been non 200 for more than 5 days\n",
    "def compute_displayed_status(row):\n",
    "    # Check if the most recent status isn't 200\n",
    "    if row[\"last_status\"] == 200:\n",
    "        last_200_date = pd.to_datetime(row[\"last_updated_date\"])\n",
    "    elif row[\"last_status\"] != None:\n",
    "        last_200_date = pd.to_datetime(row[\"date_last_status_200\"])\n",
    "    else:\n",
    "        # If the most recent status is 200 then we can return 200\n",
    "        return 200\n",
    "    \n",
    "    days_since_200 = (row[\"maxentrydate\"] - last_200_date).days\n",
    "    # Only show non 200 statuses if they have been non 200 for more than 5 days\n",
    "    if days_since_200 >= 5:\n",
    "        status = row['status']\n",
    "        # Handle cases where there is no status by looking at the exception\n",
    "        if not pd.isna(status):\n",
    "            status = int(status)\n",
    "        else:\n",
    "            status=latest_endpoints_df.loc[latest_endpoints_df['status'].isna(), 'exception'].values[0]\n",
    "            if status is None:\n",
    "                status=\"Unknown Error\"\n",
    "        return status\n",
    "    else:\n",
    "        return 200\n",
    "\n",
    "\n",
    "rows_list = []\n",
    "organisation_dataset_statuses_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    latest_endpoints_df = all_orgs_latest_endpoints[organisation]\n",
    "    latest_endpoints_df = latest_endpoints_df[pd.isna(latest_endpoints_df['end_date'])]\n",
    "    try:\n",
    "        name = organisation_name_dict[organisation]\n",
    "    except:\n",
    "        name = organisation\n",
    "    \n",
    "    dataset_statuses_dict = {}\n",
    "    for index, row in latest_endpoints_df.iterrows():\n",
    "        if 'WFS' in row['endpoint_url']:\n",
    "            response = requests.get(row['endpoint_url'], stream=True)\n",
    "            try:\n",
    "                content = next(response.iter_content(chunk_size=1024)).decode('utf-8')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                content = response.text\n",
    "            if 'Cannot find layer' in content:\n",
    "                row['status']='Cannot find layer'\n",
    "        resource = row['resource']\n",
    "        if ',' in row['pipelines']:\n",
    "            datasets = row['pipelines'].split(',')\n",
    "        else:\n",
    "            datasets = [row['pipelines']]\n",
    "        for dataset in datasets:\n",
    "            # Consider cases where a dataset is contributed to by multiple endpoints\n",
    "            same_datasets_df = latest_endpoints_df[latest_endpoints_df[\"pipelines\"].apply(lambda x: dataset in x.split(','))]\n",
    "            if len(same_datasets_df) > 1:\n",
    "                skip_dataset = handle_skip_dataset(same_datasets_df, dataset, row)\n",
    "            else:\n",
    "                skip_dataset = False\n",
    "\n",
    "            if not skip_dataset:\n",
    "                dataset_statuses_dict[dataset] = compute_displayed_status(row)\n",
    "    organisation_dataset_statuses_dict[organisation] = dataset_statuses_dict\n",
    "   \n",
    "    new_row = {'organisation': name}\n",
    "    new_row.update(dataset_statuses_dict)\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "output_df = pd.DataFrame(rows_list, columns=['organisation', *dataset_list])\n",
    "output_df = output_df.replace(np.nan, \"No endpoint\")\n",
    "\n",
    "output_df = output_df.astype(str)\n",
    "output_df = output_df.applymap(cut_zeros)\n",
    "\n",
    "output_df.to_csv('endpoint_status_master_report.csv', index=False)\n",
    "output_df.style.applymap(compute_cell_colour, subset=dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An output .csv under the name 'endpoint_status_not_200.csv' is created, containing the latest endpoints that do not have a status of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output csv containing endpoints with a status other than 200\n",
    "not_200_output_columns = ['name', 'pipelines', 'endpoint_url', 'organisation', 'collection', 'maxentrydate', 'entrydate', 'end_date', 'last_status', 'last_updated_date']\n",
    "\n",
    "not_200_output_df = produce_output_csv(all_orgs_latest_endpoints, organisation_dataset_statuses_dict, \"status\", 200, not_200_output_columns)\n",
    "not_200_output_df.to_csv('endpoint_status_not_200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output csv containing endpoints with any status\n",
    "all_status_output_columns = ['name', 'pipelines', 'endpoint_url', 'organisation', 'collection', 'maxentrydate', 'entrydate', 'end_date', 'last_status', 'last_updated_date']\n",
    "\n",
    "all_status_output_df = produce_output_csv(all_orgs_latest_endpoints, organisation_dataset_statuses_dict, \"status\", \"\", all_status_output_columns)\n",
    "all_status_output_df.to_csv('all_status_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
