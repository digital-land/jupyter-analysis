{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides status information on the most recent endpoints for a hardcoded list of prioritised list of LPAs, or organisations from an input.\n",
    "\n",
    "The input should be called 'input.csv' and contain one column, 'organisation' that has the organisation codes for the LPAs to be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "%pip install wget\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_file = \"master_report_endpoint_utils.py\"\n",
    "if os.path.isfile(util_file):\n",
    "    from master_report_endpoint_utils import *\n",
    "else:\n",
    "    url = \"https://raw.githubusercontent.com/digital-land/jupyter-analysis/main/service_report/master_report/master_report_endpoint_utils.py\"\n",
    "    wget.download(url)\n",
    "    from master_report_endpoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from .csv or use default prioritised LPAs\n",
    "input_path = './input.csv'\n",
    "if os.path.isfile(input_path):\n",
    "    input_df = pd.read_csv(input_path)\n",
    "    organisation_list = input_df['organisation'].tolist()\n",
    "    print('Input file found. Using', len(organisation_list), 'organisations from input file.')\n",
    "else:\n",
    "    organisation_list = ['local-authority-eng:BUC', 'local-authority-eng:DAC', 'local-authority-eng:DNC',\n",
    "    'local-authority-eng:GLO', 'local-authority-eng:CMD', 'local-authority-eng:LBH', 'local-authority-eng:SWK',\n",
    "    'local-authority-eng:MDW', 'local-authority-eng:NET', 'local-authority-eng:BIR', 'local-authority-eng:CAT',\n",
    "    'local-authority-eng:EPS', 'local-authority-eng:BNE', 'local-authority-eng:GAT', 'local-authority-eng:GRY',\n",
    "    'local-authority-eng:KTT', 'local-authority-eng:SAL', 'local-authority-eng:TEW', 'local-authority-eng:WBK',\n",
    "    'local-authority-eng:DST', 'local-authority-eng:DOV', 'local-authority-eng:LIV', 'local-authority-eng:RDB',\n",
    "    'local-authority-eng:WFT', 'local-authority-eng:NLN', 'local-authority-eng:NSM', 'local-authority-eng:SLF',\n",
    "    'local-authority-eng:WRL' ]\n",
    "    print('Input file not found. Using default list of organisations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get organisation info\n",
    "organisation_info_df = pd.read_csv('https://raw.githubusercontent.com/digital-land/organisation-collection/main/data/local-authority.csv')\n",
    "organisation_info_df.head()\n",
    "organisation_name_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    organisation_code = organisation.split(':')[1]\n",
    "    organisation_name = organisation_info_df.loc[organisation_info_df['reference'] == organisation_code].iloc[0]['name']\n",
    "    organisation_name_dict[organisation] = organisation_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "def update_dataframe(organisation):\n",
    "    global result_df  \n",
    "    if organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          e.endpoint_url,\n",
    "          l.status,\n",
    "          l.exception,\n",
    "          s.collection,\n",
    "          group_concat(DISTINCT sp.pipeline) as pipelines,\n",
    "          s.organisation,\n",
    "          o.name,\n",
    "          max(l.entry_date) maxentrydate,\n",
    "          max(e.entry_date) entrydate,\n",
    "          e.end_date\n",
    "        from\n",
    "          log l\n",
    "          inner join source s on l.endpoint = s.endpoint\n",
    "          inner join organisation o on s.organisation=o.organisation\n",
    "          inner join endpoint e on l.endpoint = e.endpoint\n",
    "          inner join source_pipeline sp on s.source = sp.source\n",
    "        where\n",
    "           {query} and not collection=\"brownfield-land\"\n",
    "        group by\n",
    "          l.endpoint,\n",
    "          l.status\n",
    "        order by\n",
    "          l.endpoint,\n",
    "          s.collection,\n",
    "          maxentrydate desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    result_df = df\n",
    "    return df\n",
    "\n",
    "def update_dataframe_latest_status(organisation):\n",
    "    global new_df\n",
    "    all_endpoints=update_dataframe(organisation)\n",
    "    new_df=all_endpoints.copy()\n",
    "    new_df['maxentrydate'] = pd.to_datetime(new_df['maxentrydate'])\n",
    "    new_df['last_status'] = None\n",
    "    new_df['last_updated_date'] = None\n",
    "    new_df['date_last_status_200'] = None\n",
    "    \n",
    "    for index, row in new_df.iterrows():\n",
    "        if index < len(new_df) - 1 and (row['status']!=200 or pd.isna(row['status'])):\n",
    "            if row['endpoint_url'] == new_df.at[index + 1, 'endpoint_url']:\n",
    "                new_df.at[index, 'last_status'] = new_df.at[index + 1, 'status']\n",
    "                new_df.at[index, 'last_updated_date'] = new_df.at[index + 1, 'maxentrydate']   \n",
    "    \n",
    "    new_df.drop_duplicates(subset='endpoint_url', keep='first', inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    for index, row in new_df.iterrows():\n",
    "        if row['last_status'] is not None:\n",
    "            if row['last_status'] != 200  or row['last_status'] is None:\n",
    "                filtered_df = all_endpoints[(all_endpoints['endpoint_url'] == row['endpoint_url'] ) & (all_endpoints['status'] == 200)]\n",
    "                if not filtered_df.empty:\n",
    "                    new_df.at[index, 'date_last_status_200'] = filtered_df['maxentrydate'].values[0][:19] \n",
    "    return new_df\n",
    "\n",
    "collection_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree']\n",
    "pipelines_list = ['article-4-direction', 'article-4-direction-area', 'conservation-area', 'conservation-area-document', 'listed-building-outline', 'tree-preservation-order', 'tree-preservation-zone', 'tree', 'tree,tree-preservation-order', 'tree-preservation-order,tree-preservation-zone','article-4-direction,article-4-direction-area']\n",
    "all_orgs_recent_endpoints={}\n",
    "for organisation in organisation_list:\n",
    "    recent_endpoints_df = update_dataframe_latest_status(organisation)\n",
    "    recent_endpoints_df = recent_endpoints_df[recent_endpoints_df['pipelines'].isin(pipelines_list)]\n",
    "    all_orgs_recent_endpoints[organisation] = recent_endpoints_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orgs_recent_endpoints[\"local-authority-eng:BUC\"].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def compute_cell_colour(status):\n",
    "    if status == 200:\n",
    "        return 'background-color: green'\n",
    "    elif status == 'No endpoint':\n",
    "        return 'background-color: orange'\n",
    "    else:\n",
    "        return 'background-color: red'\n",
    "    \n",
    "def compute_status(existing_statuses, collection, status):\n",
    "    if collection in existing_statuses:\n",
    "        if existing_statuses[collection] == 200:\n",
    "            return status\n",
    "        elif existing_statuses[collection] == 'No endpoint':\n",
    "            return status\n",
    "        else:\n",
    "            return existing_statuses[collection]\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "rows_list = []\n",
    "organisation_dataset_statuses_dict = {}\n",
    "for organisation in organisation_list:\n",
    "    # organisation = \"local-authority-eng:BUC\"\n",
    "    df = all_orgs_recent_endpoints[organisation]\n",
    "    df = df[pd.isna(df['end_date'])]\n",
    "    try:\n",
    "        name = organisation_name_dict[organisation]\n",
    "    except:\n",
    "        name = organisation\n",
    "    dataset_statuses_dict = {}\n",
    "    for pipeline in pipelines_list:\n",
    "        status = df[df['pipelines'] == pipeline]['status']\n",
    "        df_datasets = pd.DataFrame(df['pipelines'], columns=[\"pipelines\"])\n",
    "        list_datasets = df_datasets['pipelines'].tolist()\n",
    "        if ',' in pipeline:\n",
    "            if pipeline in list_datasets:\n",
    "                datasets = pipeline.split(',')\n",
    "                status = pd.Series(status)\n",
    "                if status.empty or (status=='No endpoint').any():\n",
    "                    status = 'No endpoint'\n",
    "                else:\n",
    "                    if not pd.isna(status.values[0]):\n",
    "                        status = int(status.values[0])\n",
    "                    else:\n",
    "                        status=df.loc[df['status'].isna(), 'exception'].values[0]\n",
    "                        if status is None:\n",
    "                            status=\"Unknown Error\"\n",
    "                for i in datasets:\n",
    "                    dataset_statuses_dict[i] = compute_status(dataset_statuses_dict, i, status)\n",
    "        else:\n",
    "            if status.empty:\n",
    "                status = 'No endpoint'\n",
    "            else:\n",
    "                df_status = status.to_frame()\n",
    "                df_no_duplicates = df_status.drop_duplicates()\n",
    "                df_filtered = df_no_duplicates[df_no_duplicates['status'] != 200]\n",
    "                if df_filtered.size>0:\n",
    "                    if not pd.isna(df_filtered.iloc[0, 0]):\n",
    "                        status=int(df_filtered.iloc[0, 0])\n",
    "                    else:\n",
    "                        status=df.loc[df['status'].isna(), 'exception'].values[0]\n",
    "                        if status is None:\n",
    "                            status=\"Unknown Error\"\n",
    "                else:\n",
    "                    status=200\n",
    "            dataset_statuses_dict[pipeline] = compute_status(dataset_statuses_dict, pipeline, status)\n",
    "    organisation_dataset_statuses_dict[organisation] = dataset_statuses_dict\n",
    "    new_row = {'organisation': name}\n",
    "    new_row.update(dataset_statuses_dict)\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "output_df = pd.DataFrame(rows_list, columns=['organisation', *collection_list])\n",
    "output_df.to_csv('endpoint_status_master_report.csv', index=False)\n",
    "output_df.style.applymap(compute_cell_colour, subset=collection_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_columns = ['name', 'pipelines', 'endpoint_url', 'organisation', 'collection', 'maxentrydate', 'entrydate', 'end_date', 'last_status', 'last_updated_date']\n",
    "\n",
    "output_df = produce_output_csv(all_orgs_recent_endpoints, organisation_dataset_statuses_dict, \"status\", \"200\", output_columns)\n",
    "output_df.to_csv('endpoint_status_not_200.csv', index=False)\n",
    "output_df.head(1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
