{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "from datetime import datetime as dt\n",
    "from IPython.display import display\n",
    "import urllib\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Endpoints Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "primary_lpas = [\"local-authority-eng:BUC\",\"local-authority-eng:DAC\",\"local-authority-eng:DNC\",\"local-authority-eng:GLO\",\"local-authority-eng:CMD\",\"local-authority-eng:LBH\",\"local-authority-eng:SWK\",\"local-authority-eng:MDW\",\"local-authority-eng:NET\", \"local-authority-eng:BIR\",\"local-authority-eng:CAT\",\"local-authority-eng:EPS\",\"local-authority-eng:BNE\",\"local-authority-eng:GAT\",\"local-authority-eng:GRY\",\"local-authority-eng:KTT\",\"local-authority-eng:SAL\",\"local-authority-eng:TEW\",\"local-authority-eng:WBK\",\"local-authority-eng:DST\",\"local-authority-eng:DOV\",\"local-authority-eng:LIV\",\"local-authority-eng:RDB\",\"local-authority-eng:WFT\",\"local-authority-eng:NLN\",\"local-authority-eng:NSM\",\"local-authority-eng:SLF\",\"local-authority-eng:WRL\"]\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe(organisation):\n",
    "    global result_df\n",
    "    if organisation == \"Primary LPA's\":\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    elif organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          e.endpoint_url,\n",
    "          l.status,\n",
    "          l.exception,\n",
    "          s.collection,\n",
    "          group_concat(DISTINCT sp.pipeline) as pipelines,\n",
    "          s.organisation,\n",
    "          o.name,\n",
    "          max(l.entry_date) maxentrydate,\n",
    "          max(e.entry_date) entrydate,\n",
    "          e.end_date\n",
    "        from\n",
    "          log l\n",
    "          inner join source s on l.endpoint = s.endpoint\n",
    "          inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "          inner join endpoint e on l.endpoint = e.endpoint\n",
    "          inner join source_pipeline sp on s.source = sp.source\n",
    "        where\n",
    "           {query} and not collection=\"brownfield-land\"\n",
    "        group by\n",
    "          l.endpoint,\n",
    "          l.status\n",
    "        order by\n",
    "          l.endpoint,\n",
    "          s.collection,\n",
    "          maxentrydate desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    result_df = pd.read_csv(url)\n",
    "    if(organisation == \"Primary LPA's\"):\n",
    "        result_df = result_df[result_df['organisation'].isin(primary_lpas)].reset_index(drop=True)\n",
    "    return result_df\n",
    "\n",
    "def get_provisions():\n",
    "    global provisions_df  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            p.cohort,\n",
    "            p.notes,\n",
    "            p.organisation,\n",
    "            p.project,\n",
    "            p.provision_reason,\n",
    "            o.reference,\n",
    "            o.name\n",
    "        FROM\n",
    "            provision AS p\n",
    "        JOIN\n",
    "            organisation AS o ON SUBSTR(p.organisation, INSTR(p.organisation, ':') + 1) = o.reference\n",
    "        WHERE\n",
    "            p.cohort IN (\"ODP-Track1\", \"ODP-Track3\", \"ODP-Track2\", \"RIPA-BOPS\")\n",
    "            AND p.provision_reason = \"expected\"\n",
    "        GROUP BY\n",
    "            p.organisation\n",
    "        ORDER BY\n",
    "            p.cohort\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    provisions_df = pd.read_csv(url)\n",
    "    return provisions_df\n",
    "\n",
    "def get_org_dict():\n",
    "    global organisation_options\n",
    "    provisions_df = get_provisions()[[\"organisation\", \"name\"]]\n",
    "    provisions_df.loc[:, \"organisation\"] = provisions_df[\"organisation\"].str.replace(\":\", \"-eng:\")\n",
    "    organisation_options = dict(zip(provisions_df['name'], provisions_df['organisation']))\n",
    "    organisation_options.update({\"All LPA's\":None})\n",
    "    return organisation_options\n",
    "\n",
    "global organisation_dropdown\n",
    "organisation_dropdown = widgets.Dropdown(\n",
    "    options = get_org_dict(),\n",
    "    description=\"Select LPA:\",\n",
    ")\n",
    "\n",
    "widgets.interact(update_dataframe, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with all endpoints? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    result_df.to_csv(\"endpoints_with_all_status.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_with_all_status.csv'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints with current/latest status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe_latest_status(organisation):\n",
    "    global new_df\n",
    "    all_endpoints=update_dataframe(organisation)\n",
    "    new_df=all_endpoints.copy()\n",
    "    new_df['maxentrydate'] = pd.to_datetime(new_df['maxentrydate'])\n",
    "    new_df['last_status'] = None\n",
    "    new_df['last_updated_date'] = None\n",
    "    new_df['date_last_status_200'] = None\n",
    "    \n",
    "    for index, row in new_df.iterrows():\n",
    "        if index < len(new_df) - 1 and (row['status']!=200 or pd.isna(row['status'])):\n",
    "            if row['endpoint_url'] == new_df.at[index + 1, 'endpoint_url']:\n",
    "                new_df.at[index, 'last_status'] = new_df.at[index + 1, 'status']\n",
    "                new_df.at[index, 'last_updated_date'] = new_df.at[index + 1, 'maxentrydate']   \n",
    "    \n",
    "    new_df.drop_duplicates(subset='endpoint_url', keep='first', inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    for index, row in new_df.iterrows():\n",
    "        if row['last_status'] is not None:\n",
    "            if row['last_status'] != 200  or row['last_status'] is None:\n",
    "                filtered_df = all_endpoints[(all_endpoints['endpoint_url'] == row['endpoint_url'] ) & (all_endpoints['status'] == 200)]\n",
    "                if not filtered_df.empty:\n",
    "                    new_df.at[index, 'date_last_status_200'] = filtered_df['maxentrydate'].values[0][:19] \n",
    "    return new_df\n",
    "\n",
    "widgets.interact(update_dataframe_latest_status, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with latest endpoints? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    new_df.to_csv(\"endpoints_with_latest_status.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_with_latest_status.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints with status NOT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe_erroring_endpoints(organisation):\n",
    "    global filtered_df\n",
    "    filtered_df=update_dataframe_latest_status(organisation)\n",
    "    filtered_df = filtered_df[filtered_df['status'] != 200] \n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    return filtered_df\n",
    "\n",
    "widgets.interact(update_dataframe_erroring_endpoints, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with erroring endpoints being collected till date? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    filtered_df.to_csv(\"endpoints_not_200.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_not_200.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 4 datasets - Endpoints with status NOT 200 - All LPA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe1(collection):\n",
    "    global df1  \n",
    "    if collection:\n",
    "        query = f\" s.collection = '{collection}'\"\n",
    "    else:\n",
    "        query = f\" s.collection IN ('article-4-direction', 'listed-building', 'tree-preservation-order', 'conservation-area','article-4-direction-area','article-4-direction-rule','listed-building-grade','listed-building-outline','listed-building-building','tree','tree-preservation-zone','tree-preservation-zone-type')\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          e.endpoint_url,\n",
    "          l.status,\n",
    "          l.exception,\n",
    "          s.collection,\n",
    "          group_concat(DISTINCT sp.pipeline) as pipelines,\n",
    "          s.organisation,\n",
    "          o.name,\n",
    "          max(l.entry_date) maxentrydate,\n",
    "          max(e.entry_date) entrydate,\n",
    "          e.end_date\n",
    "        from\n",
    "          log l\n",
    "          inner join source s on l.endpoint = s.endpoint\n",
    "          inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "          inner join endpoint e on l.endpoint = e.endpoint\n",
    "          inner join source_pipeline sp on s.source = sp.source\n",
    "        where\n",
    "           {query} \n",
    "        group by\n",
    "          l.endpoint,\n",
    "          l.status\n",
    "        order by\n",
    "          pipelines,\n",
    "          o.name,\n",
    "          maxentrydate desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df1 = pd.read_csv(url)\n",
    "    result_df1 = df1.copy()\n",
    "\n",
    "    df1['maxentrydate'] = pd.to_datetime(df1['maxentrydate'])\n",
    "    df1['last_status'] = None\n",
    "    df1['last_updated_date'] = None\n",
    "    df1['date_last_status_200'] = None\n",
    "    \n",
    "    for index, row in df1.iterrows():\n",
    "        if index < len(df1) - 1 and (row['status']!=200 or pd.isna(row['status'])):\n",
    "            if row['endpoint_url'] == df1.at[index + 1, 'endpoint_url']:\n",
    "                df1.at[index, 'last_status'] = df1.at[index + 1, 'status']\n",
    "                df1.at[index, 'last_updated_date'] = df1.at[index + 1, 'maxentrydate']   \n",
    "    \n",
    "    df1.drop_duplicates(subset='endpoint_url', keep='first', inplace=True)\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    for index, row in df1.iterrows():\n",
    "        if row['last_status'] is not None:\n",
    "                if row['last_status'] != 200  or row['last_status'] is None:\n",
    "                    filtered_df = result_df1[(result_df1['endpoint_url'] == row['endpoint_url'] ) & (result_df1['status'] == 200)]\n",
    "                    if not filtered_df.empty:\n",
    "                        df1.at[index, 'date_last_status_200'] = filtered_df['maxentrydate'].values[0][:19]\n",
    "    df1 = df1[df1['status'] != 200]\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df1\n",
    "\n",
    "\n",
    "global collection_options    \n",
    "collection_options = {\n",
    "    \"All 4 datasets\":None,\"article 4 direction\": \"article-4-direction\",\"conservation area\": \"conservation-area\",\"listed building\": \"listed-building\",\n",
    "    \"tree preservation order\": \"tree-preservation-order\"\n",
    "    \n",
    "}\n",
    "global organisation_dropdown\n",
    "collection_dropdown = widgets.Dropdown(\n",
    "    options=collection_options,\n",
    "    description=\"Select dataset:\",\n",
    ")\n",
    "\n",
    "widgets.interact(update_dataframe1, collection=collection_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with erroring endpoints being collected till date? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    df1.to_csv(\"endpoints_not_200_first_4_datasets.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_not_200_first_4_datasets.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the Number of Distinct Non-ended Endpoints\n",
    "\n",
    "This SQL query will **count** the total number of **distinct** and **active** endpoint urls for each collection dataset, for each organisation, and will only return rows with counts greater than 1. Datasets with more than one endpoint url can then be further investigated, so see whether these additional endpoint urls are disfunctional or no longer needed.\n",
    "\n",
    "**N.B:**\n",
    "- Whether the endpoint_url is active it determined by the presence of an end_date value.\n",
    "- Currently the LPA list isn't exhaustive and may need to be added to (defined above) but \"all LPAs\" can be selected.\n",
    "- The BFL dataset is not included in this query because of how large the BFL dataset is.\n",
    "- The number of non-ended endpoints where `status != 200` can also be counted here but it pushes the datasette query limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_aggregate(organisation):\n",
    "    global endpoint_aggregate  \n",
    "    if organisation == \"Primary LPA's\":\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    elif organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            s.organisation AS organisation,\n",
    "            s.collection AS collection,\n",
    "            sp.pipeline AS pipeline,\n",
    "            COUNT ( \n",
    "                DISTINCT \n",
    "                    CASE \n",
    "                        WHEN e.end_date = \"\" THEN e.endpoint_url ELSE NULL END\n",
    "                )\n",
    "                AS non_ended_endpoints,\n",
    "            COUNT ( \n",
    "                DISTINCT \n",
    "                    CASE \n",
    "                        WHEN e.end_date != \"\" THEN e.endpoint_url ELSE NULL END\n",
    "                )\n",
    "                AS ended_endpoints\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          LEFT JOIN source_pipeline sp ON s.source = sp.source\n",
    "        WHERE\n",
    "           ({query})\n",
    "           AND (NOT collection=\"brownfield-land\")\n",
    "        GROUP BY\n",
    "            s.organisation,\n",
    "            s.collection,\n",
    "            sp.pipeline\n",
    "        HAVING\n",
    "            non_ended_endpoints > 1\n",
    "        ORDER BY\n",
    "            non_ended_endpoints DESC\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    endpoint_aggregate = pd.read_csv(url)\n",
    "    if(organisation == \"Primary LPA's\"):\n",
    "        endpoint_aggregate = endpoint_aggregate[endpoint_aggregate['organisation'].isin(primary_lpas)].reset_index(drop=True)\n",
    "    return endpoint_aggregate\n",
    "\n",
    "widgets.interact(get_endpoint_aggregate, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    endpoint_aggregate.to_csv(\"endpoint_aggregate.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoint_aggregate.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status and end_date of **all** Distinct Endpoints\n",
    "\n",
    "This cell contains functions which retrieve **all** distinct **non BFL** endpoints, grouped by collection and pipeline **(not just the endpoints counted above)**. The endpoint entry date, end date, most recent log date and most recent status for each endpoint are retrieved to help deduce whether the endpoint is no longer functional and if it has been manually listed as an old endpoint.\n",
    "\n",
    "**N.B:** \n",
    "- This query pushes the datasette timeout limit, and often gives an internal server (500) error, rerun this and following cells if this is the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_logs():\n",
    "    global most_recent_logs  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT e.endpoint AS endpoint_hash, MAX(l.entry_date) AS most_recent_entry_date, l.status AS most_recent_status\n",
    "        FROM endpoint e\n",
    "        JOIN log l ON e.endpoint = l.endpoint\n",
    "        GROUP BY e.endpoint\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    most_recent_logs = pd.read_csv(url)\n",
    "    return most_recent_logs\n",
    "\n",
    "def get_distinct_endpoints_status_and_end_dates(organisation):\n",
    "    global distinct_endpoints_status_and_end_dates_df  \n",
    "    if organisation == \"Primary LPA's\":\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    elif organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            s.organisation AS organisation,\n",
    "            s.collection AS collection,\n",
    "            sp.pipeline AS pipeline,\n",
    "            e.endpoint_url AS endpoint,\n",
    "            e.endpoint AS endpoint_hash,\n",
    "            e.entry_date,\n",
    "            s.end_date AS end_date\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          LEFT JOIN source_pipeline sp ON s.source = sp.source\n",
    "        WHERE\n",
    "           ({query})\n",
    "           AND (NOT collection=\"brownfield-land\")\n",
    "        GROUP BY\n",
    "            s.collection,\n",
    "            sp.pipeline,\n",
    "            e.endpoint\n",
    "        ORDER BY\n",
    "            s.collection,\n",
    "            sp.pipeline\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    distinct_endpoints_status_and_end_dates_df = pd.read_csv(url)\n",
    "    if(organisation == \"Primary LPA's\"):\n",
    "        distinct_endpoints_status_and_end_dates_df = distinct_endpoints_status_and_end_dates_df[distinct_endpoints_status_and_end_dates_df['organisation'].isin(primary_lpas)].reset_index(drop=True)\n",
    "    return distinct_endpoints_status_and_end_dates_df\n",
    "\n",
    "def combine_dataframes(organisation):\n",
    "    global distinct_endpoints_status_and_end_dates_with_status_df\n",
    "    left_df = get_distinct_endpoints_status_and_end_dates(organisation)\n",
    "    right_df = get_most_recent_logs()\n",
    "    distinct_endpoints_status_and_end_dates_with_status_df = pd.merge(left_df, right_df, left_on=['endpoint_hash'], right_on=['endpoint_hash'])\n",
    "    return distinct_endpoints_status_and_end_dates_with_status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Datasets with Multiple Endpoint Urls\n",
    "The `endpoint_aggregate` dataframe counts the number of active endpoints for each organisation dataset, the function below merges this with the `distinct_endpoints_status_and_end_dates_with_status_df` to list all suspect endpoints in more detail, rather than their aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suspect_endpoints(organisation):\n",
    "    global possible_duplicate_endpoints\n",
    "    # Create both dataframes to be joined\n",
    "    all_distinct_endpoints = combine_dataframes(organisation)\n",
    "    endpoint_aggregate = get_endpoint_aggregate(organisation)\n",
    "    # A One-to-many merge\n",
    "    df = pd.merge(all_distinct_endpoints, endpoint_aggregate, left_on=['organisation', 'collection', 'pipeline'], right_on=['organisation', 'collection', 'pipeline'])\n",
    "    #Removing endpoint rows which had no match with the endpoint_aggregate dataframe\n",
    "    df = df.dropna(subset=[\"non_ended_endpoints\"]).drop([\"non_ended_endpoints\", \"ended_endpoints\"], axis=1)\n",
    "    # Dropping rows with an end_date value (non-active), sorting and reseting the index field\n",
    "    df = df.drop(df[df.end_date.notnull()].index, axis=0).drop([\"end_date\"], axis=1).sort_values(by = [\"organisation\", \"collection\", \"pipeline\"]).reset_index(drop=True)\n",
    "    possible_duplicate_endpoints = df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider when these endpoints were last successfully accessed\n",
    "The functions below add the field describing \"the timestamp the endpoint was last successfully accessed\" (gave a 200-like response) to the `possible_duplicate_endpoints` dataframe generated above, this dataframe is then rendered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_when_successfully_accessed():\n",
    "    global last_successfully_accessed  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            MAX(l.entry_date) AS last_200_LIKE_response_timestamp,\n",
    "            e.endpoint AS endpoint_hash\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "        WHERE\n",
    "            (NOT collection=\"brownfield-land\")\n",
    "            AND l.status LIKE \"2%\"\n",
    "        GROUP BY\n",
    "            e.endpoint_url\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    last_successfully_accessed = pd.read_csv(url)\n",
    "    return last_successfully_accessed\n",
    "\n",
    "def append_last_successfully_accessed(organisation):\n",
    "    global possible_duplicate_endpoints_last_200\n",
    "    possible_duplicate_endpoints_last_200 = pd.merge(get_suspect_endpoints(organisation), get_when_successfully_accessed(), left_on=['endpoint_hash'], right_on=['endpoint_hash'])\n",
    "    return possible_duplicate_endpoints_last_200\n",
    "\n",
    "widgets.interact(append_last_successfully_accessed, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    possible_duplicate_endpoints_last_200.to_csv(\"possible_duplicate_endpoints_last_200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Stale Endpoints\n",
    "Endpoints which were last successfully accessed over **5 days** ago can be assumed faulty, or effectively ended and can be recommended for removal by the standard process https://docs.google.com/document/d/1Xm1frOBY-J4mLfigXuFdeq976cQGghhnt0gbmZleAyc/edit#heading=h.y6u78drjip12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stale_endpoints(organisation):\n",
    "    global stale_endpoints\n",
    "    # Grab datetime of 5 days ago relative to current date\n",
    "    five_days_ago_timestamp = pd.to_datetime('today').normalize().tz_localize(\"Europe/London\" ,ambiguous=True) - pd.Timedelta(days=5)\n",
    "    # Assign new df variable and convert most_recent_status to string for later string comparison\n",
    "    df = append_last_successfully_accessed(organisation)\n",
    "    df[\"most_recent_status\"] = df[\"most_recent_status\"].astype(str)\n",
    "    # Convert last_200_response_timestamp field to datetime\n",
    "    df[\"last_200_LIKE_response_timestamp\"] = pd.to_datetime(df[\"last_200_LIKE_response_timestamp\"])\n",
    "    # Grab non-200 data by comparison\n",
    "    df = df[~df[\"most_recent_status\"].str.contains(\"2\")]\n",
    "    # Filter for entries which only returned 200 over 5 days ago\n",
    "    df = df[df['last_200_LIKE_response_timestamp'] < five_days_ago_timestamp].reset_index(drop=True)\n",
    "    # Convert most_recent_status back to float\n",
    "    df[\"most_recent_status\"] = df[\"most_recent_status\"].astype(float)\n",
    "    stale_endpoints =df\n",
    "    return df\n",
    "\n",
    "widgets.interact(get_stale_endpoints, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the stale_endpoints table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    stale_endpoints.to_csv(\"stale_endpoints.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Endpoints then Converting to Lists\n",
    "The `possible_duplicate_endpoints` dataframe above is grouped by organisation, collection and pipeline, all unique endpoint urls are then placed into a list so that they can be easily looped through. This is so that the csv contents of each endpoint can be compared in future, and then decide on which endpoints to keep.\n",
    "\n",
    "**N.B:** This currently includes the endpoints previously highlighted as stale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_endpoints_per_org_dataset(organisation):\n",
    "    global possible_duplicate_endpoints_aggregate\n",
    "    # Remove unnecessary columns\n",
    "    df = get_suspect_endpoints(organisation).drop([\"most_recent_status\"], axis = 1)\n",
    "    # Group by organisation, collection and pipeline, append the aggregated endpoint urls into a list then reset the index\n",
    "    df = df.groupby([\"organisation\",\"collection\",\"pipeline\"])[\"endpoint\"].apply(list).reset_index()\n",
    "    possible_duplicate_endpoints_aggregate = df\n",
    "    return possible_duplicate_endpoints_aggregate\n",
    "\n",
    "widgets.interact(list_endpoints_per_org_dataset, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value\n",
    "\n",
    "# This can be looped through programmatically to check the contents of the remaining endpoints to be eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the possible_duplicate_endpoints_aggregate table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    possible_duplicate_endpoints_aggregate.to_csv(\"possible_duplicate_endpoints_aggregate.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
