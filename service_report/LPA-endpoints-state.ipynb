{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from IPython.display import display\n",
    "import urllib\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Endpoints Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8e60d646c1461399b28db2b3ace85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select LPA:', options={\"All LPA's\": None, 'Newcastle': 'local-auth…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe(organisation):\n",
    "    global result_df  \n",
    "    if organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          e.endpoint_url,\n",
    "          l.status,\n",
    "          l.exception,\n",
    "          s.collection,\n",
    "          group_concat(DISTINCT sp.pipeline) as pipelines,\n",
    "          s.organisation,\n",
    "          o.name,\n",
    "          max(l.entry_date) maxentrydate,\n",
    "          max(e.entry_date) entrydate,\n",
    "          e.end_date\n",
    "        from\n",
    "          log l\n",
    "          inner join source s on l.endpoint = s.endpoint\n",
    "          inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "          inner join endpoint e on l.endpoint = e.endpoint\n",
    "          inner join source_pipeline sp on s.source = sp.source\n",
    "        where\n",
    "           {query} and not collection=\"brownfield-land\"\n",
    "        group by\n",
    "          l.endpoint,\n",
    "          l.status\n",
    "        order by\n",
    "          l.endpoint,\n",
    "          s.collection,\n",
    "          maxentrydate desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    result_df = df\n",
    "    return df\n",
    "\n",
    "global organisation_options    \n",
    "organisation_options = {\n",
    "    \"All LPA's\":None,\"Newcastle\": \"local-authority-eng:NET\",\"Medway\": \"local-authority-eng:MDW\",\"Lambeth\": \"local-authority-eng:LBH\",\n",
    "    \"Gloucester\": \"local-authority-eng:GLO\",\"Doncaster\": \"local-authority-eng:DNC\",\"Buckinghamshire\": \"local-authority-eng:BUC\",\"Epsom and Ewell\": \"local-authority-eng:EPS\",\n",
    "    \"Canterbury\": \"local-authority-eng:CAT\",\"Bolton\": \"local-authority-eng:BOL\", \"London Borough of Southwark\": \"local-authority-eng:SWK\"\n",
    "    \n",
    "}\n",
    "global organisation_dropdown\n",
    "organisation_dropdown = widgets.Dropdown(\n",
    "    options=organisation_options,\n",
    "    description=\"Select LPA:\",\n",
    ")\n",
    "\n",
    "widgets.interact(update_dataframe, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with all endpoints? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    result_df.to_csv(\"endpoints_with_all_status.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_with_all_status.csv'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints with current/latest status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe_latest_status(organisation):\n",
    "    global new_df\n",
    "    all_endpoints=update_dataframe(organisation)\n",
    "    new_df=all_endpoints.copy()\n",
    "    new_df['maxentrydate'] = pd.to_datetime(new_df['maxentrydate'])\n",
    "    new_df['last_status'] = None\n",
    "    new_df['last_updated_date'] = None\n",
    "    new_df['date_last_status_200'] = None\n",
    "    \n",
    "    for index, row in new_df.iterrows():\n",
    "        if index < len(new_df) - 1 and (row['status']!=200 or pd.isna(row['status'])):\n",
    "            if row['endpoint_url'] == new_df.at[index + 1, 'endpoint_url']:\n",
    "                new_df.at[index, 'last_status'] = new_df.at[index + 1, 'status']\n",
    "                new_df.at[index, 'last_updated_date'] = new_df.at[index + 1, 'maxentrydate']   \n",
    "    \n",
    "    new_df.drop_duplicates(subset='endpoint_url', keep='first', inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    for index, row in new_df.iterrows():\n",
    "        if row['last_status'] is not None:\n",
    "            if row['last_status'] != 200  or row['last_status'] is None:\n",
    "                filtered_df = all_endpoints[(all_endpoints['endpoint_url'] == row['endpoint_url'] ) & (all_endpoints['status'] == 200)]\n",
    "                if not filtered_df.empty:\n",
    "                    new_df.at[index, 'date_last_status_200'] = filtered_df['maxentrydate'].values[0][:19] \n",
    "    return new_df\n",
    "\n",
    "widgets.interact(update_dataframe_latest_status, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with latest endpoints? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    new_df.to_csv(\"endpoints_with_latest_status.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_with_latest_status.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints with status NOT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe_erroring_endpoints(organisation):\n",
    "    global filtered_df\n",
    "    filtered_df=update_dataframe_latest_status(organisation)\n",
    "    filtered_df = filtered_df[filtered_df['status'] != 200] \n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    return filtered_df\n",
    "\n",
    "widgets.interact(update_dataframe_erroring_endpoints, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with erroring endpoints being collected till date? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    filtered_df.to_csv(\"endpoints_not_200.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_not_200.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 4 datasets - Endpoints with status NOT 200 - All LPA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e7985f60f540faa6636470132686ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select dataset:', options={'All 4 datasets': None, 'article 4 dire…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "def update_dataframe1(collection):\n",
    "    global df1  \n",
    "    if collection:\n",
    "        query = f\" s.collection = '{collection}'\"\n",
    "    else:\n",
    "        query = f\" s.collection IN ('article-4-direction', 'listed-building', 'tree-preservation-order', 'conservation-area','article-4-direction-area','article-4-direction-rule','listed-building-grade','listed-building-outline','listed-building-building','tree','tree-preservation-zone','tree-preservation-zone-type')\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select\n",
    "          e.endpoint_url,\n",
    "          l.status,\n",
    "          l.exception,\n",
    "          s.collection,\n",
    "          group_concat(DISTINCT sp.pipeline) as pipelines,\n",
    "          s.organisation,\n",
    "          o.name,\n",
    "          max(l.entry_date) maxentrydate,\n",
    "          max(e.entry_date) entrydate,\n",
    "          e.end_date\n",
    "        from\n",
    "          log l\n",
    "          inner join source s on l.endpoint = s.endpoint\n",
    "          inner join organisation o on o.organisation = replace(s.organisation, '-eng', '')\n",
    "          inner join endpoint e on l.endpoint = e.endpoint\n",
    "          inner join source_pipeline sp on s.source = sp.source\n",
    "        where\n",
    "           {query} \n",
    "        group by\n",
    "          l.endpoint,\n",
    "          l.status\n",
    "        order by\n",
    "          pipelines,\n",
    "          o.name,\n",
    "          maxentrydate desc\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df1 = pd.read_csv(url)\n",
    "    result_df1 = df1.copy()\n",
    "\n",
    "    df1['maxentrydate'] = pd.to_datetime(df1['maxentrydate'])\n",
    "    df1['last_status'] = None\n",
    "    df1['last_updated_date'] = None\n",
    "    df1['date_last_status_200'] = None\n",
    "    \n",
    "    for index, row in df1.iterrows():\n",
    "        if index < len(df1) - 1 and (row['status']!=200 or pd.isna(row['status'])):\n",
    "            if row['endpoint_url'] == df1.at[index + 1, 'endpoint_url']:\n",
    "                df1.at[index, 'last_status'] = df1.at[index + 1, 'status']\n",
    "                df1.at[index, 'last_updated_date'] = df1.at[index + 1, 'maxentrydate']   \n",
    "    \n",
    "    df1.drop_duplicates(subset='endpoint_url', keep='first', inplace=True)\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    for index, row in df1.iterrows():\n",
    "        if row['last_status'] is not None:\n",
    "                if row['last_status'] != 200  or row['last_status'] is None:\n",
    "                    filtered_df = result_df1[(result_df1['endpoint_url'] == row['endpoint_url'] ) & (result_df1['status'] == 200)]\n",
    "                    if not filtered_df.empty:\n",
    "                        df1.at[index, 'date_last_status_200'] = filtered_df['maxentrydate'].values[0][:19]\n",
    "    df1 = df1[df1['status'] != 200]\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df1\n",
    "\n",
    "\n",
    "global collection_options    \n",
    "collection_options = {\n",
    "    \"All 4 datasets\":None,\"article 4 direction\": \"article-4-direction\",\"conservation area\": \"conservation-area\",\"listed building\": \"listed-building\",\n",
    "    \"tree preservation order\": \"tree-preservation-order\"\n",
    "    \n",
    "}\n",
    "global organisation_dropdown\n",
    "collection_dropdown = widgets.Dropdown(\n",
    "    options=collection_options,\n",
    "    description=\"Select dataset:\",\n",
    ")\n",
    "\n",
    "widgets.interact(update_dataframe1, collection=collection_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table with erroring endpoints being collected till date? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    df1.to_csv(\"endpoints_not_200_first_4_datasets.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoints_not_200_first_4_datasets.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the Number of Distinct Non-ended Endpoints\n",
    "\n",
    "This SQL query will **count** the total number of **distinct** and **active** endpoint urls for each collection dataset, for each organisation, and will only return rows with counts greater than 1. Datasets with more than one endpoint url can then be further investigated, so see whether these additional endpoint urls are disfunctional or no longer needed.\n",
    "\n",
    "**N.B:**\n",
    "- Whether the endpoint_url is active it determined by the presence of an end_date value.\n",
    "- Currently the LPA list isn't exhaustive and may need to be added to (defined above) but \"all LPAs\" can be selected.\n",
    "- The BFL dataset is not included in this query because of how large the BFL dataset is.\n",
    "- The number of non-ended endpoints where `status != 200` can also be counted here but it pushes the datasette query limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_aggregate(organisation):\n",
    "    global endpoint_aggregate  \n",
    "    if organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            s.organisation AS organisation,\n",
    "            s.collection AS collection,\n",
    "            sp.pipeline AS pipeline,\n",
    "            COUNT ( \n",
    "                DISTINCT \n",
    "                    CASE \n",
    "                        WHEN e.end_date = \"\" THEN e.endpoint_url ELSE NULL END\n",
    "                )\n",
    "                AS non_ended_endpoints,\n",
    "            COUNT ( \n",
    "                DISTINCT \n",
    "                    CASE \n",
    "                        WHEN e.end_date != \"\" THEN e.endpoint_url ELSE NULL END\n",
    "                )\n",
    "                AS ended_endpoints\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          LEFT JOIN source_pipeline sp ON s.source = sp.source\n",
    "        WHERE\n",
    "           ({query})\n",
    "           AND (NOT collection=\"brownfield-land\")\n",
    "        GROUP BY\n",
    "            s.organisation,\n",
    "            s.collection,\n",
    "            sp.pipeline\n",
    "        HAVING\n",
    "            non_ended_endpoints > 1\n",
    "        ORDER BY\n",
    "            non_ended_endpoints DESC\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    endpoint_aggregate = pd.read_csv(url)\n",
    "    return endpoint_aggregate\n",
    "\n",
    "widgets.interact(get_endpoint_aggregate, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    endpoint_aggregate.to_csv(\"endpoint_aggregate.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'endpoint_aggregate.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status and end_date of **all** Distinct Endpoints\n",
    "\n",
    "This cell should retrieve **all** distinct **non BFL** endpoints, grouped by collection and pipeline **(not just the endpoints counted above)**. The endpoint entry date, end date, most recent log date and most recent status for each endpoint are retrieved to help deduce whether the endpoint is no longer functional and if it has been manually listed as an old endpoint.\n",
    "\n",
    "**N.B:** \n",
    "- This is queried mainly for a merge later on but may be useful nonetheless.\n",
    "- This query pushes the datasette timeout limit, and often gives an internal server (500) error, rerun this and following cells if this is the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_logs():\n",
    "    global most_recent_logs  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT e.endpoint AS endpoint_hash, MAX(l.entry_date) AS most_recent_entry_date, l.status AS most_recent_status\n",
    "        FROM endpoint e\n",
    "        JOIN log l ON e.endpoint = l.endpoint\n",
    "        GROUP BY e.endpoint\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    most_recent_logs = pd.read_csv(url)\n",
    "    return most_recent_logs\n",
    "\n",
    "def get_distinct_endpoints_status_and_end_dates(organisation):\n",
    "    global distinct_endpoints_status_and_end_dates_df  \n",
    "    if organisation:\n",
    "        query = f\" s.organisation = '{organisation}'\"\n",
    "    else:\n",
    "        query = f\" s.organisation LIKE '%'\"\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            s.organisation AS organisation,\n",
    "            s.collection AS collection,\n",
    "            sp.pipeline AS pipeline,\n",
    "            e.endpoint_url AS endpoint,\n",
    "            e.endpoint AS endpoint_hash,\n",
    "            e.entry_date,\n",
    "            s.end_date AS end_date\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          LEFT JOIN source_pipeline sp ON s.source = sp.source\n",
    "        WHERE\n",
    "           ({query})\n",
    "           AND (NOT collection=\"brownfield-land\")\n",
    "        GROUP BY\n",
    "            s.collection,\n",
    "            sp.pipeline,\n",
    "            e.endpoint\n",
    "        ORDER BY\n",
    "            s.collection,\n",
    "            sp.pipeline\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    distinct_endpoints_status_and_end_dates_df = pd.read_csv(url)\n",
    "    return distinct_endpoints_status_and_end_dates_df\n",
    "\n",
    "def combine_dataframes(organisation):\n",
    "    global distinct_endpoints_status_and_end_dates_with_status_df\n",
    "    left_df = get_distinct_endpoints_status_and_end_dates(organisation)\n",
    "    right_df = get_most_recent_logs()\n",
    "    distinct_endpoints_status_and_end_dates_with_status_df = pd.merge(left_df, right_df, left_on=['endpoint_hash'], right_on=['endpoint_hash'])\n",
    "    return distinct_endpoints_status_and_end_dates_with_status_df\n",
    "\n",
    "widgets.interact(combine_dataframes, organisation=organisation_dropdown)\n",
    "initial_organisation = organisation_dropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    distinct_endpoints_status_and_end_dates_with_status_df.to_csv(\"distinct_endpoints_status_and_end_dates_with_status_df.csv\", index=False)\n",
    "    print(\"Query result downloaded as 'distinct_endpoints_status_and_end_dates_with_status_df.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Datasets with Multiple Endpoint Urls\n",
    "The `endpoint_aggregate` dataframe counts the number of active endpoints for each organisation dataset, below this is merged with the `distinct_endpoints_status_and_end_dates_with_status_df` to list all suspect endpoints in more detail, rather than their aggregates.\n",
    "\n",
    "**N.B:** To function, the following cell requires the \"Counting the Number of Distinct Active Endpoints\" and \"Investigating All Endpoints\" cells to have been run successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A One-to-many merge\n",
    "df = pd.merge(distinct_endpoints_status_and_end_dates_with_status_df, endpoint_aggregate, left_on=['organisation', 'collection', 'pipeline'], right_on=['organisation', 'collection', 'pipeline'])\n",
    "\n",
    "#Removing endpoint rows which had no match with the endpoint_aggregate dataframe\n",
    "df = df.dropna(subset=[\"non_ended_endpoints\"]).drop([\"non_ended_endpoints\", \"ended_endpoints\"], axis=1)\n",
    "\n",
    "# Dropping rows with an end_date value (non-active), sorting and reseting the index field\n",
    "df = df.drop(df[df.end_date.notnull()].index, axis=0).drop([\"end_date\"], axis=1).sort_values(by = [\"organisation\", \"collection\", \"pipeline\"]).reset_index(drop=True)\n",
    "\n",
    "possible_duplicate_endpoints = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider when these endpoints were last successfully accessed\n",
    "Below we add the field describing the timestamp the endpoint was last successfully accessed (gave a 200-like response) to the `possible_duplicate_endpoints` dataframe generated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_when_successfully_accessed():\n",
    "    global last_successfully_accessed  \n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        SELECT\n",
    "            MAX(l.entry_date) AS last_200_LIKE_response_timestamp,\n",
    "            e.endpoint AS endpoint_hash\n",
    "        FROM\n",
    "          log l\n",
    "          INNER JOIN endpoint e ON l.endpoint = e.endpoint\n",
    "          INNER JOIN source s ON l.endpoint = s.endpoint\n",
    "        WHERE\n",
    "            (NOT collection=\"brownfield-land\")\n",
    "            AND l.status LIKE \"2%\"\n",
    "        GROUP BY\n",
    "            e.endpoint_url\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "    })\n",
    "\n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    last_successfully_accessed = pd.read_csv(url)\n",
    "    return last_successfully_accessed\n",
    "\n",
    "\n",
    "\n",
    "possible_duplicate_endpoints_last_200 = pd.merge(possible_duplicate_endpoints, get_when_successfully_accessed(), left_on=['endpoint_hash'], right_on=['endpoint_hash'])\n",
    "\n",
    "possible_duplicate_endpoints_last_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    possible_duplicate_endpoints_last_200.to_csv(\"possible_duplicate_endpoints_last_200.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Stale Endpoints\n",
    "Endpoints which were last successfully accessed over **5 days** ago can be assumed faulty, or effectively ended and can be recommended for removal by the standard process https://docs.google.com/document/d/1Xm1frOBY-J4mLfigXuFdeq976cQGghhnt0gbmZleAyc/edit#heading=h.y6u78drjip12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab datetime of 5 days ago relative to current date\n",
    "five_days_ago_timestamp = pd.to_datetime('today').normalize().tz_localize(\"Europe/London\" ,ambiguous=True) - pd.Timedelta(days=5)\n",
    "\n",
    "# Assign new df variable and convert most_recent_status to string for later string comparison\n",
    "df = possible_duplicate_endpoints_last_200\n",
    "df[\"most_recent_status\"] = df[\"most_recent_status\"].astype(str)\n",
    "\n",
    "# Convert last_200_response_timestamp field to datetime\n",
    "df[\"last_200_LIKE_response_timestamp\"] = pd.to_datetime(df[\"last_200_LIKE_response_timestamp\"])\n",
    "\n",
    "# Grab non-200 data by comparison\n",
    "df = df[~df[\"most_recent_status\"].str.contains(\"2\")]\n",
    "\n",
    "# Filter for entries which only returned 200 over 5 days ago\n",
    "df = df[df['last_200_LIKE_response_timestamp'] < five_days_ago_timestamp].reset_index(drop=True)\n",
    "\n",
    "# Convert most_recent_status back to float\n",
    "df[\"most_recent_status\"] = df[\"most_recent_status\"].astype(float)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Endpoints then Converting to Lists\n",
    "The `possible_duplicate_endpoints` dataframe above is grouped by organisation, collection and pipeline, all unique endpoint urls are then placed into a list so that they can be easily looped through. This is so that the csv contents of each endpoint can be compared in future, and then decide on which endpoints to keep.\n",
    "\n",
    "**N.B:** This currently includes the endpoints previously highlighted as stale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "df = possible_duplicate_endpoints.drop([\"most_recent_status\"], axis = 1)\n",
    "\n",
    "# Group by organisation, collection and pipeline, append the aggregated endpoint urls into a list then reset the index\n",
    "df = df.groupby([\"organisation\",\"collection\",\"pipeline\"])[\"endpoint\"].apply(list).reset_index()\n",
    "possible_duplicate_endpoints_aggregate = df\n",
    "possible_duplicate_endpoints_aggregate\n",
    "\n",
    "# This can be looped through programmatically to check the contents of the remaining endpoints to be eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = input(\"Do you want to download the possible_duplicate_endpoints_aggregate table? (yes/no): \")\n",
    "\n",
    "if download.lower() == \"yes\":\n",
    "    possible_duplicate_endpoints_aggregate.to_csv(\"possible_duplicate_endpoints_aggregate.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
