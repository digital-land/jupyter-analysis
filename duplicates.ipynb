{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses a local copy of the digital-land datasette to generate a csv file with the info needed to retire the duplicated/stale brownfield endpoints.\n",
    "\n",
    "Download the digital land SQLite3 file from https://datasette.planning.data.gov.uk/digital-land. You can then set the path to it per the next section. Note how the paths of the files show how to do it from WSL2 to your windows host :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile =r'/mnt/c/Users/MarkSmith/Downloads/digital-land.sqlite3'\n",
    "csvfile = r'/mnt/c/Users/MarkSmith/Downloads/duplicates.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Create the connection\n",
    "cnx = sqlite3.connect(dbfile)\n",
    "\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view in your SQLite file that lists organisations and datasets where the dataset has more than a single source. I could have done this as an inner join but divide and conquer is how I prefer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"drop view duplicates\")\n",
    "\n",
    "duplicates_view_sql = \"\"\"\n",
    "create view duplicates as \n",
    "select org.name || '_' || ds.name as dupkey,  ds.name as dataset,  count( res.resource) as count \n",
    "from resource res\n",
    "inner join resource_organisation ro on ro.resource = res.resource\n",
    "inner join organisation org on org.organisation = ro.organisation\n",
    "inner join resource_dataset rd on rd.resource = res.resource\n",
    "inner join dataset ds on ds.dataset = rd.dataset\n",
    "where res.end_date = ''\n",
    "group by 1\n",
    "having count > 1\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(duplicates_view_sql)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM duplicates order by count desc\", cnx)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bydataset = df.groupby('dataset')['count'].sum()\n",
    "print(bydataset)\n",
    "bydataset.plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build out the endpoints that are candidates for retiring. Note how we use an aggregate of org name and dataset to identify each row. This assumes for each org, a dataset should be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select org.name || '_' || ds.name as thiskey, org.name as organization, ds.name as dataset, \n",
    "res.start_date as res_start_date, res.resource, ep.endpoint, ep.endpoint_url, ep.start_date as ep_start_date, ep.end_date\n",
    "\n",
    "from resource res\n",
    "inner join resource_organisation ro on ro.resource = res.resource\n",
    "inner join organisation org on org.organisation = ro.organisation\n",
    "inner join resource_dataset rd on rd.resource = res.resource\n",
    "inner join dataset ds on ds.dataset = rd.dataset\n",
    "inner join resource_endpoint re on re.resource = res.resource\n",
    "inner join endpoint ep on ep.endpoint = re.endpoint\n",
    "\n",
    "where res.end_date = ''\n",
    "and thiskey in (select dupkey from duplicates)\n",
    "order by 2,3,4, ep.start_date \n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(sql, cnx)\n",
    "\n",
    "df.to_csv(csvfile)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * from endpoint ep\n",
    "where ep.start_date = '' and ep.end_date = ''\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(sql, cnx)\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
