{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6037f3c4",
   "metadata": {},
   "source": [
    "# Assign entity-organisation ranges\n",
    "**Author**:  Greg Slater <br>\n",
    "**Date**:  11th February 2025 <br>\n",
    "**Dataset Scope**: ODP datasets <br>\n",
    "**Report Type**: Tool / analysis aid <br>\n",
    "**Purpose**: Some scripts which use the `lookup.csv` file to generate the ranges required in the `entity-organisation.csv` config file. \"Assign Ranges\" section will generate the file required, while the \"Check ranges\" section does some standard QA checks of the outputs - these should be used to sense-check the output before it's used. Likely will require some iteration between checking and fixing the lookup file, then re-running the file generation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b549161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "\n",
    "td = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "data_dir = \"../../data/\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "out_dir = \"../../data/org-entity-ranges/\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea41e77",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8424300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get org data from datasette - flag LPAs (note - Purbeck and North Dorset are incorrectly missing LPA codes, so manually add in to flag)\n",
    "\n",
    "def get_all_organisations():\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"sql\": f\"\"\"\n",
    "        select entity as organisation_entity, name as org_name, organisation, dataset as org_type, end_date, \n",
    "        local_planning_authority as LPACD, local_authority_district,\n",
    "        case when local_planning_authority != \"\" then 1 else 0 end as lpa_flag\n",
    "        from organisation\n",
    "        where name != \"Waveney District Council\"\n",
    "        and end_date = \"\"\n",
    "        \"\"\",\n",
    "        \"_size\": \"max\"\n",
    "        })\n",
    "    url = f\"https://datasette.planning.data.gov.uk/digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url, dtype = str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849522e4",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fee779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset to assign organisation-entity ranges for\n",
    "dataset = \"brownfield-land\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lookup = get_all_organisations()\n",
    "print(len(org_lookup))\n",
    "\n",
    "# lookup file\n",
    "lookup_df= pd.read_csv(f\"https://raw.githubusercontent.com/digital-land/config/refs/heads/main/pipeline/{dataset}/lookup.csv\")\n",
    "print(len(lookup_df))\n",
    "\n",
    "# old-entity file\n",
    "old_ent_df= pd.read_csv(f\"https://raw.githubusercontent.com/digital-land/config/refs/heads/main/pipeline/{dataset}/old-entity.csv\")\n",
    "print(len(old_ent_df))\n",
    "\n",
    "lookup_df = lookup_df.merge(\n",
    "    org_lookup[[\"organisation\", \"org_name\", \"lpa_flag\"]],\n",
    "    how = \"left\", \n",
    "    on = \"organisation\"\n",
    ")\n",
    "print(len(lookup_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0764bc",
   "metadata": {},
   "source": [
    "## Assign ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117384f0-36e7-4d6a-a2e3-6e0548f7d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to assign ranges for LPAs (as they're the authoritative source)\n",
    "# and for entities which haven't been retired or redirected (as we don't want to give ranges for orgs which were wrongly assigned from bad OrganisationURI values)\n",
    "lookup_lpa = lookup_df[\n",
    "    (lookup_df[\"lpa_flag\"] == \"1\") &\n",
    "    (~lookup_df[\"entity\"].isin(old_ent_df[\"old-entity\"]))\n",
    "].copy()\n",
    "\n",
    "print(f\"len of lookup_lpa: {len(lookup_lpa)}\")\n",
    "\n",
    "lookup_lpa = lookup_lpa.dropna(subset=['entity', 'organisation'])\n",
    "\n",
    "lookup_lpa['entity'] = pd.to_numeric(lookup_lpa['entity'])\n",
    "lookup_lpa = lookup_lpa.sort_values(by=['prefix', 'entity']).reset_index(drop=True)\n",
    "\n",
    "# increment_id tracks when the organisation or non-consecutive entity changes\n",
    "lookup_lpa['increment'] = (lookup_lpa['organisation'] != lookup_lpa['organisation'].shift(1)) | \\\n",
    "                           (lookup_lpa['prefix'] != lookup_lpa['prefix'].shift(1)) | \\\n",
    "                           ((lookup_lpa['entity'] - lookup_lpa['entity'].shift(1)) != 1)\n",
    "\n",
    "\n",
    "# Cumulatively sum the 'increment_id' to get the unique range IDs\n",
    "lookup_lpa['increment_id'] = lookup_lpa['increment'].cumsum()\n",
    "\n",
    "lookup_lpa.to_csv(os.path.join(out_dir, f\"{dataset}_lookup_lpa_incremented.csv\"), index = False)\n",
    "\n",
    "# Group by organisation and the 'increment_id' to calculate min and max entities for each range\n",
    "entity_ranges = lookup_lpa.groupby(['prefix','organisation', 'increment_id']).agg(\n",
    "    entity_min=('entity', 'min'),\n",
    "    entity_max=('entity', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# add field for the range size\n",
    "entity_ranges[\"entity_range\"] = entity_ranges[\"entity_max\"] - entity_ranges[\"entity_min\"]\n",
    "\n",
    "print(f\"count of ranges: {len(entity_ranges)}\")\n",
    "entity_ranges.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_lpa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23edd69-7e30-4b34-ab11-794ca33f0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entity-organisation.csv\n",
    "entity_organisation = entity_ranges.copy()\n",
    "entity_organisation.drop([\"increment_id\", \"entity_range\"], axis=1, inplace=True)\n",
    "entity_organisation.rename(columns={\"prefix\":\"dataset\", \"entity_min\": \"entity-minimum\", \"entity_max\": \"entity-maximum\"}, inplace=True)\n",
    "\n",
    "entity_organisation.to_csv(os.path.join(out_dir, f\"{dataset}_entity-organisation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d60599",
   "metadata": {},
   "source": [
    "## Check ranges\n",
    "### Entities in multiple ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5256b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any entities in multiple ranges\n",
    "\n",
    "# df = pd.read_csv('entity-organisation.csv')  \n",
    "\n",
    "# need to test conservation-area and conservation-area-document ranges separately\n",
    "er_test = entity_ranges[entity_ranges[\"prefix\"] == dataset].copy()\n",
    "\n",
    "# entity range for chunk\n",
    "e_range = np.arange(\n",
    "    er_test[\"entity_min\"].min(), \n",
    "    er_test[\"entity_max\"].max()\n",
    "    )\n",
    "\n",
    "print(f\"checking ranges for {len(e_range)} entities\")\n",
    "\n",
    "# check how many ranges in range table each entity has\n",
    "range_checks = [len(er_test[(er_test[\"entity_min\"] <= e) & (er_test[\"entity_max\"] >= e)]) for e in e_range]\n",
    "\n",
    "# df for results\n",
    "check_df = pd.DataFrame(\n",
    "    {\n",
    "        \"entity\" : e_range,\n",
    "        \"n_ranges\" : range_checks\n",
    "    }\n",
    ")\n",
    "\n",
    "# test if any with > 1 range\n",
    "entity_dupes = check_df[check_df[\"n_ranges\"] > 1]\n",
    "print(f\"Found {len(entity_dupes)} entities which appear more than once in lookup.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feab2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "833ea6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_entity_dupes = lookup_lpa[lookup_lpa[\"entity\"].isin(entity_dupes[\"entity\"])].sort_values(\"entity\")\n",
    "lookup_entity_dupes.to_csv(os.path.join(out_dir, f\"{dataset}_lookup_entity_dupes.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c4cf0",
   "metadata": {},
   "source": [
    "### Short entity-org ranges (a sign of funny entities which may need manual fixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ce23ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ranges = entity_ranges[entity_ranges[\"entity_range\"] <= 5]\n",
    "\n",
    "lookup_range_flagged = lookup_df.merge(\n",
    "    short_ranges[[\"entity_min\", \"entity_range\"]],\n",
    "    how = \"left\",\n",
    "    left_on = \"entity\",\n",
    "    right_on = \"entity_min\"\n",
    ")\n",
    "\n",
    "lookup_range_flagged[[\n",
    "    'prefix', 'resource', 'endpoint', 'entry-number', 'organisation',\n",
    "    'reference', 'entity', 'entry-date', 'start-date', 'end-date', 'entity_range']\n",
    "    ].to_csv(\n",
    "        os.path.join(out_dir, f\"{dataset}_lookup_short-ranges-flagged.csv\"), \n",
    "        index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdp_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
