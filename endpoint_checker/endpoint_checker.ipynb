{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f33af68-507b-4831-9186-46067200bb87",
   "metadata": {},
   "source": [
    "## Check an endpoint\n",
    "This notebook aims to run the pipeline on a given endpoint to check to see if it will be successful. This includes collecting, pipeline and datset stages. It aims to also highlight useful information as a summary as to whether the endpoint would be successful on our platform. it will download all relevant data to do this and hence might be disk intensive. You'll need to provide the following information:\n",
    "- collection - this is the collection that the dataset belongs to, this can be extracted from the specification but for this notebook we ask to provide it incase you want to test the pipeline on something which isn't being included in the main site right now\n",
    "- dataset - this is the dataset that the endpoint is meant to provide data for, technically this can be multiple datasets but this this use case it should just be one. It is also the name of the pipeline that is ran on the individual resources that are downloaded from the endpoint. the terms dataset/pipline are often the same\n",
    "- organisation - the organisation identifier to be used for the endpoint\n",
    "- endpoint url - the actual url needed for the endpoint\n",
    "- plugin - often we use plugins to download the data this is only needed for specific endpoints\n",
    "\n",
    "If you are seeing errors regarding digital-land, then try \n",
    "\n",
    "`pip install -e git+https://github.com/digital-land/pipeline.git#egg=digital-land`\n",
    "\n",
    "And restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9959de-69a3-4199-86ad-ecf5fa3150c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from functions import run_endpoint_workflow, missing_columns\n",
    "from sqlite_query_functions import DatasetSqlite\n",
    "from convert_functions import convert_resource\n",
    "from digital_land.collection import Collection\n",
    "from data_file import get_duplicates_between_orgs\n",
    "from download_data import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f3f438-0b45-485e-b1f2-6e9b4bb01729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extend these lists as/when you need to add other collections\n",
    "\n",
    "# collection_name = 'article-4-direction-collection'\n",
    "# collection_name = 'brownfield-land-collection'\n",
    "# collection_name = 'conservation-area-collection'\n",
    "# collection_name = 'flood-risk-zone-collection'\n",
    "collection_name = 'listed-building-collection'\n",
    "# collection_name = 'tree-preservation-order-collection'\n",
    "\n",
    "# dataset = 'article-4-direction'\n",
    "# dataset = 'article-4-direction-area'\n",
    "# dataset = 'brownfield-land'\n",
    "# dataset = 'conservation-area'\n",
    "# dataset = 'conservation-area-document'\n",
    "# dataset = 'flood-risk-zone'\n",
    "dataset = 'listed-building-outline'\n",
    "# dataset = 'tree'\n",
    "# dataset = 'tree-preservation-order'\n",
    "# dataset = 'tree-preservation-zone'\n",
    "\n",
    "# additional_column_mappings=None\n",
    "# additional_concats=None\n",
    "\n",
    "# plugin = None\n",
    "# plugin = 'arcgis'\n",
    "# plugin = 'wfs'\n",
    " \n",
    "# additional_column_mappings=None\n",
    "\n",
    "# additional_concats=None\n",
    "\n",
    "# EXAMPLE / TEST DATA HERE\n",
    "\n",
    "# collection_name = 'brownfield-land-collection'\n",
    "# dataset = 'brownfield-land'\n",
    "organisation = 'local-authority-eng:SAL'\n",
    "endpoint_url = 'https://www.stalbans.gov.uk/sites/default/files/documents/publications/planning-building-control/Agile/Listed_Buildings_Dataset.json'\n",
    "documentation_url = \"https://opendata.camden.gov.uk/Environment/Brownfield-Land-Register/izhm-jdrx/about_data\"\n",
    "start_date=\"2024-01-31\"\n",
    "plugin = ''\n",
    "reference_column = \"\"\n",
    "\n",
    "additional_column_mappings=None\n",
    "additional_concats=None\n",
    "\n",
    "# generic data_dir setting\n",
    "data_dir = '../data/endpoint_checker'\n",
    "\n",
    "# example playing with additional confiigs\n",
    "# additional_concats = [{\n",
    "#     'dataset':'tree-preservation-zone',\n",
    "#     'endpoint':'de1eb90a8b037292ef8ae14bfabd1184847ef99b7c6296bb6e75379e6c1f9572',\n",
    "#     'resource':'e6b0ccaf9b50a7f57a543484fd291dbe43f52a6231b681c3a7cc5e35a6aba254',\n",
    "#     'field':'reference',\n",
    "#     'fields':'REFVAL;LABEL',\n",
    "#     'separator':'/'\n",
    "# }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e38144f-1c71-4283-840e-bddffd3ea776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Hello\n",
      "../data/endpoint_checker/collection\n",
      "Hello again\n",
      "[{'resource': 'e28b118b256247283cca33d03a2cfddbc89c01478ae05bf52020e963687a2b18', 'bytes': '', 'endpoints': '4ecb586385c9fbdaec5df01e81f63ea222d2a7e79c9209d4253e7ffc6fca20f0', 'organisations': 'local-authority-eng:SAL', 'datasets': 'listed-building-outline', 'start-date': '2024-02-21', 'end-date': ''}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:entry-number: unexpected fieldname in lookups.csv\n"
     ]
    }
   ],
   "source": [
    "run_endpoint_workflow(\n",
    "    collection_name,\n",
    "    dataset,\n",
    "    organisation,\n",
    "    endpoint_url,\n",
    "    plugin,\n",
    "    data_dir,\n",
    "    additional_col_mappings=additional_column_mappings,\n",
    "    additional_concats=additional_concats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4ef1f-9456-4e1f-b654-cbb56d5e3cf2",
   "metadata": {},
   "source": [
    "### Collection log summaries\n",
    "\n",
    "We need to establish if a resource was downloaded from the endpoint and whether there were any issues during the collection process. Examine the output of the below. There should be one log for the attempt made at downloading from the endpoint. If status code is 200 then the resource was downloaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62286dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize the collection from the specified directory\n",
    "    collection = Collection(os.path.join(data_dir,'collection'))\n",
    "    \n",
    "    # Load the collection\n",
    "    collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "    \n",
    "    # Access the records of a resource within the collection\n",
    "    collection.resource.records\n",
    "    \n",
    "    print(\"Download has succeeded.\")\n",
    "except Exception as e:\n",
    "    # If anything goes wrong, print the error and status code\n",
    "    print(f\"Download failed with error: {e}\")\n",
    "    if hasattr(e, 'status'):\n",
    "        print(f\"Status code: {e.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad17395f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bytes</th>\n",
       "      <th>content-type</th>\n",
       "      <th>elapsed</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>resource</th>\n",
       "      <th>status</th>\n",
       "      <th>entry-date</th>\n",
       "      <th>start-date</th>\n",
       "      <th>end-date</th>\n",
       "      <th>exception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.327</td>\n",
       "      <td>4ecb586385c9fbdaec5df01e81f63ea222d2a7e79c9209...</td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>200</td>\n",
       "      <td>2024-02-21T12:41:55.807203</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bytes content-type elapsed  \\\n",
       "0                      0.327   \n",
       "\n",
       "                                            endpoint  \\\n",
       "0  4ecb586385c9fbdaec5df01e81f63ea222d2a7e79c9209...   \n",
       "\n",
       "                                            resource status  \\\n",
       "0  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...    200   \n",
       "\n",
       "                   entry-date start-date end-date exception  \n",
       "0  2024-02-21T12:41:55.807203                                "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = collection.log.entries\n",
    "logs = pd.DataFrame.from_records(logs)\n",
    "logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f842a08",
   "metadata": {},
   "source": [
    "### Check unnassigned entiities\n",
    "Detect and assign entity numbers where entries are currently unnassigned. Examine the list below to see what (if any) entities have been assigned. if you were to include these in an actual pipeline you would need to update the configuration lookup.csv with these values. It's worth checking they are sensible before this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b9a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839 unassigned entities\n",
      "\n",
      "                organisation                   prefix  reference\n",
      "0    local-authority-eng:SAL  listed-building-outline    1295678\n",
      "1    local-authority-eng:SAL  listed-building-outline    1175422\n",
      "2    local-authority-eng:SAL  listed-building-outline    1102888\n",
      "3    local-authority-eng:SAL  listed-building-outline    1295340\n",
      "4    local-authority-eng:SAL  listed-building-outline    1347243\n",
      "..                       ...                      ...        ...\n",
      "834  local-authority-eng:SAL  listed-building-outline    1347202\n",
      "835  local-authority-eng:SAL  listed-building-outline    1102969\n",
      "836  local-authority-eng:SAL  listed-building-outline    1174580\n",
      "837  local-authority-eng:SAL  listed-building-outline    1347203\n",
      "838  local-authority-eng:SAL  listed-building-outline    1459132\n",
      "\n",
      "[839 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "unassigned_entries = pd.read_csv(os.path.join(data_dir,'var','cache','unassigned-entries.csv'))\n",
    "if len(unassigned_entries) == 0:\n",
    "    print('No additional entity numbers required')\n",
    "else:\n",
    "    print(F\"{len(unassigned_entries)} unassigned entities\\n\")\n",
    "    print(unassigned_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313b0fa-151d-4c52-bb46-f6e733d2369e",
   "metadata": {},
   "source": [
    "### Check logs collated from the pipeline process\n",
    "We need to read the logs and examine to see if the data points were all read in correctly. This uses the sqlite database to do so with some custom queries. You could directly examine the csvs if the pipeline fails.\n",
    "\n",
    "First, check the column mappings to see what columns the pipeline automatically mapped. If this is empty or has missing values, then it's likely to be the reason data isn't appearing at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c01973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns for dataset 'listed-building-outline': address, point, entity, organisation, wikidata, document-url, description, wikipedia, address-text, prefix, documentation-url.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_date</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>field</th>\n",
       "      <th>dataset</th>\n",
       "      <th>start_date</th>\n",
       "      <th>resource</th>\n",
       "      <th>column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>geometry</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>WKT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>end-date</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>end_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>entry-date</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>entry_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>listed-building</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>listed_building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>listed-building-grade</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>listed_building_grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>name</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>notes</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>reference</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>2024-02-21T12:42:16Z</td>\n",
       "      <td>start-date</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>e28b118b256247283cca33d03a2cfddbc89c01478ae05b...</td>\n",
       "      <td>start_date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  end_date            entry_date                  field  \\\n",
       "0           2024-02-21T12:42:16Z               geometry   \n",
       "1           2024-02-21T12:42:16Z               end-date   \n",
       "2           2024-02-21T12:42:16Z             entry-date   \n",
       "3           2024-02-21T12:42:16Z        listed-building   \n",
       "4           2024-02-21T12:42:16Z  listed-building-grade   \n",
       "5           2024-02-21T12:42:16Z                   name   \n",
       "6           2024-02-21T12:42:16Z                  notes   \n",
       "7           2024-02-21T12:42:16Z              reference   \n",
       "8           2024-02-21T12:42:16Z             start-date   \n",
       "\n",
       "                   dataset start_date  \\\n",
       "0  listed-building-outline              \n",
       "1  listed-building-outline              \n",
       "2  listed-building-outline              \n",
       "3  listed-building-outline              \n",
       "4  listed-building-outline              \n",
       "5  listed-building-outline              \n",
       "6  listed-building-outline              \n",
       "7  listed-building-outline              \n",
       "8  listed-building-outline              \n",
       "\n",
       "                                            resource                 column  \n",
       "0  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...                    WKT  \n",
       "1  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...               end_date  \n",
       "2  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...             entry_date  \n",
       "3  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...        listed_building  \n",
       "4  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...  listed_building_grade  \n",
       "5  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...                   name  \n",
       "6  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...                  notes  \n",
       "7  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...              reference  \n",
       "8  e28b118b256247283cca33d03a2cfddbc89c01478ae05b...             start_date  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_column_mappings()\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "expected_columns = df.groupby('dataset')['field'].apply(list).to_dict()\n",
    "missing_columns(results, dataset, expected_columns)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c9e67-e108-424e-a5cb-6aab2cbde912",
   "metadata": {},
   "source": [
    "### Issues Logs\n",
    "\n",
    "Lists all of the issues/warnings, their respective severities and whose responsibility it is to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4848627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_type</th>\n",
       "      <th>count</th>\n",
       "      <th>responsibility</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default-value</td>\n",
       "      <td>4637</td>\n",
       "      <td>internal</td>\n",
       "      <td>info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>invalid geometry - fixed</td>\n",
       "      <td>3</td>\n",
       "      <td>external</td>\n",
       "      <td>warning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 issue_type  count responsibility severity\n",
       "0             default-value   4637       internal     info\n",
       "1  invalid geometry - fixed      3       external  warning"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "dataset_issues = dataset_db.get_issues_by_type()\n",
    "\n",
    "def get_issue_types_with_responsibility_and_severity():\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "    \"sql\": f\"\"\"\n",
    "    select issue_type, responsibility, severity\n",
    "    from issue_type\n",
    "    \"\"\",\n",
    "    \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "    \n",
    "issue_reference = get_issue_types_with_responsibility_and_severity()\n",
    "matched_dataset_issues = dataset_issues.merge(issue_reference, on='issue_type', how='left')\n",
    "matched_dataset_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e374935",
   "metadata": {},
   "source": [
    "#### Look at a specific problem type in more detail\n",
    "\n",
    "Take the issue_type from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2555d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_date</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>entry_number</th>\n",
       "      <th>field</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>line_number</th>\n",
       "      <th>dataset</th>\n",
       "      <th>resource</th>\n",
       "      <th>start_date</th>\n",
       "      <th>value</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [end_date, entry_date, entry_number, field, issue_type, line_number, dataset, resource, start_date, value, message]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = dataset_db.get_issues()\n",
    "\n",
    "#problem = 'OSGB out of bounds of England'\n",
    "#problem = 'invalid geometry'\n",
    "# problem = 'Unexpected geom type'\n",
    "problem = 'OSGB'\n",
    "\n",
    "results.loc[(results.issue_type == problem) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f612d0-ca02-4080-b6f6-847f2b1ea22c",
   "metadata": {},
   "source": [
    "### Final dataset \n",
    "\n",
    "Shows the end result of the processing. You should see a decent number of these columns populated with data from the raw resources above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c7b0a5-3d38-4389-bf3c-413ecae98a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data contains 833 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>end_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>geojson</th>\n",
       "      <th>geometry</th>\n",
       "      <th>json</th>\n",
       "      <th>name</th>\n",
       "      <th>organisation_entity</th>\n",
       "      <th>point</th>\n",
       "      <th>prefix</th>\n",
       "      <th>reference</th>\n",
       "      <th>start_date</th>\n",
       "      <th>typology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42125879</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.407211 51.827450,-0.407261 ...</td>\n",
       "      <td>{\"listed-building\": \"1295678\", \"listed-buildin...</td>\n",
       "      <td>Turners Hall House &amp; Rear Outbuilding, Annable...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.407362 51.827393)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1295678</td>\n",
       "      <td>1953-10-19</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42125880</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.363897 51.761415,-0.363871 ...</td>\n",
       "      <td>{\"listed-building\": \"1175422\", \"listed-buildin...</td>\n",
       "      <td>The Pre Hotel, Redbourn Road, St Albans</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.363904 51.761558)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1175422</td>\n",
       "      <td>1984-09-05</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42125881</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.411670 51.743107,-0.411589 ...</td>\n",
       "      <td>{\"listed-building\": \"1102888\", \"listed-buildin...</td>\n",
       "      <td>West range of outbuildings, inc former pigsty,...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.411716 51.743165)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1102888</td>\n",
       "      <td>1978-03-06</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42125882</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.411590 51.742953,-0.411760 ...</td>\n",
       "      <td>{\"listed-building\": \"1295340\", \"listed-buildin...</td>\n",
       "      <td>Corner Farmhouse, Hemel Hempstead Road, St Albans</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.411755 51.742966)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1295340</td>\n",
       "      <td>1978-03-06</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42125883</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.411224 51.743066,-0.411323 ...</td>\n",
       "      <td>{\"listed-building\": \"1347243\", \"listed-buildin...</td>\n",
       "      <td>L-plan range of outbuildings at Corner Farm, H...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.411281 51.742879)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1347243</td>\n",
       "      <td>1978-03-06</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42126713</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.404076 51.827962,-0.404085 ...</td>\n",
       "      <td>{\"listed-building\": \"1347202\", \"listed-buildin...</td>\n",
       "      <td>Well House at Annables Manor, Annables Lane, K...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.404152 51.827978)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1347202</td>\n",
       "      <td>1984-09-27</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42126714</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.404347 51.828060,-0.404506 ...</td>\n",
       "      <td>{\"listed-building\": \"1102969\", \"listed-buildin...</td>\n",
       "      <td>Annables Lodge, Annables Lane, Kinsbourne Green</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.404380 51.828048)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1102969</td>\n",
       "      <td>1984-09-27</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42126715</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.404517 51.827833,-0.404379 ...</td>\n",
       "      <td>{\"listed-building\": \"1174580\", \"listed-buildin...</td>\n",
       "      <td>Barn adjoining Annables Lodge, Annables Lane, ...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.404483 51.827935)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1174580</td>\n",
       "      <td>1986-03-25</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42126716</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.404418 51.827547,-0.404376 ...</td>\n",
       "      <td>{\"listed-building\": \"1347203\", \"listed-buildin...</td>\n",
       "      <td>Barn to SW side at Annables Farm (Annables Hou...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.404508 51.827620)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1347203</td>\n",
       "      <td>1984-09-27</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td></td>\n",
       "      <td>42126717</td>\n",
       "      <td>2018-11-21</td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((-0.358471 51.705958,-0.358723 ...</td>\n",
       "      <td>{\"listed-building\": \"1459132\", \"listed-buildin...</td>\n",
       "      <td>Air Raid Precaution Railway Control Centre, St...</td>\n",
       "      <td>278</td>\n",
       "      <td>POINT(-0.358536 51.705826)</td>\n",
       "      <td>listed-building-outline</td>\n",
       "      <td>1459132</td>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>833 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset end_date    entity  entry_date geojson  \\\n",
       "0    listed-building-outline           42125879  2004-06-01           \n",
       "1    listed-building-outline           42125880  2004-06-01           \n",
       "2    listed-building-outline           42125881  2004-06-01           \n",
       "3    listed-building-outline           42125882  2004-06-01           \n",
       "4    listed-building-outline           42125883  2004-06-01           \n",
       "..                       ...      ...       ...         ...     ...   \n",
       "828  listed-building-outline           42126713  2004-06-01           \n",
       "829  listed-building-outline           42126714  2004-06-01           \n",
       "830  listed-building-outline           42126715  2004-06-01           \n",
       "831  listed-building-outline           42126716  2004-06-01           \n",
       "832  listed-building-outline           42126717  2018-11-21           \n",
       "\n",
       "                                              geometry  \\\n",
       "0    MULTIPOLYGON (((-0.407211 51.827450,-0.407261 ...   \n",
       "1    MULTIPOLYGON (((-0.363897 51.761415,-0.363871 ...   \n",
       "2    MULTIPOLYGON (((-0.411670 51.743107,-0.411589 ...   \n",
       "3    MULTIPOLYGON (((-0.411590 51.742953,-0.411760 ...   \n",
       "4    MULTIPOLYGON (((-0.411224 51.743066,-0.411323 ...   \n",
       "..                                                 ...   \n",
       "828  MULTIPOLYGON (((-0.404076 51.827962,-0.404085 ...   \n",
       "829  MULTIPOLYGON (((-0.404347 51.828060,-0.404506 ...   \n",
       "830  MULTIPOLYGON (((-0.404517 51.827833,-0.404379 ...   \n",
       "831  MULTIPOLYGON (((-0.404418 51.827547,-0.404376 ...   \n",
       "832  MULTIPOLYGON (((-0.358471 51.705958,-0.358723 ...   \n",
       "\n",
       "                                                  json  \\\n",
       "0    {\"listed-building\": \"1295678\", \"listed-buildin...   \n",
       "1    {\"listed-building\": \"1175422\", \"listed-buildin...   \n",
       "2    {\"listed-building\": \"1102888\", \"listed-buildin...   \n",
       "3    {\"listed-building\": \"1295340\", \"listed-buildin...   \n",
       "4    {\"listed-building\": \"1347243\", \"listed-buildin...   \n",
       "..                                                 ...   \n",
       "828  {\"listed-building\": \"1347202\", \"listed-buildin...   \n",
       "829  {\"listed-building\": \"1102969\", \"listed-buildin...   \n",
       "830  {\"listed-building\": \"1174580\", \"listed-buildin...   \n",
       "831  {\"listed-building\": \"1347203\", \"listed-buildin...   \n",
       "832  {\"listed-building\": \"1459132\", \"listed-buildin...   \n",
       "\n",
       "                                                  name organisation_entity  \\\n",
       "0    Turners Hall House & Rear Outbuilding, Annable...                 278   \n",
       "1              The Pre Hotel, Redbourn Road, St Albans                 278   \n",
       "2    West range of outbuildings, inc former pigsty,...                 278   \n",
       "3    Corner Farmhouse, Hemel Hempstead Road, St Albans                 278   \n",
       "4    L-plan range of outbuildings at Corner Farm, H...                 278   \n",
       "..                                                 ...                 ...   \n",
       "828  Well House at Annables Manor, Annables Lane, K...                 278   \n",
       "829    Annables Lodge, Annables Lane, Kinsbourne Green                 278   \n",
       "830  Barn adjoining Annables Lodge, Annables Lane, ...                 278   \n",
       "831  Barn to SW side at Annables Farm (Annables Hou...                 278   \n",
       "832  Air Raid Precaution Railway Control Centre, St...                 278   \n",
       "\n",
       "                          point                   prefix reference  \\\n",
       "0    POINT(-0.407362 51.827393)  listed-building-outline   1295678   \n",
       "1    POINT(-0.363904 51.761558)  listed-building-outline   1175422   \n",
       "2    POINT(-0.411716 51.743165)  listed-building-outline   1102888   \n",
       "3    POINT(-0.411755 51.742966)  listed-building-outline   1295340   \n",
       "4    POINT(-0.411281 51.742879)  listed-building-outline   1347243   \n",
       "..                          ...                      ...       ...   \n",
       "828  POINT(-0.404152 51.827978)  listed-building-outline   1347202   \n",
       "829  POINT(-0.404380 51.828048)  listed-building-outline   1102969   \n",
       "830  POINT(-0.404483 51.827935)  listed-building-outline   1174580   \n",
       "831  POINT(-0.404508 51.827620)  listed-building-outline   1347203   \n",
       "832  POINT(-0.358536 51.705826)  listed-building-outline   1459132   \n",
       "\n",
       "     start_date   typology  \n",
       "0    1953-10-19  geography  \n",
       "1    1984-09-05  geography  \n",
       "2    1978-03-06  geography  \n",
       "3    1978-03-06  geography  \n",
       "4    1978-03-06  geography  \n",
       "..          ...        ...  \n",
       "828  1984-09-27  geography  \n",
       "829  1984-09-27  geography  \n",
       "830  1986-03-25  geography  \n",
       "831  1984-09-27  geography  \n",
       "832  2018-11-21  geography  \n",
       "\n",
       "[833 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_entities()\n",
    "\n",
    "final_length = len(results)\n",
    "\n",
    "print(\"\")\n",
    "print (F\"Final data contains {final_length} records\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34438da9-86e2-4154-9dc2-5d0e0bb7ff3e",
   "metadata": {},
   "source": [
    "### Existing Duplicate Entities between organisations\n",
    "\n",
    "This downloads a sqlite db for the current dataset  \n",
    "It compares the current endpoint entities with existing ones   \n",
    "Identifies duplicates between all organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b466d48-c431-44d2-afd6-f64125f60b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset,collection_name,f\"{data_dir}/entity_resolution\")\n",
    "dataset_path = os.path.join(f\"{data_dir}/entity_resolution\",f'{dataset}.sqlite3')\n",
    "duplicates = get_duplicates_between_orgs(dataset_path,f'../data/endpoint_checker/dataset/{dataset}.sqlite3')\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicate entities found with existing entities\")\n",
    "\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55f99f-22c7-4275-ac03-61b5a73987c3",
   "metadata": {},
   "source": [
    "### Duplicates with different entity numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870521b7-4394-485a-a77f-cf413f2da4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not duplicates.empty:\n",
    "    filtered_df = duplicates[duplicates['primary_entity'] != duplicates['secondary_entity']]\n",
    "    filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74e98b-da56-4d5f-8f86-4f26c669619e",
   "metadata": {},
   "source": [
    "To merge the duplicated entities  \n",
    "For each entity, use the corresponding primary_entity value (as the entity number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60391993-7245-4eab-a3af-dcaf4768efd8",
   "metadata": {},
   "source": [
    "### Possible Internal Duplicate Entities\n",
    "\n",
    "The below table displays duplicates in the data provided identified using the geographical info (geometry and point column).  \n",
    "Sometimes it is legit, but worth checking in the source data to make sure it passes the sniff test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90666c-22b5-4b6c-a5f3-2cf0a4427c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (results[['geometry', 'point']].apply(lambda x: x.str.strip() == '')).all().all():\n",
    "    grouped = results.groupby(['geometry', 'point'])\n",
    "    grouped_list=[]\n",
    "    for key, value in (grouped.groups).items():\n",
    "        if len(value) > 1 and key[1] !='':\n",
    "            filtered_df = results[(results['geometry'] == key[0]) & (results['point'] == key[1])]\n",
    "            grouped_list.append(filtered_df)\n",
    "\n",
    "    if len(grouped_list)>1:\n",
    "        for i in range(len(grouped_list)):\n",
    "            display(grouped_list[i])\n",
    "    else:\n",
    "        print(\"No internal duplicates found in the given endpoint\")\n",
    "else:\n",
    "    print(\"No geometry or point data in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe5bba",
   "metadata": {},
   "source": [
    "### RAW (ish) DATA\n",
    "\n",
    "This is the lightly processed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw resources\n",
    "collection = Collection(os.path.join(data_dir,'collection'))\n",
    "collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "resources = collection.resource.entries\n",
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = resources[0]['resource']\n",
    "resource_path = os.path.join(data_dir,'collection','resource',resource)\n",
    "\n",
    "print (F\"Reading raw resource from {resource_path}\")\n",
    "\n",
    "try:\n",
    "    raw_resource = pd.read_csv(resource_path)\n",
    "except (UnicodeDecodeError,TypeError,pd.errors.ParserError):\n",
    "    converted_resource_dir = os.path.join(data_dir,'var','converted_resources')\n",
    "    converted_resource_path = os.path.join(converted_resource_dir,f'{resource}.csv') \n",
    "    if not os.path.exists(converted_resource_path):\n",
    "        convert_resource(resource,resource_path,converted_resource_dir,dataset)\n",
    "    print (F\"Failed - reading from {converted_resource_path} instead.\")\n",
    "    raw_resource = pd.read_csv(converted_resource_path)\n",
    "\n",
    "raw_length = len(raw_resource)\n",
    "print(\"\")\n",
    "print (F\"Raw data contains {raw_length} records\")\n",
    "\n",
    "raw_resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05a588-4540-413f-9f8e-7557eae1bc35",
   "metadata": {},
   "source": [
    "### Duplicate Values in Reference Columns\n",
    "The provided reference field must contain unique values. This will check whether there are any duplicated values in the reference column selected (at the top of the Jupyter Notebook). If the reference_column variable is an empty string (\"\") the aggregates will be calculated for all fields.\n",
    "\n",
    "Please note \"**count**\" is the number of entries in a field **not including NaN values**, \"**size**\" is the number of entries in a field **including NaN values**, and \"**nunique**\" is the number of unique values. An appropriate primary key should not include any NaN values, and all values must be unique. \n",
    "\n",
    "For the ideal reference/primary key field: `count = nunique = size`.\n",
    "\n",
    "Please note calculations are run on raw(ish) data generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6688ee-621d-40ff-961d-fff236f1c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reference_column == \"\":\n",
    "    duplicate_reference_check = raw_resource.agg(['count', 'size', 'nunique'])\n",
    "else:\n",
    "    duplicate_reference_check = raw_resource[reference_column].agg(['count', 'size', 'nunique'])\n",
    "duplicate_reference_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853fcb09",
   "metadata": {},
   "source": [
    "### Scripting\n",
    "\n",
    "if everything above looks OK, you can use the scripts below to insert the relevant updates into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb097c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( F\"IMPORTING INTO {collection_name} -------------------\")\n",
    "print (\"\")\n",
    "print (\"touch import.csv\")\n",
    "\n",
    "header = \"organisation,documentation-url,endpoint-url,start-date,pipelines,plugin\"\n",
    "\n",
    "line = F\"{organisation},{documentation_url},{endpoint_url},{start_date},{dataset},\"\n",
    "if plugin is not None:\n",
    "    line = line + F\"{plugin}\"\n",
    "\n",
    "collection = collection_name.rsplit('-', 1)[0]\n",
    "\n",
    "print (\"\")\n",
    "print (header)\n",
    "print (line)\n",
    "print (\"\")\n",
    "print (F\"Save the two lines above to `import.csv` and run the line below from inside your collection folder. You need a .venv in place.\\n\")\n",
    "print (\"\")\n",
    "print (F\"digital-land add-endpoints-and-lookups ./import.csv {collection}\")\n",
    "print (\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
