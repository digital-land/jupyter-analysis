{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f33af68-507b-4831-9186-46067200bb87",
   "metadata": {},
   "source": [
    "## Check an endpoint\n",
    "This notebook aims to run the pipeline on a given endpoint to check to see if it will be successful. This includes collecting, pipeline and datset stages. It aims to also highlight useful information as a summary as to whether the endpoint would be successful on our platform. it will download all relevant data to do this and hence might be disk intensive. You'll need to provide the following information:\n",
    "- collection - this is the collection that the dataset belongs to, this can be extracted from the specification but for this notebook we ask to provide it incase you want to test the pipeline on something which isn't being included in the main site right now\n",
    "- dataset - this is the dataset that the endpoint is meant to provide data for, technically this can be multiple datasets but this this use case it should just be one. It is also the name of the pipeline that is ran on the individual resources that are downloaded from the endpoint. the terms dataset/pipline are often the same\n",
    "- organisation - the organisation identifier to be used for the endpoint\n",
    "- endpoint url - the actual url needed for the endpoint\n",
    "- plugin - often we use plugins to download the data this is only needed for specific endpoints\n",
    "\n",
    "If you are seeing errors regarding digital-land, then try \n",
    "\n",
    "`pip3 install -e git+https://github.com/digital-land/pipeline.git#egg=digital-land`\n",
    "\n",
    "And restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9959de-69a3-4199-86ad-ecf5fa3150c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from functions import run_endpoint_workflow, missing_columns\n",
    "from sqlite_query_functions import DatasetSqlite\n",
    "from convert_functions import convert_resource\n",
    "from digital_land.collection import Collection\n",
    "from data_file import get_duplicates_between_orgs\n",
    "from download_data import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3f438-0b45-485e-b1f2-6e9b4bb01729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extend these lists as/when you need to add other collections\n",
    "\n",
    "# collection_name = 'article-4-direction-collection'\n",
    "# collection_name = 'brownfield-land-collection'\n",
    "# collection_name = 'conservation-area-collection'\n",
    "# collection_name = 'flood-risk-zone-collection'\n",
    "# collection_name = 'listed-building-collection'\n",
    "# collection_name = 'tree-preservation-order-collection'\n",
    "collection_name = 'developer-contributions'\n",
    "\n",
    "# dataset = 'article-4-direction'\n",
    "# dataset = 'article-4-direction-area'\n",
    "# dataset = 'brownfield-land'\n",
    "# dataset = 'conservation-area'\n",
    "# dataset = 'conservation-area-document'\n",
    "# dataset = 'flood-risk-zone'\n",
    "# dataset = 'listed-building-outline'\n",
    "# dataset = 'tree'\n",
    "# dataset = 'tree-preservation-order'\n",
    "# dataset = 'tree-preservation-zone'\n",
    "# dataset = 'developer-agreement-transaction'\n",
    "# dataset = 'developer-agreement-contribution'\n",
    "dataset = 'developer-agreement'\n",
    "\n",
    "# additional_column_mappings=None\n",
    "# additional_concats=None\n",
    "\n",
    "# plugin = None\n",
    "# plugin = 'arcgis'\n",
    "# plugin = 'wfs'\n",
    " \n",
    "# additional_column_mappings=None\n",
    "\n",
    "# additional_concats=None\n",
    "\n",
    "# EXAMPLE / TEST DATA HERE\n",
    "\n",
    "# collection_name = 'brownfield-land-collection'\n",
    "# dataset = 'brownfield-land'\n",
    "organisation = 'local-authority-eng:TEW'\n",
    "endpoint_url = 'https://tewkesburyborough-my.sharepoint.com/:x:/g/personal/website_tewkesburyborough_onmicrosoft_com/EUtZYznG_r9DmnRTN2GJFywBQDyfS9oWczBkcHk6DEqj5A?e=V8myPt&download=1'\n",
    "documentation_url = \"https://tewkesbury.gov.uk/services/planning/community-infrastructure-levy-cil/developer-contributions/\"\n",
    "start_date=\"\"\n",
    "plugin = ''\n",
    "licence = \"ogl3\"\n",
    "\n",
    "reference_column = \"\"\n",
    "\n",
    "additional_column_mappings=None\n",
    "additional_concats=None\n",
    "\n",
    "# generic data_dir setting\n",
    "data_dir = '../data/endpoint_checker'\n",
    "\n",
    "# example playing with additional confiigs\n",
    "# additional_concats = [{\n",
    "#     'dataset':'tree-preservation-zone',\n",
    "#     'endpoint':'de1eb90a8b037292ef8ae14bfabd1184847ef99b7c6296bb6e75379e6c1f9572',\n",
    "#     'resource':'e6b0ccaf9b50a7f57a543484fd291dbe43f52a6231b681c3a7cc5e35a6aba254',\n",
    "#     'field':'reference',\n",
    "#     'fields':'REFVAL;LABEL',\n",
    "#     'separator':'/'\n",
    "# }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38144f-1c71-4283-840e-bddffd3ea776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_endpoint_workflow(\n",
    "    collection_name,\n",
    "    dataset,\n",
    "    organisation,\n",
    "    endpoint_url,\n",
    "    plugin,\n",
    "    data_dir,\n",
    "    additional_col_mappings=additional_column_mappings,\n",
    "    additional_concats=additional_concats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4ef1f-9456-4e1f-b654-cbb56d5e3cf2",
   "metadata": {},
   "source": [
    "### Collection log summaries\n",
    "\n",
    "We need to establish if a resource was downloaded from the endpoint and whether there were any issues during the collection process. Examine the output of the below. There should be one log for the attempt made at downloading from the endpoint. If status code is 200 then the resource was downloaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the collection from the specified directory\n",
    "    collection = Collection(os.path.join(data_dir,'collection'))\n",
    "    \n",
    "    # Load the collection\n",
    "    collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "    \n",
    "    # Access the records of a resource within the collection\n",
    "    collection.resource.records\n",
    "    \n",
    "    print(\"Download has succeeded.\")\n",
    "except Exception as e:\n",
    "    # If anything goes wrong, print the error and status code\n",
    "    print(f\"Download failed with error: {e}\")\n",
    "    if hasattr(e, 'status'):\n",
    "        print(f\"Status code: {e.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17395f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logs = collection.log.entries\n",
    "logs = pd.DataFrame.from_records(logs)\n",
    "logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f842a08",
   "metadata": {},
   "source": [
    "### Check unnassigned entiities\n",
    "Detect and assign entity numbers where entries are currently unnassigned. Examine the list below to see what (if any) entities have been assigned. if you were to include these in an actual pipeline you would need to update the configuration lookup.csv with these values. It's worth checking they are sensible before this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unassigned_entries = pd.read_csv(os.path.join(data_dir,'var','cache','unassigned-entries.csv'))\n",
    "if len(unassigned_entries) == 0:\n",
    "    print('No additional entity numbers required')\n",
    "else:\n",
    "    print(F\"{len(unassigned_entries)} unassigned entities\\n\")\n",
    "    print(unassigned_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313b0fa-151d-4c52-bb46-f6e733d2369e",
   "metadata": {},
   "source": [
    "### Check logs collated from the pipeline process\n",
    "We need to read the logs and examine to see if the data points were all read in correctly. This uses the sqlite database to do so with some custom queries. You could directly examine the csvs if the pipeline fails.\n",
    "\n",
    "First, check the column mappings to see what columns the pipeline automatically mapped. If this is empty or has missing values, then it's likely to be the reason data isn't appearing at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c01973",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_column_mappings()\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/digital-land/specification/main/specification/dataset-field.csv')\n",
    "expected_columns = df.groupby('dataset')['field'].apply(list).to_dict()\n",
    "missing_columns(results, dataset, expected_columns)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c9e67-e108-424e-a5cb-6aab2cbde912",
   "metadata": {},
   "source": [
    "### Issues Logs\n",
    "\n",
    "Lists all of the issues/warnings, their respective severities and whose responsibility it is to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "dataset_issues = dataset_db.get_issues_by_type()\n",
    "\n",
    "def get_issue_types_with_responsibility_and_severity():\n",
    "    datasette_url = \"https://datasette.planning.data.gov.uk/\"\n",
    "    params = urllib.parse.urlencode({\n",
    "    \"sql\": f\"\"\"\n",
    "    select issue_type, responsibility, severity\n",
    "    from issue_type\n",
    "    \"\"\",\n",
    "    \"_size\": \"max\"\n",
    "    })\n",
    "    \n",
    "    url = f\"{datasette_url}digital-land.csv?{params}\"\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "    \n",
    "issue_reference = get_issue_types_with_responsibility_and_severity()\n",
    "matched_dataset_issues = dataset_issues.merge(issue_reference, on='issue_type', how='left')\n",
    "matched_dataset_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e374935",
   "metadata": {},
   "source": [
    "#### Look at a specific problem type in more detail\n",
    "\n",
    "Take the issue_type from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset_db.get_issues()\n",
    "\n",
    "#problem = 'OSGB out of bounds of England'\n",
    "#problem = 'invalid geometry'\n",
    "# problem = 'Unexpected geom type'\n",
    "problem = 'OSGB'\n",
    "\n",
    "results.loc[(results.issue_type == problem) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f612d0-ca02-4080-b6f6-847f2b1ea22c",
   "metadata": {},
   "source": [
    "### Final dataset \n",
    "\n",
    "Shows the end result of the processing. You should see a decent number of these columns populated with data from the raw resources above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7b0a5-3d38-4389-bf3c-413ecae98a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_entities()\n",
    "\n",
    "final_length = len(results)\n",
    "\n",
    "print(\"\")\n",
    "print (F\"Final data contains {final_length} records\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34438da9-86e2-4154-9dc2-5d0e0bb7ff3e",
   "metadata": {},
   "source": [
    "### Existing Duplicate Entities between organisations\n",
    "\n",
    "This downloads a sqlite db for the current dataset  \n",
    "It compares the current endpoint entities with existing ones   \n",
    "Identifies duplicates between all organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b466d48-c431-44d2-afd6-f64125f60b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset,collection_name,f\"{data_dir}/entity_resolution\")\n",
    "dataset_path = os.path.join(f\"{data_dir}/entity_resolution\",f'{dataset}.sqlite3')\n",
    "duplicates = get_duplicates_between_orgs(dataset_path,f'../data/endpoint_checker/dataset/{dataset}.sqlite3')\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicate entities found with existing entities\")\n",
    "\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55f99f-22c7-4275-ac03-61b5a73987c3",
   "metadata": {},
   "source": [
    "### Duplicates with different entity numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870521b7-4394-485a-a77f-cf413f2da4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not duplicates.empty:\n",
    "    filtered_df = duplicates[duplicates['primary_entity'] != duplicates['secondary_entity']]\n",
    "    filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74e98b-da56-4d5f-8f86-4f26c669619e",
   "metadata": {},
   "source": [
    "To merge the duplicated entities  \n",
    "For each entity, use the corresponding primary_entity value (as the entity number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60391993-7245-4eab-a3af-dcaf4768efd8",
   "metadata": {},
   "source": [
    "### Possible Internal Duplicate Entities\n",
    "\n",
    "The below table displays duplicates in the data provided identified using the geographical info (geometry and point column).  \n",
    "Sometimes it is legit, but worth checking in the source data to make sure it passes the sniff test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90666c-22b5-4b6c-a5f3-2cf0a4427c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (results[['geometry', 'point']].apply(lambda x: x.str.strip() == '')).all().all():\n",
    "    grouped = results.groupby(['geometry', 'point'])\n",
    "    grouped_list=[]\n",
    "    for key, value in (grouped.groups).items():\n",
    "        if len(value) > 1 and key[1] !='':\n",
    "            filtered_df = results[(results['geometry'] == key[0]) & (results['point'] == key[1])]\n",
    "            grouped_list.append(filtered_df)\n",
    "\n",
    "    if len(grouped_list)>1:\n",
    "        for i in range(len(grouped_list)):\n",
    "            display(grouped_list[i])\n",
    "    else:\n",
    "        print(\"No internal duplicates found in the given endpoint\")\n",
    "else:\n",
    "    print(\"No geometry or point data in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe5bba",
   "metadata": {},
   "source": [
    "### RAW (ish) DATA\n",
    "\n",
    "This is the lightly processed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw resources\n",
    "collection = Collection(os.path.join(data_dir,'collection'))\n",
    "collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "resources = collection.resource.entries\n",
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = resources[0]['resource']\n",
    "resource_path = os.path.join(data_dir,'collection','resource',resource)\n",
    "\n",
    "print (F\"Reading raw resource from {resource_path}\")\n",
    "\n",
    "try:\n",
    "    raw_resource = pd.read_csv(resource_path)\n",
    "except (UnicodeDecodeError,TypeError,pd.errors.ParserError):\n",
    "    converted_resource_dir = os.path.join(data_dir,'var','converted_resources')\n",
    "    converted_resource_path = os.path.join(converted_resource_dir,f'{resource}.csv') \n",
    "    if not os.path.exists(converted_resource_path):\n",
    "        convert_resource(resource,resource_path,converted_resource_dir,dataset)\n",
    "    print (F\"Failed - reading from {converted_resource_path} instead.\")\n",
    "    raw_resource = pd.read_csv(converted_resource_path)\n",
    "\n",
    "raw_length = len(raw_resource)\n",
    "print(\"\")\n",
    "print (F\"Raw data contains {raw_length} records\")\n",
    "\n",
    "raw_resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05a588-4540-413f-9f8e-7557eae1bc35",
   "metadata": {},
   "source": [
    "### Duplicate Values in Reference Columns\n",
    "The provided reference field must contain unique values. This will check whether there are any duplicated values in the reference column selected (at the top of the Jupyter Notebook). If the reference_column variable is an empty string (\"\") the aggregates will be calculated for all fields.\n",
    "\n",
    "Please note \"**count**\" is the number of entries in a field **not including NaN values**, \"**size**\" is the number of entries in a field **including NaN values**, and \"**nunique**\" is the number of unique values. An appropriate primary key should not include any NaN values, and all values must be unique. \n",
    "\n",
    "For the ideal reference/primary key field: `count = nunique = size`.\n",
    "\n",
    "Please note calculations are run on raw(ish) data generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6688ee-621d-40ff-961d-fff236f1c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reference_column == \"\":\n",
    "    duplicate_reference_check = raw_resource.agg(['count', 'size', 'nunique'])\n",
    "else:\n",
    "    duplicate_reference_check = raw_resource[reference_column].agg(['count', 'size', 'nunique'])\n",
    "duplicate_reference_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853fcb09",
   "metadata": {},
   "source": [
    "### Scripting\n",
    "\n",
    "if everything above looks OK, you can use the scripts below to insert the relevant updates into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb097c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( F\"IMPORTING INTO {collection_name} -------------------\")\n",
    "print (\"\")\n",
    "print (\"touch import.csv\")\n",
    "\n",
    "\n",
    "\n",
    "header = \"organisation,documentation-url,endpoint-url,start-date,pipelines,plugin,licence\"\n",
    "\n",
    "line = F\"{organisation},{documentation_url},{endpoint_url},{start_date},{dataset},{licence}\"\n",
    "if plugin is not None:\n",
    "    line = line + F\"{plugin}\"\n",
    "\n",
    "collection = collection_name.rsplit('-', 1)[0]\n",
    "\n",
    "if(licence == \"\" or licence == None):\n",
    "    print(\"The licence field cannot be null or empty\")\n",
    "elif(documentation_url == \"\" or documentation_url == None):\n",
    "    print(\"The licence field cannot be null or empty\")\n",
    "else:\n",
    "    print (\"\")\n",
    "    print (header)\n",
    "    print (line)\n",
    "    print (\"\")\n",
    "    print (F\"Save the two lines above to `import.csv` and run the line below from inside your collection folder. You need a .venv in place.\\n\")\n",
    "    print (\"\")\n",
    "    print (F\"digital-land add-endpoints-and-lookups ./import.csv {collection}\")\n",
    "    print (\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01153e46-37e4-439f-ad36-32527d0bd061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
